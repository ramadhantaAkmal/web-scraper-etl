[
    {
        "title":"#20499 - SAP Data Engineer",
        "company_name":"Qualitest",
        "location":"Atlanta, GA",
        "description":"Job description:\n\nQualitest is seeking a SAP Data Engineer to join our Data Center of Excellence (COE) within the Enterprise division. In this role, you will design, develop, and maintain scalable data pipelines and integration frameworks that connect SAP and non-SAP ecosystems, enabling enterprise-wide insights and automation.\n\nAs a key contributor to the enterprise data strategy, you\u2019ll ensure data quality, consistency, and accessibility across systems while working closely with AI\/ML, analytics, and business teams. You will help shape the data foundation that powers predictive insights, intelligent automation, and data-driven decision-making.\n\nThis position offers an exciting opportunity for an engineer who thrives at the intersection of SAP technologies, cloud data engineering, and advanced analytics \u2014 building the infrastructure that drives digital transformation and business intelligence.\n\nLocation: Remote \u2013 U.S. (CST Preferred)\nEmployment Type: Full-time, Permanent\n\nCore Focus Areas\n\u2022 SAP Data Integration Expertise\n\u2022 Hands-on experience with SAP data tools: BODS, SLT, SDI, CPI-DS\n\u2022 Integration of SAP platforms (S\/4HANA, BW\/4HANA, Ariba) with enterprise and cloud systems\n\u2022 ETL & Data Modeling\n\u2022 Proven ability to design, optimize, and automate ETL\/ELT workflows\n\u2022 Experience developing data models and architectures to support analytics and AI\/ML initiatives\n\u2022 Cloud & Analytics Integration\n\u2022 Practical experience with Azure, AWS, or GCP data services\n\u2022 Familiarity with SAP Analytics Cloud (SAC), Power BI, or similar BI\/reporting tools\n\nKey Responsibilities\n\u2022 Design, build, and maintain end-to-end data pipelines integrating SAP (S\/4HANA, BW\/4HANA, Ariba) with enterprise and cloud platforms.\n\u2022 Develop and optimize ETL\/ELT workflows using SAP BODS, SLT, SDI, CPI-DS, and related integration tools.\n\u2022 Partner with AI\/ML teams to enable data-driven automation, predictive analytics, and intelligent process optimization.\n\u2022 Collaborate with business and analytics stakeholders to define efficient data models and semantic layers for reporting.\n\u2022 Support SAP analytics initiatives using SAC, Power BI, and other visualization tools.\n\u2022 Implement and enforce data governance, validation, and compliance standards (HIPAA, audit, security).\n\u2022 Design and optimize data architectures for scalability, performance, and cloud readiness.\n\u2022 Ensure end-to-end data lineage, integrity, and traceability across SAP and non-SAP systems.\n\u2022 Diagnose and resolve complex data integration and performance issues.\n\u2022 Maintain technical documentation, reusable frameworks, and process standards for consistency and knowledge sharing.\n\u2022 Stay current with emerging SAP, data engineering, and AI technologies to drive innovation and best practices.\n\nQualifications\n\u2022 Bachelor\u2019s degree in Computer Science, Information Systems, Data Engineering, or a related field (or equivalent work experience).\n\u2022 5\u20138 years of experience in SAP Data Engineering, ETL Development, or Analytics Enablement.\n\u2022 Strong expertise in SQL, data modeling, and integration of SAP HANA and BW\/4HANA environments.\n\u2022 Proven proficiency with SAP data integration tools (BODS, SLT, SDI, CPI-DS).\n\u2022 Experience with SAP Analytics Cloud (SAC), Power BI, or equivalent BI tools.\n\u2022 Exposure to AI\/ML data pipelines or collaboration with data science teams is a plus.\n\u2022 Working knowledge of Python, Spark, or cloud data platforms (Azure, AWS, GCP).\n\u2022 Understanding of data governance, master data management (MDM), and data lifecycle management principles.\n\u2022 Excellent analytical thinking, communication, and problem-solving skills with a strong sense of ownership and collaboration.\n\nWe offer:\n\nQualitest is seeking a SAP Data Engineer to join our Data Center of Excellence (COE) within the Enterprise division. In this role, you will design, develop, and maintain scalable data pipelines and integration frameworks that connect SAP and non-SAP ecosystems, enabling enterprise-wide insights and automation.\n\nAs a key contributor to the enterprise data strategy, you\u2019ll ensure data quality, consistency, and accessibility across systems while working closely with AI\/ML, analytics, and business teams. You will help shape the data foundation that powers predictive insights, intelligent automation, and data-driven decision-making.\n\nThis position offers an exciting opportunity for an engineer who thrives at the intersection of SAP technologies, cloud data engineering, and advanced analytics \u2014 building the infrastructure that drives digital transformation and business intelligence.\n\nLocation: Remote \u2013 U.S. (CST Preferred)\nEmployment Type: Full-time, Permanent\n\nCore Focus Areas\n\u2022 SAP Data Integration Expertise\n\u2022 Hands-on experience with SAP data tools: BODS, SLT, SDI, CPI-DS\n\u2022 Integration of SAP platforms (S\/4HANA, BW\/4HANA, Ariba) with enterprise and cloud systems\n\u2022 ETL & Data Modeling\n\u2022 Proven ability to design, optimize, and automate ETL\/ELT workflows\n\u2022 Experience developing data models and architectures to support analytics and AI\/ML initiatives\n\u2022 Cloud & Analytics Integration\n\u2022 Practical experience with Azure, AWS, or GCP data services\n\u2022 Familiarity with SAP Analytics Cloud (SAC), Power BI, or similar BI\/reporting tools\n\nKey Responsibilities\n\u2022 Design, build, and maintain end-to-end data pipelines integrating SAP (S\/4HANA, BW\/4HANA, Ariba) with enterprise and cloud platforms.\n\u2022 Develop and optimize ETL\/ELT workflows using SAP BODS, SLT, SDI, CPI-DS, and related integration tools.\n\u2022 Partner with AI\/ML teams to enable data-driven automation, predictive analytics, and intelligent process optimization.\n\u2022 Collaborate with business and analytics stakeholders to define efficient data models and semantic layers for reporting.\n\u2022 Support SAP analytics initiatives using SAC, Power BI, and other visualization tools.\n\u2022 Implement and enforce data governance, validation, and compliance standards (HIPAA, audit, security).\n\u2022 Design and optimize data architectures for scalability, performance, and cloud readiness.\n\u2022 Ensure end-to-end data lineage, integrity, and traceability across SAP and non-SAP systems.\n\u2022 Diagnose and resolve complex data integration and performance issues.\n\u2022 Maintain technical documentation, reusable frameworks, and process standards for consistency and knowledge sharing.\n\u2022 Stay current with emerging SAP, data engineering, and AI technologies to drive innovation and best practices.\n\nQualifications\n\u2022 Bachelor\u2019s degree in Computer Science, Information Systems, Data Engineering, or a related field (or equivalent work experience).\n\u2022 5\u20138 years of experience in SAP Data Engineering, ETL Development, or Analytics Enablement.\n\u2022 Strong expertise in SQL, data modeling, and integration of SAP HANA and BW\/4HANA environments.\n\u2022 Proven proficiency with SAP data integration tools (BODS, SLT, SDI, CPI-DS).\n\u2022 Experience with SAP Analytics Cloud (SAC), Power BI, or equivalent BI tools.\n\u2022 Exposure to AI\/ML data pipelines or collaboration with data science teams is a plus.\n\u2022 Working knowledge of Python, Spark, or cloud data platforms (Azure, AWS, GCP).\n\u2022 Understanding of data governance, master data management (MDM), and data lifecycle management principles.\n\u2022 Excellent analytical thinking, communication, and problem-solving skills with a strong sense of ownership and collaboration.\n\nBenefits\nWhy QualiTest?\n\u2022 Be a part of a company who strives to support for diversity and inclusion in the workplace \u2013 we are one, we are many at Qualitest. Celebrate culture, share knowledge with engineers from around the globe, and inspire each other through our differences.\u202fWe have more than 40% women and around 120 different nationalities.\n\u2022 Local and global opportunities \u2013 we offer you internal rotation and international mobility opportunities to grow your career.\n\u2022 Clear view of your career and progression with the company \u2013 Qualitest is growing massively (since 2021 \u2013 tripled our employees base \u2013 we now have more than 8,000 engineers) and giving you the opportunity to grow with us.\n\u2022 Work hard and play harder with our flexible and casual culture. Take a break from work and join an employee event, or\u202fenjoy the amenities and games provided from one of our Employees Centers.Save your earnings and prepare for your future by enrolling in our 401k plan where Qualitest will match your contributions accelerating your savings plan.\n\u2022 Take care of health with enrollment into one of our competitive healthcare benefits. Qualitest will match towards your HSA if you choose to participate.\n\u2022 Never stop experimenting and learning with\u202fQCraft \u2013 our Learning & Development platform: 50,000+ courses, 300+ virtual labs, mentorship and leadership programs, professional tribes, sponsored certifications, and much more.\n\u2022 Stay active and get rewarded with our Corporate Wellness Program. We pay your Gym membership and giving you opportunities to Earn additional vacation times for attendance the gym!\n\u2022 Earn bonuses via our Client Referral and Employee Referral Program\u2019s. Refer and earn \u2013 tap your network for net-worth.\n\u2022 We recognize our employees work via our Qudos platform - You can earn bonuses and spot awards by celebrating your and your peers\u2019 achievements.\n\u2022 Planning a vacation? Looking for car insurance? Get access to Qualitest Employee Perks for discounts on anything from travel to electronics. With so many offerings the savings are endless!\n\u2022 A Competitive pay, the salary range for the role is $110,000 - $120,000.\n\u2022 Intrigued to find more about us?\n\u2022 Visit our website at https:\/\/www.qualitestgroup.com\/\n\u2022 If you like what you have read, send us your resume and let\u2019s start talking!",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "Hands-on experience with SAP data tools: BODS, SLT, SDI, CPI-DS",
                    "Practical experience with Azure, AWS, or GCP data services",
                    "Familiarity with SAP Analytics Cloud (SAC), Power BI, or similar BI\/reporting tools",
                    "Bachelor\u2019s degree in Computer Science, Information Systems, Data Engineering, or a related field (or equivalent work experience)",
                    "5\u20138 years of experience in SAP Data Engineering, ETL Development, or Analytics Enablement",
                    "Strong expertise in SQL, data modeling, and integration of SAP HANA and BW\/4HANA environments",
                    "Proven proficiency with SAP data integration tools (BODS, SLT, SDI, CPI-DS)",
                    "Experience with SAP Analytics Cloud (SAC), Power BI, or equivalent BI tools",
                    "Working knowledge of Python, Spark, or cloud data platforms (Azure, AWS, GCP)",
                    "Understanding of data governance, master data management (MDM), and data lifecycle management principles",
                    "Excellent analytical thinking, communication, and problem-solving skills with a strong sense of ownership and collaboration",
                    "SAP Data Integration Expertise",
                    "Hands-on experience with SAP data tools: BODS, SLT, SDI, CPI-DS",
                    "Integration of SAP platforms (S\/4HANA, BW\/4HANA, Ariba) with enterprise and cloud systems",
                    "Proven ability to design, optimize, and automate ETL\/ELT workflows",
                    "Cloud & Analytics Integration",
                    "Practical experience with Azure, AWS, or GCP data services",
                    "Familiarity with SAP Analytics Cloud (SAC), Power BI, or similar BI\/reporting tools",
                    "Bachelor\u2019s degree in Computer Science, Information Systems, Data Engineering, or a related field (or equivalent work experience)",
                    "5\u20138 years of experience in SAP Data Engineering, ETL Development, or Analytics Enablement",
                    "Strong expertise in SQL, data modeling, and integration of SAP HANA and BW\/4HANA environments",
                    "Proven proficiency with SAP data integration tools (BODS, SLT, SDI, CPI-DS)",
                    "Experience with SAP Analytics Cloud (SAC), Power BI, or equivalent BI tools",
                    "Working knowledge of Python, Spark, or cloud data platforms (Azure, AWS, GCP)",
                    "Understanding of data governance, master data management (MDM), and data lifecycle management principles",
                    "Excellent analytical thinking, communication, and problem-solving skills with a strong sense of ownership and collaboration"
                ]
            },
            {
                "title":"Benefits",
                "items":[
                    "Work hard and play harder with our flexible and casual culture",
                    "Take a break from work and join an employee event, or\u202fenjoy the amenities and games provided from one of our Employees Centers",
                    "Save your earnings and prepare for your future by enrolling in our 401k plan where Qualitest will match your contributions accelerating your savings plan",
                    "Take care of health with enrollment into one of our competitive healthcare benefits",
                    "Never stop experimenting and learning with\u202fQCraft \u2013 our Learning & Development platform: 50,000+ courses, 300+ virtual labs, mentorship and leadership programs, professional tribes, sponsored certifications, and much more",
                    "Stay active and get rewarded with our Corporate Wellness Program",
                    "We pay your Gym membership and giving you opportunities to Earn additional vacation times for attendance the gym!",
                    "Earn bonuses via our Client Referral and Employee Referral Program\u2019s",
                    "Refer and earn \u2013 tap your network for net-worth",
                    "We recognize our employees work via our Qudos platform - You can earn bonuses and spot awards by celebrating your and your peers\u2019 achievements",
                    "Planning a vacation?",
                    "Get access to Qualitest Employee Perks for discounts on anything from travel to electronics",
                    "With so many offerings the savings are endless!",
                    "A Competitive pay, the salary range for the role is $110,000 - $120,000"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "In this role, you will design, develop, and maintain scalable data pipelines and integration frameworks that connect SAP and non-SAP ecosystems, enabling enterprise-wide insights and automation",
                    "As a key contributor to the enterprise data strategy, you\u2019ll ensure data quality, consistency, and accessibility across systems while working closely with AI\/ML, analytics, and business teams",
                    "You will help shape the data foundation that powers predictive insights, intelligent automation, and data-driven decision-making",
                    "SAP Data Integration Expertise",
                    "Integration of SAP platforms (S\/4HANA, BW\/4HANA, Ariba) with enterprise and cloud systems",
                    "ETL & Data Modeling",
                    "Proven ability to design, optimize, and automate ETL\/ELT workflows",
                    "Experience developing data models and architectures to support analytics and AI\/ML initiatives",
                    "Cloud & Analytics Integration",
                    "Design, build, and maintain end-to-end data pipelines integrating SAP (S\/4HANA, BW\/4HANA, Ariba) with enterprise and cloud platforms",
                    "Develop and optimize ETL\/ELT workflows using SAP BODS, SLT, SDI, CPI-DS, and related integration tools",
                    "Partner with AI\/ML teams to enable data-driven automation, predictive analytics, and intelligent process optimization",
                    "Collaborate with business and analytics stakeholders to define efficient data models and semantic layers for reporting",
                    "Support SAP analytics initiatives using SAC, Power BI, and other visualization tools",
                    "Implement and enforce data governance, validation, and compliance standards (HIPAA, audit, security)",
                    "Design and optimize data architectures for scalability, performance, and cloud readiness",
                    "Ensure end-to-end data lineage, integrity, and traceability across SAP and non-SAP systems",
                    "Diagnose and resolve complex data integration and performance issues",
                    "Maintain technical documentation, reusable frameworks, and process standards for consistency and knowledge sharing",
                    "Stay current with emerging SAP, data engineering, and AI technologies to drive innovation and best practices",
                    "In this role, you will design, develop, and maintain scalable data pipelines and integration frameworks that connect SAP and non-SAP ecosystems, enabling enterprise-wide insights and automation",
                    "As a key contributor to the enterprise data strategy, you\u2019ll ensure data quality, consistency, and accessibility across systems while working closely with AI\/ML, analytics, and business teams",
                    "You will help shape the data foundation that powers predictive insights, intelligent automation, and data-driven decision-making",
                    "ETL & Data Modeling",
                    "Experience developing data models and architectures to support analytics and AI\/ML initiatives",
                    "Design, build, and maintain end-to-end data pipelines integrating SAP (S\/4HANA, BW\/4HANA, Ariba) with enterprise and cloud platforms",
                    "Develop and optimize ETL\/ELT workflows using SAP BODS, SLT, SDI, CPI-DS, and related integration tools",
                    "Partner with AI\/ML teams to enable data-driven automation, predictive analytics, and intelligent process optimization",
                    "Collaborate with business and analytics stakeholders to define efficient data models and semantic layers for reporting",
                    "Support SAP analytics initiatives using SAC, Power BI, and other visualization tools",
                    "Implement and enforce data governance, validation, and compliance standards (HIPAA, audit, security)",
                    "Design and optimize data architectures for scalability, performance, and cloud readiness",
                    "Ensure end-to-end data lineage, integrity, and traceability across SAP and non-SAP systems",
                    "Diagnose and resolve complex data integration and performance issues",
                    "Maintain technical documentation, reusable frameworks, and process standards for consistency and knowledge sharing",
                    "Stay current with emerging SAP, data engineering, and AI technologies to drive innovation and best practices"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"LinkedIn",
                "link":"https:\/\/www.linkedin.com\/jobs\/view\/%2320499-sap-data-engineer-at-qualitest-4321822011?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Full-time"
    },
    {
        "title":"AI Automations Data Engineer \/ Data Scientist",
        "company_name":"RTX",
        "location":"Hartford, CT",
        "description":"Date Posted:\n2025-11-07\n\nCountry:\nUnited States of America\n\nLocation:\nUTCT1: Corp - CT - Remote Remote Location, Remote City, CT, 06101 USA\n\nPosition Role Type:\nRemote\n\nU.S. Citizen, U.S. Person, or Immigration Status Requirements:\nThis job requires a U.S. Person. A U.S. Person is a lawful permanent resident as defined in 8 U.S.C. 1101(a)(20) or who is a protected individual as defined by 8 U.S.C. 1324b(a)(3). U.S. citizens, U.S. nationals, U.S. permanent residents, or individuals granted refugee or asylee status in the U.S. are considered U.S. persons. For a complete definition of \u201cU.S. Person\u201d go here. https:\/\/www.ecfr.gov\/current\/title-22\/chapter-I\/subchapter-M\/part-120\/subpart-C\/section-120.62\n\nSecurity Clearance:\nNone\/Not Required\n\nRTX Corporation is an Aerospace and Defense company that provides advanced systems and services for commercial, military and government customers worldwide. It comprises three industry-leading businesses \u2013 Collins Aerospace Systems, Pratt & Whitney, and Raytheon. Its 185,000 employees enable the company to operate at the edge of known science as they imagine and deliver solutions that push the boundaries in quantum physics, electric propulsion, directed energy, hypersonics, avionics and cybersecurity. The company, formed in 2020 through the combination of Raytheon Company and the United Technologies Corporation aerospace businesses, is headquartered in Arlington, VA.\n\nThe following position is to join our RTX Enterprise Services team:\n\nAs a Data Engineer \/ Data Scientist at Raytheon Technologies, you will play a critical role in designing, building, and optimizing data solutions that drive business insights, operational efficiency, and technological innovation. You will be responsible for developing and maintaining robust data pipelines, implementing advanced machine learning models, and delivering actionable business intelligence solutions. This role demands a blend of technical expertise and strategic vision, with opportunities to explore emerging technologies, including generative AI, to support RTX's mission to protect and connect the world.\n\nWhat You Will Do:\n\u2022 Design and implement scalable, reliable, and efficient end-to-end data pipelines and workflows using tools like Snowflake and Matillion.\n\u2022 Collaborate with business owners to identify key data needs, align them with RTX objectives, and translate them into actionable technical solutions.\n\u2022 Evaluate industry trends and emerging technologies, including generative AI, to recommend and implement innovative data strategies.\n\u2022 Demonstrate expertise in data integration, transformation, and modeling using tools such as Snowflake and Matillion.\n\u2022 Develop impactful dashboards and reports using Power BI and Tableau to support data-driven decision-making across the organization.\n\u2022 Leverage Databricks for advanced data processing, machine learning workflows, and big data analytics.\n\u2022 Build and deploy machine learning models and statistical analyses to address complex business challenges and deliver measurable outcomes.\n\u2022 Stay current with advancements in AI and ML technologies, including generative AI, to identify opportunities for innovation in RTX\u2019s data landscape.\n\u2022 Collaborate with cross-functional teams, including architecture and cloud operations, to align data solutions with organizational priorities and policies.\n\u2022 Maintain governance standards, ensure compliance with RTX data security protocols, and effectively engage with stakeholders to communicate progress and resolve challenges.\n\nQualifications You Must Have:\n\u2022 Bachelor\u2019s degree in Computer Science, Data Science, Engineering, or a related field, and 5+ years of experience in data engineering, data science, or a related field.\n\u2022 Proficiency in Snowflake, Matillion, BI Tools ( Power BI, Tableau, or similar) and Data Science Platforms (Databricks).\n\u2022 Strong programming skills in Python, SQL, or other relevant languages.\n\u2022 Hands-on experience in building and deploying machine learning models.\n\u2022 Strong analytical and problem-solving skills with a business-oriented mindset.\n\nQualifications We Prefer:\n\u2022 Master\u2019s degree in a relevant field.\n\u2022 Knowledge or experience with generative AI frameworks and tools (e.g., LLMs, RAG).\n\u2022 Familiarity with cloud platforms such as Azure or AWS.\n\u2022 Strong communication and collaboration skills to work effectively with cross-functional teams.\n\u2022 SAFe or similar certifications in agile teams or devops\n\nWhat We Offer:\n\nWhether you\u2019re just starting out on your career journey or are an experienced professional, we offer a robust total rewards package with compensation; healthcare, wellness, retirement and work\/life benefits; career development and recognition programs. Some of the benefits we offer include parental (including paternal) leave, flexible work schedules, achievement awards, educational assistance and child\/adult backup care.\n\nWork Location:\n\nRemote\n\nAs part of our commitment to maintaining a secure hiring process, candidates may be asked to attend select steps of the interview process in-person at one of our office locations, regardless of whether the role is designated as on-site, hybrid or remote.\n\nThe salary range for this role is 82,000 USD - 164,000 USD. The salary range provided is a good faith estimate representative of all experience levels. RTX considers several factors when extending an offer, including but not limited to, the role, function and associated responsibilities, a candidate\u2019s work experience, location, education\/training, and key skills.\n\nHired applicants may be eligible for benefits, including but not limited to, medical, dental, vision, life insurance, short-term disability, long-term disability, 401(k) match, flexible spending accounts, flexible work schedules, employee assistance program, Employee Scholar Program, parental leave, paid time off, and holidays. Specific benefits are dependent upon the specific business unit as well as whether or not the position is covered by a collective-bargaining agreement.\n\nHired applicants may be eligible for annual short-term and\/or long-term incentive compensation programs depending on the level of the position and whether or not it is covered by a collective-bargaining agreement. Payments under these annual programs are not guaranteed and are dependent upon a variety of factors including, but not limited to, individual performance, business unit performance, and\/or the company\u2019s performance.\n\nThis role is a U.S.-based role. If the successful candidate resides in a U.S. territory, the appropriate pay structure and benefits will apply.\n\nRTX anticipates the application window closing approximately 40 days from the date the notice was posted. However, factors such as candidate flow and business necessity may require RTX to shorten or extend the application window.\n\nRTX is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability or veteran status, or any other applicable state or federal protected class. RTX provides affirmative action in employment for qualified Individuals with a Disability and Protected Veterans in compliance with Section 503 of the Rehabilitation Act and the Vietnam Era Veterans\u2019 Readjustment Assistance Act.\n\nPrivacy Policy and Terms:\n\nClick on this link to read the Policy and Terms",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "U.S. Citizen, U.S. Person, or Immigration Status Requirements:",
                    "This job requires a U.S. Person",
                    "Bachelor\u2019s degree in Computer Science, Data Science, Engineering, or a related field, and 5+ years of experience in data engineering, data science, or a related field",
                    "Proficiency in Snowflake, Matillion, BI Tools ( Power BI, Tableau, or similar) and Data Science Platforms (Databricks)",
                    "Strong programming skills in Python, SQL, or other relevant languages",
                    "Hands-on experience in building and deploying machine learning models",
                    "Strong analytical and problem-solving skills with a business-oriented mindset",
                    "Master\u2019s degree in a relevant field",
                    "Knowledge or experience with generative AI frameworks and tools (e.g., LLMs, RAG)",
                    "Familiarity with cloud platforms such as Azure or AWS",
                    "Strong communication and collaboration skills to work effectively with cross-functional teams",
                    "SAFe or similar certifications in agile teams or devops"
                ]
            },
            {
                "title":"Benefits",
                "items":[
                    "Whether you\u2019re just starting out on your career journey or are an experienced professional, we offer a robust total rewards package with compensation; healthcare, wellness, retirement and work\/life benefits; career development and recognition programs",
                    "Some of the benefits we offer include parental (including paternal) leave, flexible work schedules, achievement awards, educational assistance and child\/adult backup care",
                    "The salary range provided is a good faith estimate representative of all experience levels",
                    "Hired applicants may be eligible for benefits, including but not limited to, medical, dental, vision, life insurance, short-term disability, long-term disability, 401(k) match, flexible spending accounts, flexible work schedules, employee assistance program, Employee Scholar Program, parental leave, paid time off, and holidays",
                    "Specific benefits are dependent upon the specific business unit as well as whether or not the position is covered by a collective-bargaining agreement",
                    "Hired applicants may be eligible for annual short-term and\/or long-term incentive compensation programs depending on the level of the position and whether or not it is covered by a collective-bargaining agreement",
                    "Payments under these annual programs are not guaranteed and are dependent upon a variety of factors including, but not limited to, individual performance, business unit performance, and\/or the company\u2019s performance"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "As a Data Engineer \/ Data Scientist at Raytheon Technologies, you will play a critical role in designing, building, and optimizing data solutions that drive business insights, operational efficiency, and technological innovation",
                    "You will be responsible for developing and maintaining robust data pipelines, implementing advanced machine learning models, and delivering actionable business intelligence solutions",
                    "This role demands a blend of technical expertise and strategic vision, with opportunities to explore emerging technologies, including generative AI, to support RTX's mission to protect and connect the world",
                    "Design and implement scalable, reliable, and efficient end-to-end data pipelines and workflows using tools like Snowflake and Matillion",
                    "Collaborate with business owners to identify key data needs, align them with RTX objectives, and translate them into actionable technical solutions",
                    "Evaluate industry trends and emerging technologies, including generative AI, to recommend and implement innovative data strategies",
                    "Demonstrate expertise in data integration, transformation, and modeling using tools such as Snowflake and Matillion",
                    "Develop impactful dashboards and reports using Power BI and Tableau to support data-driven decision-making across the organization",
                    "Leverage Databricks for advanced data processing, machine learning workflows, and big data analytics",
                    "Build and deploy machine learning models and statistical analyses to address complex business challenges and deliver measurable outcomes",
                    "Stay current with advancements in AI and ML technologies, including generative AI, to identify opportunities for innovation in RTX\u2019s data landscape",
                    "Collaborate with cross-functional teams, including architecture and cloud operations, to align data solutions with organizational priorities and policies",
                    "Maintain governance standards, ensure compliance with RTX data security protocols, and effectively engage with stakeholders to communicate progress and resolve challenges"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"RTX Careers",
                "link":"https:\/\/careers.rtx.com\/global\/en\/job\/01803369\/AI-Automations-Data-Engineer-Data-Scientist?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"ZipRecruiter",
                "link":"https:\/\/www.ziprecruiter.com\/c\/Raytheon-Technologies-Corporate-Headquarters\/Job\/AI-Automations-Data-Engineer-Data-Scientist\/-in-Hartford,CT?jid=eb5c5c80b498c05a&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"SimplyHired",
                "link":"https:\/\/www.simplyhired.com\/job\/O40WtBknJITxeMnpvP4qEOEfIyEJIFjh4s9-be0B1j4qfArEfeqqnA?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Ladders",
                "link":"https:\/\/www.theladders.com\/job\/ai-automations-date-engineer-data-scientist-raytheontechnologies-hartford-ct_84175568?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"WhatJobs",
                "link":"https:\/\/www.whatjobs.com\/jobs\/data-engineering?id=2234902481&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"USA.jobs",
                "link":"https:\/\/www.usa.jobs\/jobs\/171223789-ai-automations-date-engineer-data-scientist?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Jobrapido",
                "link":"https:\/\/us.jobrapido.com\/jobpreview\/6535106395883175936?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Expertini",
                "link":"https:\/\/us.expertini.com\/jobs\/job\/ai-automations-date-engineer-data-scientist-hartford-rtx-corporation-raytheontech-raytglobal01803369externalenglobal\/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Full-time"
    },
    {
        "title":"Senior Data Engineer (Intelligent Foundations and Experiences)",
        "company_name":"Capital One",
        "location":"Harrisonburg, VA",
        "description":"Senior Data Engineer (Intelligent Foundations and Experiences)\n\nDo you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative,inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Data Engineer, you\u2019ll have the opportunity to be on the forefront of driving a major transformation within Capital One.\n\nIntelligent Foundations & Experiences (IFX) is a powerful collective of horizontal technology organizations that are driving Capital One\u2019s real-time intelligent future. Together with our partners in the Enterprise and across lines of business, we deliver broad-reaching technical solutions and advance state-of-the-art science to help every Capital One associate and our 100+M customers succeed.\n\nWhat You\u2019ll Do:\n\u2022 Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies\n\u2022 Work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems\n\u2022 Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake\n\u2022 Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community\n\u2022 Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment\n\u2022 Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance\n\nBasic Qualifications:\n\u2022 Bachelor\u2019s Degree\n\u2022 At least 3 years of experience in application development (Internship experience does not apply)\n\u2022 At least 1 year of experience in big data technologies\n\nPreferred Qualifications:\n\u2022 5+ years of experience in application development including Python, SQL, Scala, or Java\n\u2022 2+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)\n\u2022 3+ years experience with Distributed data\/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)\n\u2022 2+ year experience working on real-time data and streaming applications\n\u2022 2+ years of experience with NoSQL implementation (Mongo, Cassandra)\n\u2022 2+ years of data warehousing experience (Redshift or Snowflake)\n\u2022 3+ years of experience with UNIX\/Linux including basic commands and shell scripting\n\u2022 2+ years of experience with Agile engineering practices\n\nAt this time, Capital One will not sponsor a new applicant for employment authorization, or offer any immigration related support for this position (i.e. H1B, F-1 OPT, F-1 STEM OPT, F-1 CPT, J-1, TN, E-2, E-3, L-1 and O-1, or any EADs or other forms of work authorization that require immigration support from an employer).\n\nThe minimum and maximum full-time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.\n\nPlano, TX: $144,200 - $164,600 for Senior Data Engineer\n\nMcLean, VA: $158,600 - $181,000 for Senior Data Engineer\n\nRichmond, VA: $144,200 - $164,600 for Senior Data Engineer\n\nCandidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate\u2019s offer letter.\n\nThis role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and\/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n\nCapital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website. Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.\n\nThis role is expected to accept applications for a minimum of 5 business days.\n\nNo agencies please. Capital One is an equal opportunity employer (EOE, including disability\/vet) committed to non-discrimination in compliance with applicable federal, state, and local laws. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City\u2019s Fair Chance Act; Philadelphia\u2019s Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n\nIf you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at\n\nRecruitingAccommodation@capitalone.com\n. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.\n\nFor technical support or questions about Capital One's recruiting process, please send an email to\n\nCareers@capitalone.com\n\nCapital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.\n\nCapital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).\n\nBy applying, you consent to your information being transmitted by Monster to the Employer, as data controller, through the Employer\u2019s data processor SonicJobs.\nSee Capital One Terms & Conditions at https:\/\/www.capitalone.com\/digital\/terms-conditions\/ and Privacy Policy at https:\/\/www.capitalone.com\/privacy\/ and SonicJobs Privacy Policy at https:\/\/www.sonicjobs.com\/us\/privacy-policy and Terms of Use at https:\/\/www.sonicjobs.com\/us\/terms-conditions\n\nSkills:\nAgile Programming Methodologies, Amazon Web Services (AWS), Apache Cassandra, Apache Hadoop, Apache Hive, Apache Kafka, Apache Spark, Big Data, Cloud Computing, Data Warehousing, Distributed Computing, Electronic Medical Records, Emerging Technology, Federal Laws and Regulations, Java, Leading Edge Technology, Legal, Linux Operating System, Machine Learning, MapReduce, Mentoring, Microservices, Microsoft Windows Azure, MySQL, NoSQL, Open Source, Performance Tuning\/Optimization, Problem Solving Skills, Programming Languages, Programming Tools, Public Cloud, Python Programming\/Scripting Language, Relational Databases (RDBMS), SQL (Structured Query Language), Scala Programming Language, Snowflake Schema, Software Development, State Laws and Regulations, Team Player, Technical Support, Test Plan\/Schedule, Testing, Training Tools, Unit Test, Unix Operating Systems, Unix Shell Programming\n\nAbout the Company:\nCapital One",
        "job_highlights":null,
        "apply_options":[
            {
                "title":"Monster",
                "link":"https:\/\/www.monster.com\/job-openings\/senior-data-engineer-intelligent-foundations-and-experiences-harrisonburg-va--cce28518-9308-4359-b16e-2249d3d78af7?mstr_dist=true&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"SaluteMyJob",
                "link":"https:\/\/salutemyjob.com\/jobs\/lead-data-engineer-intelligent-foundations-and-experiences-m-w-d-harrisonburg-virginia\/2457062909-2\/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"JobServe",
                "link":"https:\/\/www.jobserve.com\/us\/en\/extjob\/LEAD-DATA-ENGINEER-in-Harrisonburg-Virginia-USA-262D7BC39E78A9B6D1\/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Jobs For Stevenage Fans",
                "link":"https:\/\/jobs.stevenagefc.com\/jobs\/lead-data-engineer-intelligent-foundations-and-experiences-m-w-d-harrisonburg-virginia\/2457062909-2\/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"KGET Jobs",
                "link":"https:\/\/jobs.kget.com\/jobs\/lead-data-engineer-intelligent-foundations-and-experiences-m-w-d-harrisonburg-virginia\/2457062909-2\/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Snagajob",
                "link":"https:\/\/www.snagajob.com\/jobs\/1177328179?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"KSNT Jobs",
                "link":"https:\/\/jobs.ksnt.com\/jobs\/lead-data-engineer-intelligent-foundations-and-experiences-m-w-d-harrisonburg-virginia\/2457062909-2\/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"JobNet",
                "link":"https:\/\/www.jobnet.com.au\/us\/en\/search-jobs-in-Virginia,-USA\/SENIOR-LEAD-DATA-ENGINEER-INTELLIGENT-FOUNDATIONS-AND-EXPERIENCES-9D1F6812264AAA90D8\/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Full-time and Part-time"
    },
    {
        "title":"Azure Data Engineer Level 3",
        "company_name":"Golden Resources. Inc.",
        "location":"Blue Ash, OH",
        "description":"Job Description\n\nThe team is looking for a Data Engineer experienced in implementing data solutions in Azure. The Data Engineer will analyze, design and develop enterprise data and information architecture deliverables, focusing on data as an asset for the enterprise. The Data Engineer will also support the implementation of Infrastructure as Code (IaC) by working with teams to help engineer scalable, reliable, and resilient software running in the cloud.\n\u2022 Accountable for developing and delivering technological responses to targeted business outcomes.\n\u2022 Analyze, design and develop enterprise data and information architecture deliverables, focusing on data as an asset for the enterprise. Understand and follow reusable standards, design patterns, guidelines, and configurations to deliver valuable data and information across the enterprise, including direct collaboration with where needed.\n\u2022 2+ years experience with automation production systems (Ansible Tower, Jenkins, Puppet, or Selenium)\n\u2022 Working knowledge of databases and SQL Experience with software development methodologies and SDLC Candidate possess a problem-solving attitude and can work independently\n\u2022 Must be very organized, able to balance multiple priorities, and self-motivated\n\nKey Responsibilities\n\u2022 Experience in administration and configuration of API Gateways (e.g. Apigee\/Kong) Apply cloud computing skill to deploy upgrades and fixes\n\u2022 Design, develop, and implement integrations based on use feedback.\n\u2022 Troubleshoot production issues and coordinate with the development team to streamline code deployment.\n\u2022 Implement automation tools and frameworks (Ci\/CD pipelines).\n\u2022 Analyze code and communicate detailed reviews to development teams to ensure a marked improvement in applications and the timely completion of products.\n\u2022 Collaborate with team members to improve the company s engineering tools, systems and procedures, and data security.\n\u2022 Deliver quality customer service and resolve end-user issues in a timely manner\n\u2022 Draft architectural diagrams, interface specifications and other design documents\n\u2022 Participate in the development and communication of data strategy and roadmaps across the technology organization to support project portfolio and business strategy\n\u2022 Innovate, develop, and drive the development and communication of data strategy and roadmaps across the technology organization to support project portfolio\n\u2022 Drive the development and communication of enterprise standards for data domains and data solutions, focusing on simplified integration and streamlined operational and analytical uses\n\u2022 Drive digital innovation by leveraging innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platforms\n\u2022 Define high-level migration plans to address the gaps between the current and future state, typically in sync with the budgeting or other capital planning processes\n\u2022 Lead the analysis of the technology environment to detect critical deficiencies and recommend solutions for improvement\n\u2022 Mentor team members in data principles, patterns, processes and practices\n\u2022 Promote the reuse of data assets, including the management of the data catalog for reference\n\u2022 Draft and review architectural diagrams, interface specifications and other design documents",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "The team is looking for a Data Engineer experienced in implementing data solutions in Azure",
                    "2+ years experience with automation production systems (Ansible Tower, Jenkins, Puppet, or Selenium)",
                    "Working knowledge of databases and SQL Experience with software development methodologies and SDLC Candidate possess a problem-solving attitude and can work independently",
                    "Must be very organized, able to balance multiple priorities, and self-motivated"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "The Data Engineer will analyze, design and develop enterprise data and information architecture deliverables, focusing on data as an asset for the enterprise",
                    "The Data Engineer will also support the implementation of Infrastructure as Code (IaC) by working with teams to help engineer scalable, reliable, and resilient software running in the cloud",
                    "Accountable for developing and delivering technological responses to targeted business outcomes",
                    "Analyze, design and develop enterprise data and information architecture deliverables, focusing on data as an asset for the enterprise",
                    "Understand and follow reusable standards, design patterns, guidelines, and configurations to deliver valuable data and information across the enterprise, including direct collaboration with where needed",
                    "Experience in administration and configuration of API Gateways (e.g. Apigee\/Kong) Apply cloud computing skill to deploy upgrades and fixes",
                    "Design, develop, and implement integrations based on use feedback",
                    "Troubleshoot production issues and coordinate with the development team to streamline code deployment",
                    "Implement automation tools and frameworks (Ci\/CD pipelines)",
                    "Analyze code and communicate detailed reviews to development teams to ensure a marked improvement in applications and the timely completion of products",
                    "Collaborate with team members to improve the company s engineering tools, systems and procedures, and data security",
                    "Deliver quality customer service and resolve end-user issues in a timely manner",
                    "Draft architectural diagrams, interface specifications and other design documents",
                    "Participate in the development and communication of data strategy and roadmaps across the technology organization to support project portfolio and business strategy",
                    "Innovate, develop, and drive the development and communication of data strategy and roadmaps across the technology organization to support project portfolio",
                    "Drive the development and communication of enterprise standards for data domains and data solutions, focusing on simplified integration and streamlined operational and analytical uses",
                    "Drive digital innovation by leveraging innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platforms",
                    "Define high-level migration plans to address the gaps between the current and future state, typically in sync with the budgeting or other capital planning processes",
                    "Lead the analysis of the technology environment to detect critical deficiencies and recommend solutions for improvement",
                    "Mentor team members in data principles, patterns, processes and practices",
                    "Promote the reuse of data assets, including the management of the data catalog for reference",
                    "Draft and review architectural diagrams, interface specifications and other design documents"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"Dice",
                "link":"https:\/\/www.dice.com\/job-detail\/b9a4eb2f-1be3-48e8-88c7-2e635a3c2c39?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"ZipRecruiter",
                "link":"https:\/\/www.ziprecruiter.com\/c\/Apidel-Technologies\/Job\/Quality-Engineer-Data-Level-3\/-in-Blue-Ash,OH?jid=9602dd89157d8f58&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"LinkedIn",
                "link":"https:\/\/www.linkedin.com\/jobs\/view\/azure-data-engineer-level-3-at-golden-technology-4321129266?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Monster",
                "link":"https:\/\/www.monster.com\/job-openings\/quality-engineer-data-level-3-blue-ash-oh--aef7077d-ee6b-4ad4-bf63-33d6a8ff3d63?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"IHireTechnology",
                "link":"https:\/\/www.ihiretechnology.com\/jobs\/view\/499577764?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Talents By Vaia",
                "link":"https:\/\/talents.vaia.com\/companies\/leadstack\/data-engineer-level-3-32846359\/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Talentify",
                "link":"https:\/\/www.talentify.io\/job\/quality-engineer-data-level-3-blue-ash-ohio-us-leadstack-25-03017?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Jobs.weekday.works",
                "link":"https:\/\/jobs.weekday.works\/golden-resources.-inc.-azure-data-engineer-level-3?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Contractor"
    },
    {
        "title":"Data Engineer \/ EDI and Claims \/ Hybrid",
        "company_name":"RCM Healthcare Services",
        "location":"Rochelle Park, NJ",
        "description":"Data Engineer \u2013 EDI & Claims\n\nHybrid: 2 days in NYC office, 3 days remote\nSalary: Up to $135,000 + generous benefits (including pension plan)\nKey Requirement: EDI X12 Experience\n\nAbout This Role and Why It Matters\nYou\u2019ll be the go-to expert for EDI and claims data\u2014helping ensure compliance, accuracy, and efficiency across health plan operations. Your contributions will shape how we deliver care and meet regulatory standards, making a real difference in people\u2019s lives.\n\nAbout Us\nFor over 30 years, we\u2019ve been a mission-driven healthcare organization dedicated to improving the lives of New Yorkers.\n\nWhat\u2019s In It For You?\n\u2022 Make an Impact: Your work will directly improve healthcare for thousands of New Yorkers.\n\u2022 Flexibility: Enjoy a hybrid schedule that balances collaboration and remote work.\n\u2022 Growth & Stability: Join a mission-driven organization with over 30 years of success and a strong commitment to employee development.\n\u2022 Competitive Package: Up to $135,000 salary, comprehensive benefits, and a pension plan.\n\nWhat You\u2019ll Do\n\u2022 Own and optimize encounter data submissions to state and federal agencies.\n\u2022 Troubleshoot and resolve EDI mapping issues to keep data flowing smoothly.\n\u2022 Build and maintain SQL queries, stored procedures, and SSIS jobs for automation.\n\u2022 Collaborate with internal teams and vendors to solve complex data challenges.\n\u2022 Support compliance teams with timely, accurate data insights.\n\u2022 Document workflows and implement data quality checks for continuous improvement.\n\u2022 Contribute to MMCOR reporting and create KPI dashboards to track performance.\n\nWhat You Bring\n\u2022 5+ years of SQL Server experience (stored procedures, SSIS, SSRS).\n\u2022 3+ years working with healthcare data (claims, enrollment, provider).\n\u2022 Strong knowledge of EDI X12 standards and file mapping.\n\u2022 Experience with BI tools (Tableau, Power BI, BusinessObjects).\n\u2022 Bonus: Familiarity with Visual Studio, .NET, XML, BizTalk.\n\u2022 Bachelor\u2019s degree in Information Systems, Computer Science, or related field.\n\u2022 Must be legally authorized to work in the U.S. (no sponsorship available).\n#AC1\n#ACP",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "5+ years of SQL Server experience (stored procedures, SSIS, SSRS)",
                    "3+ years working with healthcare data (claims, enrollment, provider)",
                    "Strong knowledge of EDI X12 standards and file mapping",
                    "Experience with BI tools (Tableau, Power BI, BusinessObjects)",
                    "Bonus: Familiarity with Visual Studio, .NET, XML, BizTalk",
                    "Bachelor\u2019s degree in Information Systems, Computer Science, or related field",
                    "Must be legally authorized to work in the U.S. (no sponsorship available)"
                ]
            },
            {
                "title":"Benefits",
                "items":[
                    "Hybrid: 2 days in NYC office, 3 days remote",
                    "Salary: Up to $135,000 + generous benefits (including pension plan)",
                    "Make an Impact: Your work will directly improve healthcare for thousands of New Yorkers",
                    "Flexibility: Enjoy a hybrid schedule that balances collaboration and remote work",
                    "Growth & Stability: Join a mission-driven organization with over 30 years of success and a strong commitment to employee development",
                    "Competitive Package: Up to $135,000 salary, comprehensive benefits, and a pension plan"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "You\u2019ll be the go-to expert for EDI and claims data\u2014helping ensure compliance, accuracy, and efficiency across health plan operations",
                    "Your contributions will shape how we deliver care and meet regulatory standards, making a real difference in people\u2019s lives",
                    "Own and optimize encounter data submissions to state and federal agencies",
                    "Troubleshoot and resolve EDI mapping issues to keep data flowing smoothly",
                    "Build and maintain SQL queries, stored procedures, and SSIS jobs for automation",
                    "Collaborate with internal teams and vendors to solve complex data challenges",
                    "Support compliance teams with timely, accurate data insights",
                    "Document workflows and implement data quality checks for continuous improvement",
                    "Contribute to MMCOR reporting and create KPI dashboards to track performance"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"Women For Hire - Job Board",
                "link":"https:\/\/jobs.womenforhire.com\/job\/usa\/rochelle-park-nj\/data-engineer-edi-and-claims-hybrid-763701\/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Full-time"
    },
    {
        "title":"Principal Data Engineer",
        "company_name":"PepsiCo",
        "location":"Plano, TX",
        "description":"Overview\n\nAs a member of the data engineering team, you will be the key technical expert developing and overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. You'll be an empowered member of a team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. As a member of the data engineering team, you will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products\n\nResponsibilities\n\u2022 Active contributor to code development in projects and services\n\u2022 Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products\n\u2022 Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance\n\u2022 Responsible for implementing best practices around systems integration\n\u2022 Security, performance and data management\n\u2022 Empower the business by creating value through the increased adoption of data\n\u2022 Data science and business intelligence landscape\n\u2022 Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions\n\nCompensation and Benefits:\n\u2022 The expected compensation range for this position is between $89,000 - $149,000\n\u2022 Location, confirmed job-related skills, experience, and education will be considered in setting actual starting salary. Your recruiter can share more about the specific salary range during the hiring process\n\u2022 Bonus based on performance and eligibility target payout is 10% of annual salary paid out annually\n\u2022 Paid time off subject to eligibility, including paid parental leave, vacation, sick, and bereavement\n\u2022 In addition to salary, PepsiCo offers a comprehensive benefits package to support our employees and their families, subject to elections and eligibility: Medical, Dental, Vision, Disability, Health, and Dependent Care Reimbursement Accounts, Employee Assistance Program (EAP), Insurance (Accident, Group Legal, Life), Defined Contribution Retirement Plan\n\nQualifications\n\u2022 6+ years of overall technology experience that includes at least 4+ years of hands-on software development, data engineering, and systems architecture.\n\u2022 4+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools.\n\u2022 4+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.).\n\u2022 2+ years in cloud data engineering experience in Azure(Azure Data Factory(ADF), ADLS-2, Databricks(Lakehouse, Workflow SQL, Unity catalog).\n\u2022 Fluent with Azure cloud services. Azure or Databricks Certification is a plus.\n\u2022 Experience with integration of multi cloud services with on-premises technologies.\n\u2022 Experience with data modeling, data warehousing, and building high-volume ETL\/ELT pipelines.\n\u2022 Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations.\n\u2022 Experience building\/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets.\n\u2022 Experience with at least one MPP database technology such as Redshift, Synapse or SnowFlake.\n\u2022 Experience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes.\n\u2022 Experience with version control systems like Github and deployment & CI tools.\n\u2022 Experience with Azure Data Factory, Azure Databricks and Azure Machine learning tools.\n\u2022 Experience with Statistical\/ML techniques is a plus.\n\u2022 Experience with building solutions in the Supply chain space(Digital Procurement, Manufacturing, Cost, Warehouse, Network Design) is a plus.\n\u2022 Understanding of metadata management, data lineage, and data glossaries is a plus.\n\u2022 Working knowledge of agile development, including DevOps and DataOps concepts.\n\u2022 Familiarity with business intelligence tools (such as PowerBI).\n\nEEO Statement\n\nOur Company will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the Fair Credit Reporting Act, and all other applicable laws, including but not limited to, San Francisco Police Code Sections 4901-4919, commonly referred to as the San Francisco Fair Chance Ordinance; and Chapter XVII, Article 9 of the Los Angeles Municipal Code, commonly referred to as the Fair Chance Initiative for Hiring Ordinance.\n\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status.\n\nPepsiCo is an Equal Opportunity Employer: Female \/ Minority \/ Disability \/ Protected Veteran \/ Sexual Orientation \/ Gender Identity\n\nIf you'd like more information about your EEO rights as an applicant under the law, please download the available EEO is the Law & EEO is the Law Supplement documents. View PepsiCo EEO Policy.\n\nPlease view our Pay Transparency Statement",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "6+ years of overall technology experience that includes at least 4+ years of hands-on software development, data engineering, and systems architecture",
                    "4+ years of experience with Data Lake Infrastructure, Data Warehousing, and Data Analytics tools",
                    "4+ years of experience in SQL optimization and performance tuning, and development experience in programming languages like Python, PySpark, Scala etc.)",
                    "2+ years in cloud data engineering experience in Azure(Azure Data Factory(ADF), ADLS-2, Databricks(Lakehouse, Workflow SQL, Unity catalog)",
                    "Fluent with Azure cloud services",
                    "Experience with integration of multi cloud services with on-premises technologies",
                    "Experience with data modeling, data warehousing, and building high-volume ETL\/ELT pipelines",
                    "Experience with data profiling and data quality tools like Apache Griffin, Deequ, and Great Expectations",
                    "Experience building\/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets",
                    "Experience with at least one MPP database technology such as Redshift, Synapse or SnowFlake",
                    "Experience with running and scaling applications on the cloud infrastructure and containerized services like Kubernetes",
                    "Experience with version control systems like Github and deployment & CI tools",
                    "Experience with Azure Data Factory, Azure Databricks and Azure Machine learning tools",
                    "Working knowledge of agile development, including DevOps and DataOps concepts",
                    "Familiarity with business intelligence tools (such as PowerBI)"
                ]
            },
            {
                "title":"Benefits",
                "items":[
                    "The expected compensation range for this position is between $89,000 - $149,000",
                    "Location, confirmed job-related skills, experience, and education will be considered in setting actual starting salary",
                    "Your recruiter can share more about the specific salary range during the hiring process",
                    "Bonus based on performance and eligibility target payout is 10% of annual salary paid out annually",
                    "Paid time off subject to eligibility, including paid parental leave, vacation, sick, and bereavement",
                    "In addition to salary, PepsiCo offers a comprehensive benefits package to support our employees and their families, subject to elections and eligibility: Medical, Dental, Vision, Disability, Health, and Dependent Care Reimbursement Accounts, Employee Assistance Program (EAP), Insurance (Accident, Group Legal, Life), Defined Contribution Retirement Plan"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "As a member of the data engineering team, you will be the key technical expert developing and overseeing PepsiCo's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business",
                    "You'll be an empowered member of a team of data engineers who build data pipelines into various source systems, rest data on the PepsiCo Data Lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company",
                    "As a member of the data engineering team, you will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of PepsiCo's flagship data products",
                    "Active contributor to code development in projects and services",
                    "Manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products",
                    "Build and own the automation and monitoring frameworks that captures metrics and operational KPIs for data pipeline quality and performance",
                    "Responsible for implementing best practices around systems integration",
                    "Security, performance and data management",
                    "Empower the business by creating value through the increased adoption of data",
                    "Data science and business intelligence landscape",
                    "Collaborate with internal clients (data science and product teams) to drive solutioning and POC discussions"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"PepsiCo Careers",
                "link":"https:\/\/www.pepsicojobs.com\/main\/jobs\/399518?lang=en-us&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"LinkedIn",
                "link":"https:\/\/www.linkedin.com\/jobs\/view\/principal-data-engineer-at-pepsico-4303732534?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Ladders",
                "link":"https:\/\/www.theladders.com\/job\/principal-data-engineer-libertymutual-plano-tx_82515016?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Diversity.com",
                "link":"https:\/\/jobs.diversity.com\/career\/2090008\/principal-data-engineer-texas-tx-plano?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"BeBee",
                "link":"https:\/\/us.bebee.com\/job\/f609fd9b1db71462e54a5ce7294017bf?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Jobrapido",
                "link":"https:\/\/us.jobrapido.com\/jobpreview\/222004490855776256?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"JOBS.NOW",
                "link":"https:\/\/www.jobs.now\/jobs\/172158373-principal-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Hire The Better",
                "link":"https:\/\/www.hirethebetter.com\/jobs\/principal-data-engineer-plano-texas\/2430835108-2\/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Full-time"
    },
    {
        "title":"Software Engineering Manager - Data, Burger King",
        "company_name":"Restaurant Brands International",
        "location":"Miami, FL",
        "description":"Ready to make your next big professional move? Join us on our journey to achieve our big dream of building the most loved restaurant brands in the world.\n\nRestaurant Brands International Inc. is one of the world's largest quick service restaurant companies with nearly $45 billion in annual system-wide sales and over 32,000 restaurants in more than 120 countries and territories.\n\nRBI owns four of the world's most prominent and iconic quick service restaurant brands \u2013 TIM HORTONS\u00ae, BURGER KING\u00ae, POPEYES\u00ae, and FIREHOUSE SUBS\u00ae. These independently operated brands have been serving their respective guests, franchisees and communities for decades. Through its Restaurant Brands for Good framework, RBI is improving sustainable outcomes related to its food, the planet, and people and communities.\n\nRBI is committed to growing the TIM HORTONS\u00ae, BURGER KING\u00ae, POPEYES\u00ae and FIREHOUSE SUBS\u00ae brands by leveraging their respective core values, employee and franchisee relationships, and long track records of community support. Each brand benefits from the global scale and shared best practices that come from ownership by Restaurant Brands International Inc.\n\nThe Data Engineering Manager is responsible for leading a team of data engineers to design, build, and maintain robust data pipelines and infrastructure for The Burger King Company in the US. This role oversees the development and optimization of scalable data solutions that handle millions of transactions per day, enabling actionable insights and supporting strategic business initiatives. The ideal candidate will have extensive technical expertise, leadership experience, and a passion for innovation in data engineering.\n\nThis position is based in Miami, FL and is in the office 5 days a week.\n\nRole & Responsibilities:\n\u2022 Lead and mentor a team of data engineers, fostering professional growth and knowledge sharing.\n\u2022 Oversee day-to-day activities of the data engineering team, ensuring timely delivery of projects.\n\u2022 Collaborate with cross-functional teams, including data scientists, analysts, product managers, and IT, to develop and implement data solutions that meet business needs.\n\u2022 Design, build, and maintain scalable and efficient data pipelines and ETL processes.\n\u2022 Implement best practices for data integration, modeling, quality, and governance.\n\u2022 Ensure data reliability, integrity, and availability for critical analytics workflows.\n\u2022 Drive the development of the overall data engineering strategy and optimize processes to ensure efficiency and scalability.\n\u2022 Plan and execute data engineering projects, ensuring alignment with organizational objectives.\n\u2022 Gather and document requirements for projects, create data migration plans, and communicate progress with stakeholders.\n\u2022 Manage relationships with technology vendors and oversee service-level agreements.\n\u2022 Continuously evaluate emerging technologies and methodologies to improve data engineering capabilities.\n\u2022 Encourage a culture of innovation and continuous improvement within the team.\n\nQualifications & Skills:\n\u2022 5+ years of progressive experience in data engineering,\n\u2022 2+ years leading people and engineering teams.\n\u2022 Bachelor\u2019s or master\u2019s degree in Computer Science, Engineering, Information Technology, or a related field.\n\u2022 Strong programming skills in Python and SQL.\n\u2022 Extensive experience with AWS services (e.g., S3, Glue, Athena, EMR, SNS, SQS, KMS).\n\u2022 Experience with data warehousing solutions like Snowflake or Redshift.\n\u2022 Proficiency with workflow orchestration tools (e.g., Apache Airflow, Dagster).\n\u2022 Familiarity with reporting tools (e.g., Tableau, MicroStrategy).\n\u2022 Experience with Infrastructure as Code (e.g., Terraform).\n\u2022 Knowledge of NoSQL databases like DynamoDB.\n\u2022 Experience with Retool development and ticketing systems such as JIRA.\n\u2022 Demonstrated ability to lead and manage multiple complex projects in a fast-paced environment.\n\u2022 Excellent verbal and written communication skills, with the ability to translate technical concepts for non-technical stakeholders.\n\u2022 Strong analytical and problem-solving abilities.\n\u2022 Ability to build and maintain cross-functional relationships while managing competing priorities.\n\u2022 Self-motivated and driven to continuously improve processes and outcomes.\n\n#BurgerKing\n\nBenefits at all of our global offices are focused on physical, mental and financial wellness. We offer unique and progressive benefits, including a comprehensive global paid parental leave program that supports employees as they expand their families, free telemedicine and mental wellness support.\n\nRestaurant Brands International and all of its affiliated companies (collectively, RBI) are equal opportunity and affirmative action employers that do not discriminate on the basis of race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or veteran status, or any other characteristic protected by local, state, provincial or federal laws, rules, or regulations. RBI's policy applies to all terms and conditions of employment. Accommodation is available for applicants with disabilities upon request.",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "The ideal candidate will have extensive technical expertise, leadership experience, and a passion for innovation in data engineering",
                    "5+ years of progressive experience in data engineering,",
                    "2+ years leading people and engineering teams",
                    "Bachelor\u2019s or master\u2019s degree in Computer Science, Engineering, Information Technology, or a related field",
                    "Strong programming skills in Python and SQL",
                    "Extensive experience with AWS services (e.g., S3, Glue, Athena, EMR, SNS, SQS, KMS)",
                    "Experience with data warehousing solutions like Snowflake or Redshift",
                    "Proficiency with workflow orchestration tools (e.g., Apache Airflow, Dagster)",
                    "Familiarity with reporting tools (e.g., Tableau, MicroStrategy)",
                    "Experience with Infrastructure as Code (e.g., Terraform)",
                    "Knowledge of NoSQL databases like DynamoDB",
                    "Experience with Retool development and ticketing systems such as JIRA",
                    "Demonstrated ability to lead and manage multiple complex projects in a fast-paced environment",
                    "Excellent verbal and written communication skills, with the ability to translate technical concepts for non-technical stakeholders",
                    "Strong analytical and problem-solving abilities",
                    "Ability to build and maintain cross-functional relationships while managing competing priorities",
                    "Self-motivated and driven to continuously improve processes and outcomes"
                ]
            },
            {
                "title":"Benefits",
                "items":[
                    "We offer unique and progressive benefits, including a comprehensive global paid parental leave program that supports employees as they expand their families, free telemedicine and mental wellness support"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "The Data Engineering Manager is responsible for leading a team of data engineers to design, build, and maintain robust data pipelines and infrastructure for The Burger King Company in the US",
                    "This role oversees the development and optimization of scalable data solutions that handle millions of transactions per day, enabling actionable insights and supporting strategic business initiatives",
                    "This position is based in Miami, FL and is in the office 5 days a week",
                    "Lead and mentor a team of data engineers, fostering professional growth and knowledge sharing",
                    "Oversee day-to-day activities of the data engineering team, ensuring timely delivery of projects",
                    "Collaborate with cross-functional teams, including data scientists, analysts, product managers, and IT, to develop and implement data solutions that meet business needs",
                    "Design, build, and maintain scalable and efficient data pipelines and ETL processes",
                    "Implement best practices for data integration, modeling, quality, and governance",
                    "Ensure data reliability, integrity, and availability for critical analytics workflows",
                    "Drive the development of the overall data engineering strategy and optimize processes to ensure efficiency and scalability",
                    "Plan and execute data engineering projects, ensuring alignment with organizational objectives",
                    "Gather and document requirements for projects, create data migration plans, and communicate progress with stakeholders",
                    "Manage relationships with technology vendors and oversee service-level agreements",
                    "Continuously evaluate emerging technologies and methodologies to improve data engineering capabilities",
                    "Encourage a culture of innovation and continuous improvement within the team"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"Careers At RBI - Restaurant Brands International",
                "link":"https:\/\/careers.rbi.com\/global\/en\/job\/R2436\/Software-Engineering-Manager-Data-Burger-King?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Indeed",
                "link":"https:\/\/www.indeed.com\/viewjob?jk=e5066bd242b455f9&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Jobs For Stevenage Fans",
                "link":"https:\/\/jobs.stevenagefc.com\/jobs\/engineering-manager-merchandising-software-miami-florida\/2449921566-2\/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"SimplyHired",
                "link":"https:\/\/www.simplyhired.com\/job\/ev1gRZCU72SNuU1nWAYeqS9yuwWu1UKR5aml6AorbJL1LlxYKyVbuA?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"KGET Jobs",
                "link":"https:\/\/jobs.kget.com\/jobs\/engineering-manager-merchandising-software-miami-florida\/2449921566-2\/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Recruiter.com",
                "link":"https:\/\/jobs.recruiter.com\/jobs\/22975428342-manager-of-software-engineering-controls-miami?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"LilyLifestyle Jobs",
                "link":"https:\/\/jobs.lilylifestyle.co.uk\/jobs\/engineering-manager-merchandising-software-miami-florida\/2444259828-2\/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Edinburgh City FC",
                "link":"https:\/\/jobs.edinburghcityfc.com\/jobs\/engineering-manager-merchandising-software-miami-florida\/2449921566-2\/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Full-time"
    },
    {
        "title":"Data Engineer III",
        "company_name":"Sam's Club",
        "location":"Springdale, AR",
        "description":"Position Summary...The Data Engineer III is responsible for designing, building, and optimizing scalable big data pipelines, architectures, and datasets that enable advanced analytics and data-driven decision-making. This role involves developing efficient data transformation and processing frameworks, managing data structures, metadata, dependencies, and workloads, and ensuring the reliability and performance of the data ecosystem. The engineer will also work extensively with unstructured datasets, applying analytical techniques to extract insights and improve data accessibility across the organization.\n\nWhat you'll do...\n\u2022 Data Modeling: Designing and implementing data models to support structured and unstructured datasets, ensuring data integrity and efficiency.\n\u2022 Data Extraction: Developing and optimizing data extraction processes from various sources including databases, APIs, and logs.\n\u2022 Data Cleaning: Preprocessing and cleaning data to remove inconsistencies and improve data quality.\n\u2022 Data Screening: Implementing data validation and quality checks to ensure accuracy and completeness of data.\n\u2022 Data Exploration: Conducting exploratory data analysis to understand patterns, trends, and correlations in the data.\n\u2022 Data Visualization: Creating visualizations using tools like Tableau, PowerBI, or Looker to communicate insights and findings effectively.\n\u2022 Big Data Technologies: Utilizing tools and frameworks such as Spark, Spark SQL, PySpark, HDFS, and MapReduce for processing large datasets efficiently.\n\u2022 Cloud Services: Leveraging cloud platforms like GCP, Azure\/AWS, Databricks, Azure HD Insights, ADF for data storage, processing, and analytics.\n\u2022 Data Querying: Writing advanced SQL queries to extract and manipulate data from relational databases and other data stores.\n\u2022 Data Pipeline Development: Building and optimizing scalable data pipelines and architectures to move and transform data across systems.\n\u2022 Data Transformation: Developing processes for data transformation, structure, metadata, dependency, and workload management.\n\u2022 Enterprise Software Development: Contributing to the development of enterprise-level software products related to data engineering and analytics.\n\nWhat you'll bring:\n\u2022 Cross-functional Collaboration: Working closely with cross-functional teams including data scientists, analysts, and software engineers to achieve common goals.\n\u2022 Programming Languages: Proficiency in at least one scripting language like Python or Scala for automation, data manipulation, and tool development.\n\u2022 Agile Environment: Collaborating effectively in an Agile environment, participating in sprints, and adapting to changing\n\u2022 Analytical Skills: Applying strong analytical skills to work with complex and unstructured datasets, extracting valuable insights and actionable information. project requirements.\n\u2022 Big Data Data Stores: Implementing and managing highly scalable big data stores to efficiently store and access large volumes of data.\n\u2022 Data Value Extraction: Manipulating, processing, and extracting value from large, diverse datasets to drive business decisions and innovation.\n\u2022 Big Data Technologies: Experience utilizing tools and frameworks such as Spark, Spark SQL, PySpark, HDFS, and MapReduce for processing large datasets efficiently.\n\u2022 Cloud Services: Experience leveraging cloud platforms like GCP, Azure\/AWS, Databricks, Azure HD Insights, ADF for data storage, processing, and analytics.\n\nAbout Walmart General\/Not Function Specific\nSam Walton opened the first Sam's Club in 1983 to meet a growing need among customers who wanted to buy merchandise in bulk. Since then, Sam's Club has grown rapidly, opening more than 600 clubs in the U.S. and 100 clubs internationally. By offering affordable, wholesale merchandise to members, Sam's Club helps make saving simple for families and small business owners. Sam's Club employs about 110,000 associates in the U.S. The average club is 134,000 square feet and offers bulk groceries and general merchandise. Most clubs also have specialty services, such as a pharmacy, an optical department, a photo center, or a tire and battery center.\n\n\u200b\u200b\u200bFuture Ways of Working:\nOur company's success can be attributed to our employees. While technology has allowed us to be effective while working remotely, there is no substitute for being in the office together; it helps to shape our culture, collaborate, innovate, build relationships, and move more quickly. We strive to provide flexibility in order to promote a healthy work-life balance but recognize that in-person interactions are important to our culture and shared success. We'll meet in person on a regular and purposeful basis.\n\nBenefits:\nBenefits: Beyond our great compensation package, you can receive incentive awards for your performance. Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more.\n\nEqual Opportunity Employer:\nWalmart, Inc. is an Equal Opportunity Employer \u2013 By Choice. We believe we are best equipped to help our associates, customers, and the communities we serve live better when we really know them. That means understanding, respecting, and valuing unique styles, experiences, identities, ideas, and opinions \u2013 while being inclusive of all people.\n\nThe above information has been designed to indicate the general nature and level of work performed in the role. It is not designed to contain or be interpreted as a comprehensive inventory of all responsibilities and qualifications required of employees assigned to this job. The full Job Description can be made available as part of the hiring process.\n\nAt Sam's Club, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet!\n\n\u200e\n- Health benefits include medical, vision and dental coverage\n\n\u200e\n- Financial benefits include 401(k), stock purchase and company-paid life insurance\n\n\u200e\n- Paid time off benefits include PTO, parental leave, family care leave, bereavement, jury duty, and voting. You will also receive PTO and\/or PPTO that can be used for vacation, sick leave, holidays, or other purposes. The amount you receive depends on your job classification and length of employment. It will meet or exceed the requirements of paid sick leave laws, where applicable.\n\n\u200e\n\nFor information about PTO, see https:\/\/one.walmart.com\/notices.\n\n\u200e\n- Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more.\n\n\u200e\nLive Better U is a company paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities. Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates. Tuition, books, and fees are completely paid for by Walmart.\n\n\u200e\nEligibility requirements apply to some benefits and may depend on your job classification and length of employment. Benefits are subject to change and may be subject to a specific plan or program terms.\n\n\u200e\n\nFor information about benefits and eligibility, see One.Walmart.\n\n\u200e\nThe annual salary range for this position is $90,000.00-$180,000.00\n\n\u200e\nAdditional compensation includes annual or quarterly performance bonuses.\n\n\u200e\n\n\u200e\n\n\u200e\n\n\u200e\n\n\u200e\n\nMinimum Qualifications...\n\nOutlined below are the required minimum qualifications for this position. If none are listed, there are no minimum qualifications.\n\nOption 1: Bachelor\u2019s degree in Computer Science and 2 years' experience in software engineering or related field. Option 2: 4 years\u2019 experience in\nsoftware engineering or related field. Option 3: Master's degree in Computer Science.\n\nPreferred Qualifications...\n\nOutlined below are the optional preferred qualifications for this position. If none are listed, there are no preferred qualifications.\n\nData engineering, database engineering, business intelligence, or business analytics, Master\u2019s degree in Computer Science or related field and 2 years' experience in software engineering or related field, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly. The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmart\u2019s accessibility standards and guidelines for supporting an inclusive culture.\n\nPrimary Location...\n\n2101 Se Simple Savings Dr, Bentonville, AR 72712-4304, United States of America\n\nWalmart and its subsidiaries are committed to maintaining a drug-free workplace and has a no tolerance policy regarding the use of illegal drugs and alcohol on the job. This policy applies to all employees and aims to create a safe and productive work environment.",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "Cross-functional Collaboration: Working closely with cross-functional teams including data scientists, analysts, and software engineers to achieve common goals",
                    "Programming Languages: Proficiency in at least one scripting language like Python or Scala for automation, data manipulation, and tool development",
                    "Agile Environment: Collaborating effectively in an Agile environment, participating in sprints, and adapting to changing",
                    "Analytical Skills: Applying strong analytical skills to work with complex and unstructured datasets, extracting valuable insights and actionable information",
                    "Big Data Technologies: Experience utilizing tools and frameworks such as Spark, Spark SQL, PySpark, HDFS, and MapReduce for processing large datasets efficiently",
                    "Option 1: Bachelor\u2019s degree in Computer Science and 2 years' experience in software engineering or related field",
                    "Option 2: 4 years\u2019 experience in",
                    "software engineering or related field",
                    "Option 3: Master's degree in Computer Science",
                    "Data engineering, database engineering, business intelligence, or business analytics, Master\u2019s degree in Computer Science or related field and 2 years' experience in software engineering or related field, We value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing Web Content Accessibility Guidelines (WCAG) 2.2 AA standards, assistive technologies, and integrating digital accessibility seamlessly",
                    "The ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following Walmart\u2019s accessibility standards and guidelines for supporting an inclusive culture"
                ]
            },
            {
                "title":"Benefits",
                "items":[
                    "Benefits: Beyond our great compensation package, you can receive incentive awards for your performance",
                    "Other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, PTO, multiple health plans, and much more",
                    "Health benefits include medical, vision and dental coverage",
                    "Financial benefits include 401(k), stock purchase and company-paid life insurance",
                    "Paid time off benefits include PTO, parental leave, family care leave, bereavement, jury duty, and voting",
                    "You will also receive PTO and\/or PPTO that can be used for vacation, sick leave, holidays, or other purposes",
                    "The amount you receive depends on your job classification and length of employment",
                    "It will meet or exceed the requirements of paid sick leave laws, where applicable",
                    "Other benefits include short-term and long-term disability, company discounts, Military Leave Pay, adoption and surrogacy expense reimbursement, and more",
                    "Live Better U is a company paid education benefit program for full-time and part-time associates in Walmart and Sam's Club facilities",
                    "Programs range from high school completion to bachelor's degrees, including English Language Learning and short-form certificates",
                    "Tuition, books, and fees are completely paid for by Walmart",
                    "Benefits are subject to change and may be subject to a specific plan or program terms",
                    "The annual salary range for this position is $90,000.00-$180,000.00",
                    "Additional compensation includes annual or quarterly performance bonuses"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "Position Summary...The Data Engineer III is responsible for designing, building, and optimizing scalable big data pipelines, architectures, and datasets that enable advanced analytics and data-driven decision-making",
                    "This role involves developing efficient data transformation and processing frameworks, managing data structures, metadata, dependencies, and workloads, and ensuring the reliability and performance of the data ecosystem",
                    "The engineer will also work extensively with unstructured datasets, applying analytical techniques to extract insights and improve data accessibility across the organization",
                    "Data Modeling: Designing and implementing data models to support structured and unstructured datasets, ensuring data integrity and efficiency",
                    "Data Extraction: Developing and optimizing data extraction processes from various sources including databases, APIs, and logs",
                    "Data Cleaning: Preprocessing and cleaning data to remove inconsistencies and improve data quality",
                    "Data Screening: Implementing data validation and quality checks to ensure accuracy and completeness of data",
                    "Data Exploration: Conducting exploratory data analysis to understand patterns, trends, and correlations in the data",
                    "Data Visualization: Creating visualizations using tools like Tableau, PowerBI, or Looker to communicate insights and findings effectively",
                    "Big Data Technologies: Utilizing tools and frameworks such as Spark, Spark SQL, PySpark, HDFS, and MapReduce for processing large datasets efficiently",
                    "Cloud Services: Leveraging cloud platforms like GCP, Azure\/AWS, Databricks, Azure HD Insights, ADF for data storage, processing, and analytics",
                    "Data Querying: Writing advanced SQL queries to extract and manipulate data from relational databases and other data stores",
                    "Data Pipeline Development: Building and optimizing scalable data pipelines and architectures to move and transform data across systems",
                    "Data Transformation: Developing processes for data transformation, structure, metadata, dependency, and workload management",
                    "Enterprise Software Development: Contributing to the development of enterprise-level software products related to data engineering and analytics",
                    "Big Data Data Stores: Implementing and managing highly scalable big data stores to efficiently store and access large volumes of data",
                    "Data Value Extraction: Manipulating, processing, and extracting value from large, diverse datasets to drive business decisions and innovation",
                    "Cloud Services: Experience leveraging cloud platforms like GCP, Azure\/AWS, Databricks, Azure HD Insights, ADF for data storage, processing, and analytics",
                    "We'll meet in person on a regular and purposeful basis",
                    "That means understanding, respecting, and valuing unique styles, experiences, identities, ideas, and opinions \u2013 while being inclusive of all people"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"ZipRecruiter",
                "link":"https:\/\/www.ziprecruiter.com\/c\/Sams-Club\/Job\/Data-Engineer-III\/-in-Springdale,AR?jid=f06f637bea60ef59&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"The Muse",
                "link":"https:\/\/www.themuse.com\/jobs\/walmart\/usa-data-engineer-iii-6724ac?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Jobs Trabajo.org",
                "link":"https:\/\/us.trabajo.org\/job-3642-e33a8e72d810aed0517f743be00dcd1a?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Full-time and Part-time"
    },
    {
        "title":"Data Engineer & BI Expert \u2013 Data Scientist (Data Pipeline & Reporting)",
        "company_name":"Horizontal Talent",
        "location":"San Francisco, CA",
        "description":"Data Engineer & BI Expert \u2013 Data Scientist (Data Pipeline & Reporting)\n\nAbout the Role\n\nWe are hiring a Data Scientist with strong data engineering and business intelligence skills to build robust data pipelines and reporting systems. This role focuses on automating reporting workflows, creating dashboards, and ensuring data quality for enterprise analytics.\n\nKey Responsibilities\n\nDesign and maintain ETL pipelines to support reporting and analytics.\n\nDevelop and automate dashboards using BI tools (Power BI, Tableau, Looker).\n\nEnsure data quality, consistency, and reliability across reporting systems.\n\nCollaborate with business teams to understand reporting needs and deliver actionable insights.\n\nOptimize data workflows for scalability and performance.\n\nImplement CI\/CD principles and manage code repositories using GitHub Enterprise.\n\nRequired Qualifications\n\nStrong experience with SQL, Python, and data pipeline tools (Databricks Jobs).\n\nProficiency in BI platforms, dashboard development.\n\nUnderstanding of data gateways, report connection concepts and cloud platforms (Azure).\n\nFamiliarity with CI\/CD principles and GitHub Enterprise.\n\nSelf-starter with an ownership mindset and ability to work independently.\n\nPreferred Qualifications\n\n2+ years experience in retail or enterprise reporting environments.\n\nExposure to Databricks and PySpark for data processing.\n\nFamiliarity with Agile development environments.\n\nHorizontal facilitates valuable and productive conversations between you and potential employers. We can assist you in growing your career by partnering you with employers that offer challenging assignments. For those that join the team, we offer competitive compensation and benefits including medical, dental, vision, and retirement. Check out all we have to offer and how you can become part of the Horizontal Talent Team. The pay range for this role is $30 - $62 per hour. This is not a guarantee of compensation, as final offer amount may vary based on factors including but not limited to experience and geographic location.",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "Strong experience with SQL, Python, and data pipeline tools (Databricks Jobs)",
                    "Proficiency in BI platforms, dashboard development",
                    "Understanding of data gateways, report connection concepts and cloud platforms (Azure)",
                    "Familiarity with CI\/CD principles and GitHub Enterprise",
                    "Self-starter with an ownership mindset and ability to work independently",
                    "2+ years experience in retail or enterprise reporting environments",
                    "Exposure to Databricks and PySpark for data processing",
                    "Familiarity with Agile development environments",
                    "Horizontal facilitates valuable and productive conversations between you and potential employers"
                ]
            },
            {
                "title":"Benefits",
                "items":[
                    "We can assist you in growing your career by partnering you with employers that offer challenging assignments",
                    "For those that join the team, we offer competitive compensation and benefits including medical, dental, vision, and retirement",
                    "The pay range for this role is $30 - $62 per hour",
                    "This is not a guarantee of compensation, as final offer amount may vary based on factors including but not limited to experience and geographic location"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "We are hiring a Data Scientist with strong data engineering and business intelligence skills to build robust data pipelines and reporting systems",
                    "This role focuses on automating reporting workflows, creating dashboards, and ensuring data quality for enterprise analytics",
                    "Design and maintain ETL pipelines to support reporting and analytics",
                    "Develop and automate dashboards using BI tools (Power BI, Tableau, Looker)",
                    "Ensure data quality, consistency, and reliability across reporting systems",
                    "Collaborate with business teams to understand reporting needs and deliver actionable insights",
                    "Optimize data workflows for scalability and performance",
                    "Implement CI\/CD principles and manage code repositories using GitHub Enterprise"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"Horizontal Talent",
                "link":"https:\/\/www.horizontaltalent.com\/job-board\/72422?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Contractor"
    },
    {
        "title":"GCP Data Engineer",
        "company_name":"Infosys",
        "location":"Blue Ridge, TX",
        "description":"Overview\n\nInfosys is seeking a Google Cloud (GCP) data engineer with experience in Github and python . In this role, you will enable digital transformation for our clients in a global delivery model, research on technologies independently, recommend appropriate solutions and contribute to technology-specific best practices and standards. You will be responsible to interface with key stakeholders and apply your technical proficiency across different stages of the Software Development Life Cycle. You will be part of a learning culture, where teamwork and collaboration are encouraged, excellence is rewarded, and diversity is respected and valued.\nResponsibilities\n\nThe job description text describes responsibilities as part of the role: interface with key stakeholders and apply technical proficiency across different stages of the Software Development Life Cycle in a global delivery model, enabling digital transformation and contributing to technology-specific best practices and standards.\nRequired Qualifications Candidate must be located within commuting distance ofRichardson, TX or be willing to relocate to the area. This position may require travel in the US Bachelors degree or foreign equivalent required from an accredited institution. Will also consider three years of progressive experience in the specialty in lieu of every year of education. Candidates authorized to work for any employer in the United States without employer-based visa sponsorship are welcome to apply. Infosys is unable to provide immigration sponsorship for this role at thistime At least 4years of Information Technology experience. Experience working with technologies like GCP with data engineering data flow \/ air flow, pub sub\/ kafta, data proc\/Hadoop, Big Query. ETL development experience with strong SQL background such as Python\/R, Scala, Java, Hive, Spark, Kafka Strongknowledge on Python Program development to build reusable frameworks, enhance existing frameworks. Application build experience with core GCP Services like Dataproc, GKE, Composer, Deep understanding GCP IAM & Github. Musthave done IAM set up Knowledge onCICD pipeline using Terraform in Git. Preferred Qualifications Good knowledge onGoogle Big Query, using advance SQL programing techniques to build Big Query Data sets in Ingestion and Transformation layer. Experience in Relational Modeling, Dimensional Modeling and Modeling of Unstructured Data Knowledge on Airflow Dag creation, execution, and monitoring. Good understanding of Agile software development frameworks Ability to work in teams in a diverse, multi-stakeholder environment comprising of Business and Technology teams. Experience and desire towork in a global deliveryenvironment.\n\nThe job may entail extensive travel. The job may also entail sitting as well as working at a computer for extended periods of time. Candidates should be able to effectively communicate by telephone, email, and face to face.\nEEO\/About Us\n\nInfosys is a global leader in next-generation digital services and consulting. We enable clients in more than 50 countries to navigate their digital transformation. With over four decades of experience in managing the systems and workings of global enterprises, we expertly steer our clients through their digital journey. We do it by enabling the enterprise with an AI-powered core that helps prioritize the execution of change. We also empower the business with agile digital at scale to deliver unprecedented levels of performance and customer delight. Our always-on learning agenda drives their continuous improvement through building and transferring digital skills, expertise, and ideas from our innovation ecosystem.\n\nInfosys provides equal employment opportunities to applicants and employees without regard to race; color; sex; gender identity; sexual orientation; religious practices and observances; national origin; pregnancy, childbirth, or related medical conditions; status as a protected veteran or spouse\/family member of a protected veteran; or disability.\n\nCountry\n\nUSA\n\nState \/ Region \/ Province\n\nTexas\n\nWork Location\n\nRichardson, TX\n\nInterest Group\n\nInfosys Limited\n\nDomain\n\nRetail ,CPG and logistics\n\nSkillset\n\nTechnology|Cloud Platform|GCP Core Services\n\nCompany\n\nITL USA\n\nRole Designation\n\n835ATHLDUS Technology Lead\n#J-18808-Ljbffr",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "Required Qualifications Candidate must be located within commuting distance of",
                    "This position may require travel in the US Bachelors degree or foreign equivalent required from an accredited institution",
                    "Will also consider three years of progressive experience in the specialty in lieu of every year of education",
                    "Candidates authorized to work for any employer in the United States without employer-based visa sponsorship are welcome to apply",
                    "Infosys is unable to provide immigration sponsorship for this role at thistime At least 4years of Information Technology experience",
                    "Experience working with technologies like GCP with data engineering data flow \/ air flow, pub sub\/ kafta, data proc\/Hadoop, Big Query",
                    "ETL development experience with strong SQL background such as Python\/R, Scala, Java, Hive, Spark, Kafka Strongknowledge on Python Program development to build reusable frameworks, enhance existing frameworks",
                    "Application build experience with core GCP Services like Dataproc, GKE, Composer, Deep understanding GCP IAM & Github",
                    "Musthave done IAM set up Knowledge onCICD pipeline using Terraform in Git",
                    "Google Big Query, using advance SQL programing techniques to build Big Query Data sets in Ingestion and Transformation layer",
                    "Experience in Relational Modeling, Dimensional Modeling and Modeling of Unstructured Data Knowledge on Airflow Dag creation, execution, and monitoring",
                    "Good understanding of Agile software development frameworks Ability to work in teams in a diverse, multi-stakeholder environment comprising of Business and Technology teams",
                    "Experience and desire towork in a global deliveryenvironment",
                    "The job may entail extensive travel",
                    "Candidates should be able to effectively communicate by telephone, email, and face to face"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "In this role, you will enable digital transformation for our clients in a global delivery model, research on technologies independently, recommend appropriate solutions and contribute to technology-specific best practices and standards",
                    "You will be responsible to interface with key stakeholders and apply your technical proficiency across different stages of the Software Development Life Cycle",
                    "You will be part of a learning culture, where teamwork and collaboration are encouraged, excellence is rewarded, and diversity is respected and valued",
                    "The job description text describes responsibilities as part of the role: interface with key stakeholders and apply technical proficiency across different stages of the Software Development Life Cycle in a global delivery model, enabling digital transformation and contributing to technology-specific best practices and standards",
                    "The job may also entail sitting as well as working at a computer for extended periods of time"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"WhatJobs",
                "link":"https:\/\/www.whatjobs.com\/jobs\/gcp-data-engineer?id=2254700987&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Full-time"
    },
    {
        "title":"LL02-251107 EMS Data Engineer",
        "company_name":"Validation & Engineering Group",
        "location":"Rockville, MD",
        "description":"Validation & Engineering Group Inc. (V&EG) is a leading services supplier who provides solutions for the Pharmaceutical Biotechnology Chemical Food and Medical Devices industries in the following areas : Laboratory Compliance Computer Engineering Project Management Validation and other services.\n\nWe are seeking a talented dedicated individual committed to work under the highest ethics standards for the following position :\n\nEMS \/ Data Engineer\n\nRole Summary :\n\nThe EMS (Environmental Monitoring System) \/ Data Engineer ensures reliable data flow reporting and analysis across the Energy Management System. This role maintains high data integrity uptime and reporting accuracy across meters PLCs and supervisory systems.\n\nKey Responsibilities :\n\nManage data collection between PLCs EMS servers and reporting dashboards.\n\nMonitor and maintain 99% data uptime and tag validity across systems.\n\nDevelop and maintain data queries reporting scripts and dashboards.\n\nTroubleshoot time synchronization timestamp drift and missing tag issues.\n\nSupport monthly energy reporting KPI dashboards and cost-saving analyses.\n\nCollaborate with automation and IT teams to ensure secure and validated data pipelines.\n\nQualifications :\n\nBachelors degree in Computer Electrical or Industrial Engineering.\n\n35 years of experience with FactoryTalk Historian PI or similar EMS tools.\n\nStrong SQL and data visualization skills (Power BI Tableau).\n\nUnderstanding of OPC Modbus BACnet and network protocols.\n\nExperience in manufacturing energy management preferred.\n\nKey Skills\n\nApache Hive,S3,Hadoop,Redshift,Spark,AWS,Apache Pig,NoSQL,Big Data,Data Warehouse,Kafka,Scala\n\nEmployment Type : Full Time\n\nExperience : years\n\nVacancy : 1",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "Bachelors degree in Computer Electrical or Industrial Engineering",
                    "35 years of experience with FactoryTalk Historian PI or similar EMS tools",
                    "Strong SQL and data visualization skills (Power BI Tableau)",
                    "Understanding of OPC Modbus BACnet and network protocols"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "The EMS (Environmental Monitoring System) \/ Data Engineer ensures reliable data flow reporting and analysis across the Energy Management System",
                    "This role maintains high data integrity uptime and reporting accuracy across meters PLCs and supervisory systems",
                    "Manage data collection between PLCs EMS servers and reporting dashboards",
                    "Monitor and maintain 99% data uptime and tag validity across systems",
                    "Develop and maintain data queries reporting scripts and dashboards",
                    "Troubleshoot time synchronization timestamp drift and missing tag issues",
                    "Support monthly energy reporting KPI dashboards and cost-saving analyses",
                    "Collaborate with automation and IT teams to ensure secure and validated data pipelines"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"Talent.com",
                "link":"https:\/\/www.talent.com\/view?id=25d25ddbe1ec&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"WhatJobs",
                "link":"https:\/\/www.whatjobs.com\/jobs\/big-data?id=2253365197&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Jobilize",
                "link":"https:\/\/www.jobilize.com\/job\/us-md-rockville-ems-data-engineer-validation-engineering-group-hiring?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Full-time"
    },
    {
        "title":"Junior Data Engineer",
        "company_name":"Systems Planning and Analysis",
        "location":"United States",
        "description":"Overview\n\nIntrepid, an SPA Company, brings more than 20 years of experience supporting the Department of Defense and U.S. Government, consistently setting the standard for excellence in the federal marketplace. Committed to advancing the mission of the U.S. Warfighter, Intrepid leverages technological superiority to deliver innovative solutions across air, space, land, and sea domains. We are proud to foster a collaborative, dynamic work environment, offering competitive compensation and an industry-leading 401k contribution. Our team is built through merit and achievement, and we\u2019re always looking for the best and brightest to join us in our growth. We treat our people like family, we are mission-focused, and we give back! Join us today.\n\nSPA has a need for a Junior Data Engineer position with Databrick experience.\n\nResponsibilities\n\u2022 Sustaining data feeds between Army systems and target Joint data platforms.\n\u2022 New data feeds between Army systems and target Joint data platforms.\n\u2022 Documentation in Army identified project management software (e.g., Confluence) on implemented data feeds.\n\u2022 Developing and maintaining data interfaces from source Army FM systems to Joint data platforms.\n\u2022 Scheduling data pipeline operations and maintenance.\n\u2022 Automating pipelines and configuring data loads.\n\u2022 Implementing data modeling as identified and developed by project requirements.\n\u2022 Coordinating with user community for improvements and bug fixes as required.\n\u2022 Maintaining and configuring structured databases in cloud environment.\n\u2022 Other duties as assigned.\n\nQualifications\n\u2022 A bachelor's degree\n\u2022 3-5 years' relevant experience\n\u2022 Experience with Python \/ pyspark and SQL coding\n\u2022 Experience with Databricks\n\u2022 Experience with developing interfaces between systems\n\u2022 Basic knowledge of Palantir tool suite\n\u2022 US Citizenship and an active SECRET security clearance",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "SPA has a need for a Junior Data Engineer position with Databrick experience",
                    "A bachelor's degree",
                    "3-5 years' relevant experience",
                    "Experience with Python \/ pyspark and SQL coding",
                    "Experience with Databricks",
                    "Experience with developing interfaces between systems",
                    "Basic knowledge of Palantir tool suite",
                    "US Citizenship and an active SECRET security clearance"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "Sustaining data feeds between Army systems and target Joint data platforms",
                    "New data feeds between Army systems and target Joint data platforms",
                    "Documentation in Army identified project management software (e.g., Confluence) on implemented data feeds",
                    "Developing and maintaining data interfaces from source Army FM systems to Joint data platforms",
                    "Scheduling data pipeline operations and maintenance",
                    "Automating pipelines and configuring data loads",
                    "Implementing data modeling as identified and developed by project requirements",
                    "Coordinating with user community for improvements and bug fixes as required",
                    "Maintaining and configuring structured databases in cloud environment",
                    "Other duties as assigned"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"Careers - SPA",
                "link":"https:\/\/talent.spa.com\/jobs\/20589?lang=en-us&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Workday",
                "link":"https:\/\/audubon.wd5.myworkdayjobs.com\/en-US\/Audubon\/job\/New-York-NY\/Junior-Data-Engineer_JR624?q=senior+Director&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Jobs By Workable",
                "link":"https:\/\/apply.workable.com\/mod-op\/j\/388BF537E9?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Jobs",
                "link":"https:\/\/jobs.ashbyhq.com\/cylinderhealth\/a75dbad9-203f-47ca-b96d-2f0d3a309034?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Indeed",
                "link":"https:\/\/www.indeed.com\/viewjob?jk=2ac039ced2598c08&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Glassdoor",
                "link":"https:\/\/www.glassdoor.com\/job-listing\/junior-data-engineer-itransition-JV_KO0,20_KE21,32.htm?jl=1009796417162&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"LinkedIn",
                "link":"https:\/\/www.linkedin.com\/jobs\/view\/junior-data-engineer-at-contexture-4312351208?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Himalayas.app",
                "link":"https:\/\/himalayas.app\/companies\/blue-coding\/jobs\/junior-data-engineer-v?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Full-time"
    },
    {
        "title":"Data Engineer - Senior",
        "company_name":"Royce Geo",
        "location":"Arnold, MO",
        "description":"Job Description\n\nWhy Choose Royce Geo, a GRVTY Company\n\nRoyce Geo, a GRVTY Company, started with a simple, American idea: we do things not because they are easy but because they are hard. Royce Geo, a GRVTY Company, exists to answer challenges. We do it for customers in defense, intelligence, homeland security\u2014anyone whose job it is to advance America\u2019s strategic position. The size of the challenge we face demands new skills, new backgrounds, and new thinking. That\u2019s what we\u2019re here to deliver. And when you work shoulder to shoulder with brilliant people tackling the most high-stakes challenges, it\u2019s invigorating. Our culture is built on collaboration, mission-focused innovation, and a commitment to excellence, where every challenge we answer opens the door to a new possibility.\n\nThe toughest national security challenges demand vision and ingenuity, not just resources. We deliver mission and technical expertise to outpace our adversaries. We\u2019re purpose-built to tackle the most entrenched, systemic national security issues around the world.\n\nWe partner with our customers to help them overcome challenges in every corner of technology and defense\u2014including the ones still being explored. Our growing capabilities create complementary advantages, giving on-the-ground operations the edge they need to succeed. We muster everything we have to answer every challenge presented, every day of our lives.\n\nAt Royce Geo, a GRVTY Company, we believe that when our employees thrive, our company thrives. That\u2019s why we offer a comprehensive and competitive benefits package designed to support your well-being, growth, and work-life balance.\n\u2022 Robust health plan including medical, dental, and vision\n\u2022 Health Savings Account with company contribution\n\u2022 Annual Paid Time Off and Paid Holidays\n\u2022 Paid Parental Leave\n\u2022 401k with generous company match\n\u2022 Training and Development Opportunities\n\u2022 Award Programs\n\u2022 Variety of Company Sponsored Events\n\u2022 **Requires an active TS\/SCI Security Clearance with the ability to obtain a CI Poly****\n\nWe are seeking an experienced Data Engineer to join our team. As a Data Engineer, you will be responsible for collecting data, interpretation of data for business analysis. Candidates for this position will be able to sift through data, apply statistics, compare data points, and create products for external customers. The data engineer will provide extensive technical expertise and develop innovative solutions to complex problems. We want innovative problem solvers that are passionate about data and enjoy a challenge.\n\nResponsibilities:\n\u2022 Creates database models and components that meet product specification and development schedules for customer\/customers\n\u2022 Participates in large system and subsystem planning\n\u2022 Designs data pipelines, data models, profiling\/cleansing the data, and performance tuning\n\u2022 Develops and maintains data pipelines, data workflows, ETL\/ELT scripts or packages\n\u2022 Prepares comprehensive test plans\n\u2022 Collaborates and provides influence to team on project deliverables\n\u2022 Acts as a technical resource for lower-level engineers\n\u2022 Researches and integrates design strategies, product specifications, development schedules, and user expectations into product capabilities\n\u2022 Develops technical designs and specifications for complex data pipelines\/data flows for customer\/customers\n\u2022 Uses ETL tools or languages to build, test, and maintain product modules, components, and subsystems\n\u2022 Creates quality deliverables for customers\n\u2022 Drives full life-cycle of services\/solution delivery for project(s)\n\u2022 Oversees technical design, development, and implementation of large projects and\/or major data pipelines\/data flows and solutions for customer\/customers\n\u2022 Identifies data gaps and potential remediation or integration activity for consideration by PM and\/or customer\n\u2022 Responsible for services\/solution delivery and customer satisfaction of program(s)\n\u2022 Sets strategy and enforces quality assurance program for program and project deliverables\n\u2022 Provides leadership to team members on program and project deliverables\n\nRequired Qualifications:\n\u2022 Active TS clearance is required with the ability to obtain a CI Poly\n\u2022 10+ years of relevant experience. (A combination of years of experience & professional certifications\/trainings can be used in lieu of years of experience)\n\u2022 Ability to understand requirements and map them into the existing data environment or create new structures needed\n\u2022 Experience with SQL, Python, PySpark, leading ETL technologies and approaches\n\u2022 Independent, creative, and determined \u2022 Strong SQL and data management experience\n\u2022 Experience with machine learning and statistical analysis\n\u2022 Experience with modern programming languages such as Python\n\u2022 Strong analytical skills with the ability to organize, analyze and prioritize\n\u2022 Experience or exposure to one or more: Redshift, Teradata, Snowflake\n\nDesired Qualifications:\n\u2022 Familiarity with NGA data holdings\n\u2022 Familiarity with Geospatial database structures and tools\n\u2022 Familiarity with Metadata management tools like Immuta \/ Alation \/ Collibra\n\nEEO Statement\n\nRoyce Geo, A GRVTY Company, is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran and will not be discriminated against on the basis of disability.\n\nAnyone requiring reasonable accommodations should email recruiting@grvty.com or call 703-544-7930 with requested details. A member of the HR team will respond to your request within 2 business days.\n\nKnow Your Rights: Workplace Discrimination is Illegal (eeoc.gov)\n\nmediate fit, we will also keep your resume in our database for future opportunities.\n,\nAbout Royce Geo\n\nGRVTY is a defense technology company. Our automated ISR&T platforms, software and data solutions help our defense, intelligence and homeland security customers turn insight into action faster and with confidence.",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "**Requires an active TS\/SCI Security Clearance with the ability to obtain a CI Poly****",
                    "We are seeking an experienced Data Engineer to join our team",
                    "We want innovative problem solvers that are passionate about data and enjoy a challenge",
                    "Active TS clearance is required with the ability to obtain a CI Poly",
                    "10+ years of relevant experience",
                    "(A combination of years of experience & professional certifications\/trainings can be used in lieu of years of experience)",
                    "Ability to understand requirements and map them into the existing data environment or create new structures needed",
                    "Experience with SQL, Python, PySpark, leading ETL technologies and approaches",
                    "Independent, creative, and determined",
                    "Strong SQL and data management experience",
                    "Experience with machine learning and statistical analysis",
                    "Experience with modern programming languages such as Python",
                    "Strong analytical skills with the ability to organize, analyze and prioritize",
                    "Experience or exposure to one or more: Redshift, Teradata, Snowflake"
                ]
            },
            {
                "title":"Benefits",
                "items":[
                    "That\u2019s why we offer a comprehensive and competitive benefits package designed to support your well-being, growth, and work-life balance",
                    "Robust health plan including medical, dental, and vision",
                    "Health Savings Account with company contribution",
                    "Annual Paid Time Off and Paid Holidays",
                    "Paid Parental Leave",
                    "401k with generous company match",
                    "Training and Development Opportunities",
                    "Award Programs",
                    "Variety of Company Sponsored Events"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "As a Data Engineer, you will be responsible for collecting data, interpretation of data for business analysis",
                    "Candidates for this position will be able to sift through data, apply statistics, compare data points, and create products for external customers",
                    "The data engineer will provide extensive technical expertise and develop innovative solutions to complex problems",
                    "Creates database models and components that meet product specification and development schedules for customer\/customers",
                    "Participates in large system and subsystem planning",
                    "Designs data pipelines, data models, profiling\/cleansing the data, and performance tuning",
                    "Develops and maintains data pipelines, data workflows, ETL\/ELT scripts or packages",
                    "Prepares comprehensive test plans",
                    "Collaborates and provides influence to team on project deliverables",
                    "Acts as a technical resource for lower-level engineers",
                    "Researches and integrates design strategies, product specifications, development schedules, and user expectations into product capabilities",
                    "Develops technical designs and specifications for complex data pipelines\/data flows for customer\/customers",
                    "Uses ETL tools or languages to build, test, and maintain product modules, components, and subsystems",
                    "Creates quality deliverables for customers",
                    "Drives full life-cycle of services\/solution delivery for project(s)",
                    "Oversees technical design, development, and implementation of large projects and\/or major data pipelines\/data flows and solutions for customer\/customers",
                    "Identifies data gaps and potential remediation or integration activity for consideration by PM and\/or customer",
                    "Responsible for services\/solution delivery and customer satisfaction of program(s)",
                    "Sets strategy and enforces quality assurance program for program and project deliverables",
                    "Provides leadership to team members on program and project deliverables"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"Indeed",
                "link":"https:\/\/www.indeed.com\/viewjob?jk=7b4eea823d419624&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"SimplyHired",
                "link":"https:\/\/www.simplyhired.com\/job\/gtPI1WJWokRk70jhXGnW847F5LWqYsKlakMdazDGHKFV67TwYP28WA?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Glassdoor",
                "link":"https:\/\/www.glassdoor.com\/job-listing\/data-engineer-senior-the-royce-JV_IC1131193_KO0,20_KE21,30.htm?jl=1009919685526&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Ladders",
                "link":"https:\/\/www.theladders.com\/job\/data-engineer-senior-royce-geospatial-consultants-inc-arnold-mo_84119544?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Jobilize",
                "link":"https:\/\/www.jobilize.com\/job\/us-mo-arnold-data-engineer-senior-royce-geospatial-consultants-hiring?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Full-time"
    },
    {
        "title":"Sr. Manager- Data Engineering",
        "company_name":"Adtalem",
        "location":"Lisle, IL",
        "description":"Company Description\n\nAbout Adtalem Global Education\n\nAdtalem Global Education is a national leader in post-secondary education and leading provider of professional talent to the healthcare industry. Adtalem educates and empowers students with the knowledge and skills to become leaders in their communities and make a lasting impact on public health, well-being and beyond. Through equitable access to education, environments that nurture student success, and a focus on expanding and diversifying the talent pipeline in healthcare, Adtalem is building a brighter future for communities and the world.\n\nAdtalem is the parent organization of American University of the Caribbean School of Medicine, Chamberlain University, Ross University School of Medicine, Ross University School of Veterinary Medicine and Walden University.\n\nWe operate on a hybrid schedule with four in-office days per week (Monday\u2013Thursday). This approach enhances creativity, innovation, communication, and relationship-building, fostering a dynamic and collaborative work environment.\n\nVisit Adtalem.com for more information, and follow on Twitter and LinkedIn!\n\nJob Description\n\nOpportunity at a Glance\n\nAdtalem is a data driven organization. The Data Engineering team builds data solutions that powers strategic and tactical business decisions and supports the Analytics and Artificial Intelligence operations. By implementing data platform, data pipelines and data governance policies this team provides the basis for decision-making in Adtalem. Adtalem is looking for a Senior Manager, Data Engineering who will lead overall technical design, development, modification, and implementation of data solutions using existing and emerging technology platforms. This person will own the development as well as the continual improvement of the data platform for an organization that has the goal to drive best in class student outcomes in the industry.\n\nResponsibilities\n\u2022 You'll design and build trusted, reliable and timely datasets, metrics and data pipelines that are critical to the direction of the company\n\u2022 Build and lead a high-performing data engineering team in a hands-on technical capacity\n\u2022 Be responsible for shaping how we acquire, collect and leverage data\n\u2022 You will define and manage SLA's for all data sets and processes running in production\n\u2022 Work closely with Product Managers, Analysts, Data Scientists to develop and own data-driven systems\n\u2022 Develop data democratization layer for self-served reporting\n\u2022 Assist development team in troubleshooting, coding, testing, implementation, and documenting solutions\n\u2022 Support, grow, mentor and inspire new and existing team members\n\u2022 Be a key leader of the Data and Analytics team, working to propel the business towards being more data-driven\n\u2022 Performs other duties as assigned\n\u2022 Complies with all policies and standards\n\nQualifications\n\u2022 Bachelor's Degree Computer Science, Computer Engineering, Software Engineering, or other related technical field\n\u2022 Master's Degree Computer Science, Computer Engineering, Software Engineering, or other related technical field Computer Science, Computer Engineering, Software Engineering, or other related technical field\n\u2022 8+ years experience in data engineering solutions such as data platforms, data ingestion, data management, or publication\/analytics\n\u2022 3+ years *leadership experience* building and managing a high performing teams.\n\u2022 2+ years experience in Google Cloud with services like BigQuery, Composer, GCS, DataStream, Dataflows\n\u2022 Progressively responsible experience starting as Data Engineer and advancement in complexity, and level of responsibility\n\u2022 Must be highly analytical having the proven ability to develop and reverse complex engineering solutions. Critical thinking and advanced problem-solving skills are core behaviors among the team\n\u2022 Excellent oral\/written communication and presentation skills which bring clarity and precision to help management understand and visualize complex material to support organizational decision-making.\n\u2022 Experience in objectively evaluating current processes for opportunities to optimize inter departmental communication, collaboration, and end-to-end process performance\n\u2022 Have proven experience defining a roadmap and managing incremental execution through successful launches.\n\u2022 Ability to manage multiple, competing priorities\n\u2022 Must thrive in an Agile, fast-paced, constantly changing environment\n\u2022 Provide mentorship to data engineers\n\u2022 Developed technology target state, roadmaps that aligned to Short- and Long-term business goals.\n\u2022 TRAVEL REQUIREMENTS - Up to 5%\n\nAdditional Information\n\nIn support of the pay transparency laws enacted across the country, the expected salary range for this position is between $96404.1 and $169021.85. Actual pay will be adjusted based on job-related factors permitted by law, such as experience and training; geographic location; licensure and certifications; market factors; departmental budgets; and responsibility. Our Talent Acquisition Team will be happy to answer any questions you may have, and we look forward to learning more about your salary requirements. The position qualifies for the below benefits.\n\nAdtalem offers a robust suite of benefits including:\n\u2022 Health, dental, vision, life and disability insurance\n\u2022 401k Retirement Program + 6% employer match\n\u2022 Participation in Adtalem\u2019s Flexible Time Off (FTO) Policy\n\u2022 12 Paid Holidays\n\nFor more information related to our benefits please visit: https:\/\/careers.adtalem.com\/benefits.\n\nYou are also eligible to participate in an annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance.\n\nEqual Opportunity \u2013 Minority \/ Female \/ Disability \/ V \/ Gender Identity \/ Sexual Orientation",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "Bachelor's Degree Computer Science, Computer Engineering, Software Engineering, or other related technical field",
                    "Master's Degree Computer Science, Computer Engineering, Software Engineering, or other related technical field Computer Science, Computer Engineering, Software Engineering, or other related technical field",
                    "8+ years experience in data engineering solutions such as data platforms, data ingestion, data management, or publication\/analytics",
                    "3+ years *leadership experience* building and managing a high performing teams",
                    "2+ years experience in Google Cloud with services like BigQuery, Composer, GCS, DataStream, Dataflows",
                    "Progressively responsible experience starting as Data Engineer and advancement in complexity, and level of responsibility",
                    "Must be highly analytical having the proven ability to develop and reverse complex engineering solutions",
                    "Critical thinking and advanced problem-solving skills are core behaviors among the team",
                    "Excellent oral\/written communication and presentation skills which bring clarity and precision to help management understand and visualize complex material to support organizational decision-making",
                    "Experience in objectively evaluating current processes for opportunities to optimize inter departmental communication, collaboration, and end-to-end process performance",
                    "Have proven experience defining a roadmap and managing incremental execution through successful launches",
                    "Ability to manage multiple, competing priorities",
                    "Must thrive in an Agile, fast-paced, constantly changing environment",
                    "TRAVEL REQUIREMENTS - Up to 5%"
                ]
            },
            {
                "title":"Benefits",
                "items":[
                    "In support of the pay transparency laws enacted across the country, the expected salary range for this position is between $96404.1 and $169021.85",
                    "Actual pay will be adjusted based on job-related factors permitted by law, such as experience and training; geographic location; licensure and certifications; market factors; departmental budgets; and responsibility",
                    "Adtalem offers a robust suite of benefits including:",
                    "Health, dental, vision, life and disability insurance",
                    "401k Retirement Program + 6% employer match",
                    "Participation in Adtalem\u2019s Flexible Time Off (FTO) Policy",
                    "12 Paid Holidays",
                    "For more information related to our benefits please visit: https:\/\/careers.adtalem.com\/benefits",
                    "You are also eligible to participate in an annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "This approach enhances creativity, innovation, communication, and relationship-building, fostering a dynamic and collaborative work environment",
                    "The Data Engineering team builds data solutions that powers strategic and tactical business decisions and supports the Analytics and Artificial Intelligence operations",
                    "By implementing data platform, data pipelines and data governance policies this team provides the basis for decision-making in Adtalem",
                    "Adtalem is looking for a Senior Manager, Data Engineering who will lead overall technical design, development, modification, and implementation of data solutions using existing and emerging technology platforms",
                    "This person will own the development as well as the continual improvement of the data platform for an organization that has the goal to drive best in class student outcomes in the industry",
                    "You'll design and build trusted, reliable and timely datasets, metrics and data pipelines that are critical to the direction of the company",
                    "Build and lead a high-performing data engineering team in a hands-on technical capacity",
                    "Be responsible for shaping how we acquire, collect and leverage data",
                    "You will define and manage SLA's for all data sets and processes running in production",
                    "Work closely with Product Managers, Analysts, Data Scientists to develop and own data-driven systems",
                    "Develop data democratization layer for self-served reporting",
                    "Assist development team in troubleshooting, coding, testing, implementation, and documenting solutions",
                    "Support, grow, mentor and inspire new and existing team members",
                    "Be a key leader of the Data and Analytics team, working to propel the business towards being more data-driven",
                    "Performs other duties as assigned",
                    "Complies with all policies and standards",
                    "Provide mentorship to data engineers",
                    "Developed technology target state, roadmaps that aligned to Short- and Long-term business goals"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"Adtalem Careers - Adtalem Global Education",
                "link":"https:\/\/careers.adtalem.com\/job\/sr-manager-data-engineering-in-lisle-il-jid-6419?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Smart Recruiters Jobs",
                "link":"https:\/\/jobs.smartrecruiters.com\/AdtalemGlobalEducation\/744000079569251-sr-manager-data-engineering?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Indeed",
                "link":"https:\/\/www.indeed.com\/viewjob?jk=f96b922beafea2a2&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"ZipRecruiter",
                "link":"https:\/\/www.ziprecruiter.com\/c\/Adtalem-Global-Education\/Job\/Sr.-Manager-Data-Engineering\/-in-Lisle,IL?jid=4b733e7c6b411afc&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"LinkedIn",
                "link":"https:\/\/www.linkedin.com\/jobs\/view\/sr-manager-data-engineering-at-adtalem-global-education-4294680438?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Career.io",
                "link":"https:\/\/career.io\/job\/sr-manager-data-engineering-lisle-adtalem-global-education-inc-d40980a191e07615aba0f118f4450df9?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Teal",
                "link":"https:\/\/www.tealhq.com\/job\/sr-manager-data-engineering_f9487fa3-4c46-421b-b698-ae6b7c192b43?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Monster",
                "link":"https:\/\/www.monster.com\/job-openings\/sr-manager-data-engineering-lisle-il--c32cc2fe-d841-4cc7-a4d3-b2930439eb08?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Full-time"
    },
    {
        "title":"Senior\/Lead Data Engineer",
        "company_name":"ExecutivePlacements.com",
        "location":"Plano, TX",
        "description":"Senior Lead Data Engineer\n\nDo you love building and pioneering in the technology space? Do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? At Capital One, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. We are seeking Data Engineers who are passionate about marrying data with emerging technologies. As a Capital One Senior Lead Data Engineer, you'll have the opportunity to be on the forefront of driving a major transformation within Capital One.\n\nWhat You'll Do\n\u2022 Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies\n\u2022 Lead a team of developers with deep experience in machine learning, distributed microservices, and full stack systems\n\u2022 Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake\n\u2022 Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community\n\u2022 Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment\n\u2022 Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance\n\nBasic Qualifications\n\u2022 Bachelor's Degree\n\u2022 At least 6 years of experience in application development (Internship experience does not apply)\n\u2022 At least 2 years of experience in big data technologies\n\u2022 At least 1 year experience with cloud computing (AWS, Microsoft Azure, Google Cloud)\n\nPreferred Qualifications\n\u2022 Master's Degree\n\u2022 9+ years of experience in application development including Python, SQL, Scala, or Java\n\u2022 4+ years of experience with a public cloud (AWS, Microsoft Azure, Google Cloud)\n\u2022 5+ years experience with Distributed data\/computing tools (MapReduce, Hadoop, Hive, EMR, Kafka, Spark, Gurobi, or MySQL)\n\u2022 4+ year experience working on Real Time data and streaming applications\n\u2022 4+ years of experience with NoSQL implementation (Mongo, Cassandra)\n\u2022 4+ years of data warehousing experience (Redshift or Snowflake)\n\u2022 4+ years of experience with UNIX\/Linux including basic commands and Shell Scripting\n\u2022 2+ years of experience with Agile engineering practices\n\nCapital One will consider sponsoring a new qualified applicant for employment authorization for this position.\n\nThe minimum and maximum Full time annual salaries for this role are listed below, by location. Please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount Capital One is willing to pay at the time of this posting. Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked.\n\nMcLean, VA: $225,400 - $257,200 for Sr. Lead Data EngineerPlano, TX: $204,900 - $233,800 for Sr. Lead Data EngineerRichmond, VA: $204,900 - $233,800 for Sr. Lead Data Engineer\n\nCandidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate's offer letter.\n\nThis role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and\/or long term incentives (LTI). Incentives could be discretionary or non discretionary depending on the plan.\n\nCapital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. Learn more at the Capital One Careers website . Eligibility varies based on full or part-time status, exempt or non-exempt status, and management level.\n\nThis role is expected to accept applications for a minimum of 5 business days.No agencies please. Capital One is an equal opportunity employer (EOE, including disability\/vet) committed to non-discrimination in compliance with applicable federal, state, and local laws. Capital One promotes a drug-free workplace. Capital One will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, Article 23-A of the New York Correction Law; San Francisco, California Police Code Article 49, Sections 4901-4920; New York City's Fair Chance Act; Philadelphia's Fair Criminal Records Screening Act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries.\n\nIf you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact Capital One Recruiting at 1-800-304-9102 or via email at (see below) . All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations.\n\nFor technical support or questions about Capital One's recruiting process, please send an email to (see below)\n\nCapital One does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site.\n\nCapital One Financial is made up of several different entities. Please note that any position posted in Canada is for Capital One Canada, any position posted in the United Kingdom is for Capital One Europe and any position posted in the Philippines is for Capital One Philippines Service Corp. (COPSSC).",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "Bachelor's Degree",
                    "At least 6 years of experience in application development (Internship experience does not apply)",
                    "At least 2 years of experience in big data technologies",
                    "At least 1 year experience with cloud computing (AWS, Microsoft Azure, Google Cloud)"
                ]
            },
            {
                "title":"Benefits",
                "items":[
                    "Salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked",
                    "McLean, VA: $225,400 - $257,200 for Sr",
                    "Lead Data EngineerPlano, TX: $204,900 - $233,800 for Sr",
                    "Lead Data EngineerRichmond, VA: $204,900 - $233,800 for Sr",
                    "Candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate's offer letter",
                    "This role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and\/or long term incentives (LTI)",
                    "Incentives could be discretionary or non discretionary depending on the plan",
                    "Capital One offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "Collaborate with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies",
                    "Lead a team of developers with deep experience in machine learning, distributed microservices, and full stack systems",
                    "Utilize programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift and Snowflake",
                    "Share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community",
                    "Collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of Americans achieve financial empowerment",
                    "Perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"LinkedIn",
                "link":"https:\/\/www.linkedin.com\/jobs\/view\/senior-lead-data-engineer-at-executiveplacements-com-4334923479?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"JobServe",
                "link":"https:\/\/www.jobserve.com\/us\/en\/extjob\/SENIOR-LEAD-DATA-ENGINEER-in-Plano-Texas-USA-0B4781581BB2B3A048\/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"ComputerJobs",
                "link":"https:\/\/www.computerjobs.com\/search-jobs-in-Plano,-Texas,-USA\/SENIOR-LEAD-DATA-ENGINEER-220F561501D4648857\/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"CareerBoard",
                "link":"https:\/\/www.careerboard.com\/search-jobs-in-Plano,-Texas,-USA\/SENIOR-LEAD-DATA-ENGINEER-DD2B79DB3AB864C858\/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"JobNet",
                "link":"https:\/\/www.jobnet.com.au\/us\/en\/search-jobs-in-Plano,-Texas,-USA\/SENIOR-LEAD-DATA-ENGINEER-DD2B79DB3AB864C858\/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"BeBee",
                "link":"https:\/\/us.bebee.com\/job\/eb9c599a547c48abb37d228b6699fd58?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"IT Jobs For ColU Fans",
                "link":"https:\/\/itjobs.cu-fc.com\/jobs-for-colu-fans\/SENIOR-LEAD-DATA-ENGINEER-job-in-Plano-Texas-USA\/8f2a72e5b89f9a6117\/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Jobs Trabajo.org",
                "link":"https:\/\/us.trabajo.org\/job-3637-8957dfba1dd2005137e3236aa742b8e3?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Full-time"
    },
    {
        "title":"Data Engineer [On-Site, See Cities in Posting] | Birmingham, AL, USA",
        "company_name":"Regions Financial Corporation",
        "location":"Birmingham, AL",
        "description":"Data Engineer [On-Site, See Cities in Posting]\n\nThank you for your interest in a career at Regions. At Regions, we believe associates deserve more than just a job. We believe in offering performance-driven individuals a place where they can build a career --- a place to expect more opportunities. If you are focused on results, dedicated to quality, strength and integrity, and possess the drive to succeed, then we are your employer of choice.\n\nRegions is dedicated to taking appropriate steps to safeguard and protect private and personally identifiable information you submit. The information that you submit will be collected and reviewed by associates, consultants, and vendors of Regions in order to evaluate your qualifications and experience for job opportunities and will not be used for marketing purposes, sold, or shared outside of Regions unless required by law. Such information will be stored in accordance with regulatory requirements and in conjunction with Regions' Retention Schedule for a minimum of three years. You may review, modify, or update your information by visiting and logging into the careers section of the system.\n\nJob Description:\n\nAt Regions, the Data Engineer supports the Data and Analytics organization by designing, optimizing, and developing data pipelines to support machine learning and artificial intelligence (AI) models. The Data Engineer works with a cross functional team of Data Scientists and Software Developers on feature engineering and scoring pipelines. This role supports software developers, database architects, data analysts, and data scientists on data initiatives and ensures optimal data delivery architecture is consistent throughout ongoing projects.\n\nPrimary Responsibilities\n\n\u2022 Partners with Regions Technology partners to Design, Build, and Maintain the data-based structures and systems in support of Data and Analytics and Data Product use cases\n\u2022 Builds data pipelines to collect and arrange data and manage data storage in Regions' big data environment\n\u2022 Builds robust, testable programs for moving, transforming, and loading data using big data tools such as Spark\n\u2022 Ensures data is prepared, arranged and ready for each defined business use case\n\u2022 Provides consultation to all areas of the organization that plan to use data to make decisions\n\u2022 Supports any team members in the development of such information delivery and aid in the automation of data products\nThis position is exempt from timekeeping requirements under the Fair Labor Standards Act and is not eligible for overtime pay.\n\nRequirements\n\n\u2022 Bachelor's degree and four (4) years of experience in a quantitative\/analytical\/STEM field or technical related field\n\u2022 Or Master's degree and two (2) years of experience in in a quantitative\/analytical\/STEM field\n\u2022 Or Ph.D. in a quantitative\/analytical\/STEM field\n\u2022 One (1) year of working programming experience in Python\/PySpark, Scala, SQL\n\u2022 One (1) year of working experience in Big Data Technology in Hadoop, Hive, Impala, Spark, or Kafka\nPreferences\n\n\u2022 Background in Big Data Engineering and Advanced Data Analytics\n\u2022 Experience developing solutions for the financial services industry\n\u2022 Experience in Agile Software Development\n\u2022 Experience or exposure to cloud technologies and migrations\n\u2022 Prior banking or financial services experience\nSkills and Competencies\n\n\u2022 Experience building data solutions at scale\n\u2022 Experience designing and building relational data structures in multiple environments\n\u2022 Experience with DevOps principals, CI\/CD, and Software Development Lifecycle\n\u2022 Experience with No-SQL databases\n\u2022 Experience working with large-scale data Lakehouses\n\u2022 Proven record of accomplishment of delivering operational Data solutions including Report and Model Ready Data Assets\n\u2022 Significant experience working with senior executives in the use of data, reporting and visualizations to support strategic and operational decision making\n\u2022 Strong ability to transform and integrate complex data from multiple sources into accessible, understandable, and usable data assets and frameworks\n\u2022 Strong background in synthesizing data and analytics in a large (Fortune 500), complex, and highly regulated environment\n\u2022 Strong communication skills through written and oral presentations\n\u2022 Strong technical background including database and business intelligence skills\nThis position is intended to be onsite, now or in the near future . Associates will have regular work hours, including full days in the office three or more days a week. The manager will set the work schedule for this position, including in-office expectations. Regions will not provide relocation assistance for this position, and relocation would be at your expense. The locations available for this role are Birmingham, AL, Atlanta, GA or Charlotte, NC.\n\nRegions will not sponsor applicants for work visas for this position at this time. Applicants for this position must currently be authorized to work in the United States on a full-time basis.\n\nPosition Type\nFull time\n\nCompensation Details\n\nPay ranges are job specific and are provided as a point-of-market reference for compensation decisions. Other factors which directly impact pay for individual associates include: experience, skills, knowledge, contribution, job location and, most importantly, performance in the job role. As these factors vary by individuals, pay will also vary among individual associates within the same job.\n\nThe target information listed below is based on the Metropolitan Statistical Area Market Range for where the position is located and level of the position.\n\nJob Range Target:\n\nMinimum:\n$88,553.85 USD\nMedian:\n$119,210.00 USD\n\nIncentive Pay Plans:\nThis job is not incentive eligible.\n\nBenefits Information\n\nRegions offers a benefits package that is flexible, comprehensive and recognizes that \"one size does not fit all\" for benefits-eligible associates. Listed below is a synopsis of the benefits offered by Regions for informational purposes, which is not intended to be a complete summary of plan terms and conditions.\n\n\u2022 Paid Vacation\/Sick Time\n\u2022 401K with Company Match\n\u2022 Medical, Dental and Vision Benefits\n\u2022 Disability Benefits\n\u2022 Health Savings Account\n\u2022 Flexible Spending Account\n\u2022 Life Insurance\n\u2022 Parental Leave\n\u2022 Employee Assistance Program\n\u2022 Associate Volunteer Program\nPlease note, benefits and plans may be changed, amended, or terminated with respect to all or any class of associate at any time. To learn more about Regions' benefits, please click or copy the link below to your browser.\n\nhttps:\/\/www.regions.com\/about-regions\/welcome-portal\/benefits\n\nLocation Details\nRiverchase Operations Center\n\nLocation:\nHoover, Alabama\n\nEqual Opportunity Employer\/including Disabled\/Veterans\n\nJob applications at Regions are accepted electronically through our career site for a minimum of five business days from the date of posting. Job postings for higher-volume positions may remain active for longer than the minimum period due to business need and may be closed at any time thereafter at the discretion of the company.",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "Bachelor's degree and four (4) years of experience in a quantitative\/analytical\/STEM field or technical related field",
                    "Or Master's degree and two (2) years of experience in in a quantitative\/analytical\/STEM field",
                    "Or Ph.D. in a quantitative\/analytical\/STEM field",
                    "One (1) year of working programming experience in Python\/PySpark, Scala, SQL",
                    "One (1) year of working experience in Big Data Technology in Hadoop, Hive, Impala, Spark, or Kafka",
                    "Background in Big Data Engineering and Advanced Data Analytics",
                    "Experience developing solutions for the financial services industry",
                    "Experience in Agile Software Development",
                    "Experience or exposure to cloud technologies and migrations",
                    "Prior banking or financial services experience",
                    "Experience building data solutions at scale",
                    "Experience designing and building relational data structures in multiple environments",
                    "Experience with DevOps principals, CI\/CD, and Software Development Lifecycle",
                    "Experience with No-SQL databases",
                    "Experience working with large-scale data Lakehouses",
                    "Proven record of accomplishment of delivering operational Data solutions including Report and Model Ready Data Assets",
                    "Significant experience working with senior executives in the use of data, reporting and visualizations to support strategic and operational decision making",
                    "Strong ability to transform and integrate complex data from multiple sources into accessible, understandable, and usable data assets and frameworks",
                    "Strong background in synthesizing data and analytics in a large (Fortune 500), complex, and highly regulated environment",
                    "Strong communication skills through written and oral presentations",
                    "Strong technical background including database and business intelligence skills",
                    "Applicants for this position must currently be authorized to work in the United States on a full-time basis"
                ]
            },
            {
                "title":"Benefits",
                "items":[
                    "Pay ranges are job specific and are provided as a point-of-market reference for compensation decisions",
                    "Other factors which directly impact pay for individual associates include: experience, skills, knowledge, contribution, job location and, most importantly, performance in the job role",
                    "As these factors vary by individuals, pay will also vary among individual associates within the same job",
                    "$88,553.85 USD",
                    "$119,210.00 USD",
                    "Incentive Pay Plans:",
                    "This job is not incentive eligible",
                    "Regions offers a benefits package that is flexible, comprehensive and recognizes that \"one size does not fit all\" for benefits-eligible associates",
                    "Paid Vacation\/Sick Time",
                    "401K with Company Match",
                    "Medical, Dental and Vision Benefits",
                    "Disability Benefits",
                    "Health Savings Account",
                    "Flexible Spending Account",
                    "Life Insurance",
                    "Parental Leave",
                    "Employee Assistance Program",
                    "Associate Volunteer Program"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "The information that you submit will be collected and reviewed by associates, consultants, and vendors of Regions in order to evaluate your qualifications and experience for job opportunities and will not be used for marketing purposes, sold, or shared outside of Regions unless required by law",
                    "Such information will be stored in accordance with regulatory requirements and in conjunction with Regions' Retention Schedule for a minimum of three years",
                    "At Regions, the Data Engineer supports the Data and Analytics organization by designing, optimizing, and developing data pipelines to support machine learning and artificial intelligence (AI) models",
                    "The Data Engineer works with a cross functional team of Data Scientists and Software Developers on feature engineering and scoring pipelines",
                    "This role supports software developers, database architects, data analysts, and data scientists on data initiatives and ensures optimal data delivery architecture is consistent throughout ongoing projects",
                    "Partners with Regions Technology partners to Design, Build, and Maintain the data-based structures and systems in support of Data and Analytics and Data Product use cases",
                    "Builds data pipelines to collect and arrange data and manage data storage in Regions' big data environment",
                    "Builds robust, testable programs for moving, transforming, and loading data using big data tools such as Spark",
                    "Ensures data is prepared, arranged and ready for each defined business use case",
                    "Provides consultation to all areas of the organization that plan to use data to make decisions",
                    "Supports any team members in the development of such information delivery and aid in the automation of data products",
                    "Associates will have regular work hours, including full days in the office three or more days a week",
                    "The manager will set the work schedule for this position, including in-office expectations"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"EFinancialCareers",
                "link":"https:\/\/www.efinancialcareers.com\/jobs-USA-AL-Birmingham-Data_Engineer_On-Site_See_Cities_in_Posting.id23357843?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Full-time"
    },
    {
        "title":"Sr. Data Engineer",
        "company_name":"Concora Credit Inc.",
        "location":"Beaverton, OR",
        "description":"Overview\n\nAs a Sr. Data Engineer, you\u2019ll help drive Concora Credit\u2019s Mission to enable customers to Do More with Credit \u2013 every single day.\n\nThe impact you\u2019ll have at Concora Credit:\n\nWe are seeking a Sr. Data Engineer with deep expertise in Azure and Databricks to lead the design, development, and optimization of scalable data pipelines and platforms. You\u2019ll be responsible for building robust data solutions that power analytics, reporting, and machine learning across the organization using Azure cloud services and Databricks.\n\nThis position is located at our Beaverton, OR office and has a hybrid schedule. We\u2019re onsite Monday through Wednesday.\n\nWe hire people, not positions. That's because, at Concora Credit, we put people first, including our customers, partners, and Team Members. Concora Credit is guided by a single purpose: to help non-prime customers do more with credit. Today, we have helped millions of customers access credit. Our industry leadership, resilience, and willingness to adapt ensure we can help our partners responsibly say yes to millions more. As a company grounded in entrepreneurship, we're looking to expand our team and are looking for people who foster innovation, strive to make an impact, and want to Do More! We\u2019re an established company with over 20 years of experience, but now we\u2019re taking things to the next level. We're seeking someone who wants to impact the business and play a pivotal role in leading the charge for change.\n\nResponsibilities\n\nAs our Sr. Data Engineer, you will:\n\u2022 Design and develop scalable, efficient data pipelines using Azure Databricks\n\u2022 Build and manage data ingestion, transformation, and storage solutions leveraging Azure Data Factory, Azure Data Lake, and Delta Lake\n\u2022 Implement CI\/CD for data workflows using tools like Azure DevOps, Git, and Terraform\n\u2022 Optimize performance and cost efficiency across large-scale distributed data systems\n\u2022 Collaborate with analysts, data scientists, and business stakeholders to understand data needs and deliver reliable, reusable datasets\n\u2022 Provide guidance and mentor junior engineers and actively contribute to data platform best practices\n\u2022 Monitor, troubleshoot, and optimize existing pipelines and infrastructure to ensure reliability and scalability\n\nThese duties must be performed with or without reasonable accommodation.\n\nWe know experience comes in many forms and that many skills are transferable. If your experience is close to what we're looking for, consider applying. Diversity has made us the entrepreneurial and innovative company that we are today.\n\nQualifications\n\nRequirements:\n\u2022 5+ years of experience in data engineering, with a strong focus on Azure cloud technologies\n\u2022 Experience with Azure Databricks, Azure Data Lake, Data Factory including PySpark, SQL, Python and Delta Lake\n\u2022 Strong proficiency in Databricks and Apache Spark\n\u2022 Solid understanding of data warehousing, ETL\/ELT, and data modeling best practices\n\u2022 Experience with version control, CI\/CD pipelines, and infrastructure as code\n\u2022 Knowledge of Spark performance tuning, partitioning, and job orchestration\n\u2022 Excellent problem-solving skills and attention to detail\n\u2022 Strong communication and collaboration abilities across technical and non-technical teams\n\u2022 Ability to work independently and lead in a fast-paced, agile environment\n\u2022 Passion for delivering clean, high-quality, and maintainable code\n\nPreferred Qualifications:\n\u2022 Experience with Unity Catalog, Databricks Workflows, and Delta Live Tables\n\u2022 Familiarity with DevOps practices or Terraform for Azure resource provisioning\n\u2022 Understanding of data security, RBAC, and compliance in cloud environments\n\u2022 Experience integrating Databricks with Power BI or other analytics platforms\n\u2022 Exposure to real-time data processing using Kafka, Event Hubs, or Structured Streaming\n\nWhat\u2019s In It For You:\n\u2022 Medical, Dental and Vision insurance for you and your family\n\u2022 Relax and recharge with Paid Time Off (PTO)\n\u2022 6 company-observed paid holidays, plus 3 paid floating holidays\n\u2022 401k (after 90 days) plus employer match up to 4%\n\u2022 Pet Insurance for your furry family members\n\u2022 Wellness perks including onsite fitness equipment at both locations, EAP, and access to the Headspace App\n\u2022 We invest in your future through Tuition Reimbursement\n\u2022 Save on taxes with Flexible Spending Accounts\n\u2022 Peace of mind with Life and AD&D Insurance\n\u2022 Protect yourself with company-paid Long-Term Disability and voluntary Short-Term Disability\n\nConcora Credit provides equal employment opportunities to all Team Members and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.\n\nConcora Credit is an equal opportunity employer (EEO).\n\nPlease see the Concora Credit Privacy Policy for more information on how Concora Credit processes your personal information during the recruitment process and, if applicable, based on your location, how you can exercise your privacy rights. If you have questions about this privacy notice or need to contact us in connection with your personal data, including any requests to exercise your legal rights referred to at the end of this notice, please contact caprivacynotice@concoracredit.com.",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "We know experience comes in many forms and that many skills are transferable",
                    "5+ years of experience in data engineering, with a strong focus on Azure cloud technologies",
                    "Experience with Azure Databricks, Azure Data Lake, Data Factory including PySpark, SQL, Python and Delta Lake",
                    "Strong proficiency in Databricks and Apache Spark",
                    "Solid understanding of data warehousing, ETL\/ELT, and data modeling best practices",
                    "Experience with version control, CI\/CD pipelines, and infrastructure as code",
                    "Knowledge of Spark performance tuning, partitioning, and job orchestration",
                    "Excellent problem-solving skills and attention to detail",
                    "Strong communication and collaboration abilities across technical and non-technical teams",
                    "Ability to work independently and lead in a fast-paced, agile environment",
                    "Passion for delivering clean, high-quality, and maintainable code"
                ]
            },
            {
                "title":"Benefits",
                "items":[
                    "Medical, Dental and Vision insurance for you and your family",
                    "Relax and recharge with Paid Time Off (PTO)",
                    "6 company-observed paid holidays, plus 3 paid floating holidays",
                    "401k (after 90 days) plus employer match up to 4%",
                    "Pet Insurance for your furry family members",
                    "Wellness perks including onsite fitness equipment at both locations, EAP, and access to the Headspace App",
                    "We invest in your future through Tuition Reimbursement",
                    "Save on taxes with Flexible Spending Accounts",
                    "Peace of mind with Life and AD&D Insurance",
                    "Protect yourself with company-paid Long-Term Disability and voluntary Short-Term Disability"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "Data Engineer, you\u2019ll help drive Concora Credit\u2019s Mission to enable customers to Do More with Credit \u2013 every single day",
                    "Data Engineer with deep expertise in Azure and Databricks to lead the design, development, and optimization of scalable data pipelines and platforms",
                    "You\u2019ll be responsible for building robust data solutions that power analytics, reporting, and machine learning across the organization using Azure cloud services and Databricks",
                    "Design and develop scalable, efficient data pipelines using Azure Databricks",
                    "Build and manage data ingestion, transformation, and storage solutions leveraging Azure Data Factory, Azure Data Lake, and Delta Lake",
                    "Implement CI\/CD for data workflows using tools like Azure DevOps, Git, and Terraform",
                    "Optimize performance and cost efficiency across large-scale distributed data systems",
                    "Collaborate with analysts, data scientists, and business stakeholders to understand data needs and deliver reliable, reusable datasets",
                    "Provide guidance and mentor junior engineers and actively contribute to data platform best practices",
                    "Monitor, troubleshoot, and optimize existing pipelines and infrastructure to ensure reliability and scalability",
                    "These duties must be performed with or without reasonable accommodation"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"Concora Credit Inc. - ICIMS",
                "link":"https:\/\/careers-concoracredit.icims.com\/jobs\/2808\/sr.-data-engineer\/job?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Indeed",
                "link":"https:\/\/www.indeed.com\/viewjob?jk=de24ab99bcdf0425&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"ZipRecruiter",
                "link":"https:\/\/www.ziprecruiter.com\/c\/Genoa-Employment-Solutions-Inc\/Job\/Sr.-Data-Engineer\/-in-Beaverton,OR?jid=dcd4783e8019e0a8&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Glassdoor",
                "link":"https:\/\/www.glassdoor.com\/job-listing\/sr-data-engineer-flexit-inc-JV_IC1151576_KO0,16_KE17,27.htm?jl=1006855720369&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"LinkedIn",
                "link":"https:\/\/www.linkedin.com\/jobs\/view\/sr-data-engineer-at-concora-credit-4333042486?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"SimplyHired",
                "link":"https:\/\/www.simplyhired.com\/job\/6BJPTCAjozYhHQfSSLtywfk1BWw2IZYnFDUC9_wlSd9qS2DevhxqCQ?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Monster",
                "link":"https:\/\/www.monster.com\/job-openings\/sr-data-engineer-beaverton-or--991822e6-0d56-4f93-8902-9f93d2831a7e?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Ladders",
                "link":"https:\/\/www.theladders.com\/job\/sr-data-engineer-concora-credit-inc-beaverton-or_84136554?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Full-time"
    },
    {
        "title":"Data Engineer III - AMZ9162021",
        "company_name":"Amazon.com",
        "location":"Dallas, TX",
        "description":"About the position\n\nResponsibilities\n\u2022 Design, develop, implement, test, document, and operate large-scale, high-volume, high-performance data structures for business intelligence analytics.\n\u2022 Implement data structures using best practices in data modeling, ETL\/ELT processes, SQL, Oracle, and OLAP technologies.\n\u2022 Provide on-line reporting and analysis using OBIEE business intelligence tools.\n\u2022 Gather business and functional requirements and translate these into robust, scalable solutions.\n\u2022 Analyze source data systems and drive best practices in source teams.\n\u2022 Participate in the full development life cycle, from design to maintenance.\n\u2022 Produce comprehensive, usable dataset documentation and metadata.\n\u2022 Evaluate and make decisions around dataset implementations designed by peer data engineers.\n\u2022 Mentor junior data engineers.\n\nRequirements\n\u2022 Master's degree or foreign equivalent degree in Computer Science, Engineering, Information Systems, Mathematics, or a related field and three years of experience in the job offered, or as an Operations Research Analyst, Database Developer, or a related occupation.\n\u2022 Employer will accept a Bachelor's degree or foreign equivalent degree in Computer Science, Engineering, Information Systems, Mathematics, or a related field and seven years of progressive post-baccalaureate experience in the job offered or a related occupation as equivalent to the Master's degree and three years of experience.\n\u2022 Must have one year of experience in developing and operating large-scale data structures for business intelligence analytics using ETL\/ELT processes, OLAP technologies, data modeling, SQL, and Oracle.\n\nBenefits\n\u2022 Equity and sign-on payments as part of total compensation package.\n\u2022 Full range of medical, financial, and\/or other benefits.",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "Master's degree or foreign equivalent degree in Computer Science, Engineering, Information Systems, Mathematics, or a related field and three years of experience in the job offered, or as an Operations Research Analyst, Database Developer, or a related occupation",
                    "Employer will accept a Bachelor's degree or foreign equivalent degree in Computer Science, Engineering, Information Systems, Mathematics, or a related field and seven years of progressive post-baccalaureate experience in the job offered or a related occupation as equivalent to the Master's degree and three years of experience",
                    "Must have one year of experience in developing and operating large-scale data structures for business intelligence analytics using ETL\/ELT processes, OLAP technologies, data modeling, SQL, and Oracle"
                ]
            },
            {
                "title":"Benefits",
                "items":[
                    "Equity and sign-on payments as part of total compensation package",
                    "Full range of medical, financial, and\/or other benefits"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "Design, develop, implement, test, document, and operate large-scale, high-volume, high-performance data structures for business intelligence analytics",
                    "Implement data structures using best practices in data modeling, ETL\/ELT processes, SQL, Oracle, and OLAP technologies",
                    "Provide on-line reporting and analysis using OBIEE business intelligence tools",
                    "Gather business and functional requirements and translate these into robust, scalable solutions",
                    "Analyze source data systems and drive best practices in source teams",
                    "Participate in the full development life cycle, from design to maintenance",
                    "Produce comprehensive, usable dataset documentation and metadata",
                    "Evaluate and make decisions around dataset implementations designed by peer data engineers",
                    "Mentor junior data engineers"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"Teal",
                "link":"https:\/\/www.tealhq.com\/job\/data-engineer-iii-amz9162021_dc5d3762-ec42-4f92-91cf-83a5306e62fe?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Ladders",
                "link":"https:\/\/www.theladders.com\/job\/data-engineer-iii-amz9162021-amazon-dallas-tx_82868530?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"BillGoldenJobs.com",
                "link":"https:\/\/jobzone.billgoldenjobs.com\/jobs\/164702232-data-engineer-iii-amz9162021?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Full-time"
    },
    {
        "title":"Senior Analytics Data Engineer",
        "company_name":"Boston Scientific",
        "location":"Maple Grove, MN",
        "description":"Additional Location(s): N\/A\n\nDiversity - Innovation - Caring - Global Collaboration - Winning Spirit - High Performance\n\nAt Boston Scientific, we\u2019ll give you the opportunity to harness all that\u2019s within you by working in teams of diverse and high-performing employees, tackling some of the most important health industry challenges. With access to the latest tools, information and training, we\u2019ll help you in advancing your skills and career. Here, you\u2019ll be supported in progressing \u2013 whatever your ambitions.\n\nAbout the role:\n\nWe are seeking a Senior Data Analytics Engineer to join our Cardiology Marketing & Digital Enablement (CMDE) team. In this role, you will lead the design and development of scalable, analytics-ready data models in Snowflake using dbt and other modern data transformation tools. Your work will empower marketing and commercial teams with trusted, curated datasets that drive data-informed decision-making, advanced analytics, and AI-powered marketing initiatives.\n\nAs a technical leader, you will shape the data transformation layer of our marketing analytics ecosystem\u2014turning complex, multi-source data into business-ready models that fuel dashboards, audience segmentation, and campaign optimization.\n\nWork Mode:\nAt Boston Scientific, we value collaboration and synergy. This role follows a hybrid work model, requiring employees to be in our Maple Grove, MN office at least three days per week.\n\nVisa Sponsorship:\nBoston Scientific will not offer, transfer sponsorship or take over sponsorship of an employment visa for this position at this time.\n\nYour responsibilities will include:\n\u2022 Lead the\u202ftransformation and modeling\u202fof marketing, commercial, and healthcare data in\u202fSnowflake\u202fusing\u202fdbt, ensuring scalability, reliability, and documentation.\n\u2022 Collaborate with analysts, data scientists, and marketers to\u202ftranslate business needs into well-defined data models\u202fand reusable data products.\n\u2022 Implement\u202fbest practices in data modeling, testing, and version control to ensure high data quality and governance.\n\u2022 Drive\u202fdata governance and standardization\u202fefforts to improve data trust, consistency, and accessibility, to ensure all data processes align with internal governance standards, privacy requirements, and documentation protocols.\n\u2022 Partner with data platforms and architecture teams to enhance data workflows, orchestration, and automation.\n\u2022 Serve as a\u202fmentor and thought leader\u202fin dbt, analytics engineering, and modern data development practices.\n\nRequired qualifications:\n\u2022 Bachelor\u2019s degree in information technology, Computer Science, Engineering, or a related field.\n\u2022 5+ years of professional experience in data engineering or a similar technical role focused on transforming and modeling data for business use.\n\u2022 Strong expertise in dbt (Data Build Tool), SQL, and Snowflake.\n\u2022 Proven experience developing\u202fmodular, documented, and tested dbt models\u202fin production environments.\n\u2022 Familiarity with\u202fGit-based workflows, CI\/CD pipelines, and leading data test case creation to validate data quality and integrity.\n\u2022 Understanding of\u202fmarketing and commercial data\u202fdomains (CRM, campaign data, digital engagement) preferred, ideally Salesforce.\n\u2022 Familiarity with working in regulated industries or with sensitive business data (e.g., healthcare, life sciences).\n\nPreferred qualifications:\n\u2022 Excellent communication and documentation skills.\n\u2022 Experience with marketing data (e.g., campaign performance, lead funnel, audience segmentation, web\/media analytics).\n\u2022 Experience with healthcare claims and prescription data.\n\u2022 Experience with Python for data transformation or workflow automation.\n\u2022 Familiarity with semantic layers and AI\/ML data enablement.\n\nRequisition ID: 616896\n\nMinimum Salary: $ 82600\n\nMaximum Salary: $ 156900\n\nThe anticipated compensation listed above and the value of core and optional employee benefits offered by Boston Scientific (BSC) \u2013 see www.bscbenefitsconnect.com\u2014will vary based on actual location of the position and other pertinent factors considered in determining actual compensation for the role. Compensation will be commensurate with demonstrable level of experience and training, pertinent education including licensure and certifications, among other relevant business or organizational needs. At BSC, it is not typical for an individual to be hired near the bottom or top of the anticipated salary range listed above.\n\nCompensation for non-exempt (hourly), non-sales roles may also include variable compensation from time to time (e.g., any overtime and shift differential) and annual bonus target (subject to plan eligibility and other requirements).\n\nCompensation for exempt, non-sales roles may also include variable compensation, i.e., annual bonus target and long-term incentives (subject to plan eligibility and other requirements).\n\nFor MA positions: It is unlawful to require or administer a lie detector test for employment. Violators are subject to criminal penalties and civil liability.\n\nAs a leader in medical science for more than 40 years, we are committed to solving the challenges that matter most \u2013 united by a deep caring for human life. Our mission to advance science for life is about transforming lives through innovative medical solutions that improve patient lives, create value for our customers, and support our employees and the communities in which we operate. Now more than ever, we have a responsibility to apply those values to everything we do \u2013 as a global business and as a global corporate citizen.\n\nSo, choosing a career with Boston Scientific (NYSE: BSX) isn\u2019t just business, it\u2019s personal. And if you\u2019re a natural problem-solver with the imagination, determination, and spirit to make a meaningful difference to people worldwide, we encourage you to apply and look forward to connecting with you!\n\nAt Boston Scientific, we recognize that nurturing a diverse and inclusive workplace helps us be more innovative and it is important in our work of advancing science for life and improving patient health. That is why we stand for inclusion, equality, and opportunity for all. By embracing the richness of our unique backgrounds and perspectives, we create a better, more rewarding place for our employees to work and reflect the patients, customers, and communities we serve.\n\nBoston Scientific Corporation has been and will continue to be an equal opportunity employer. To ensure full implementation of its equal employment policy, the Company will continue to take steps to assure that recruitment, hiring, assignment, promotion, compensation, and all other personnel decisions are made and administered without regard to race, religion, color, national origin, citizenship, sex, sexual orientation, gender identity, gender expression, veteran status, age, mental or physical disability, genetic information or any other protected class.\n\nPlease be advised that certain US based positions, including without limitation field sales and service positions that call on hospitals and\/or health care centers, require acceptable proof of COVID-19 vaccination status. Candidates will be notified during the interview and selection process if the role(s) for which they have applied require proof of vaccination as a condition of employment. Boston Scientific continues to evaluate its policies and protocols regarding the COVID-19 vaccine and will comply with all applicable state and federal law and healthcare credentialing requirements. As employees of the Company, you will be expected to meet the ongoing requirements for your roles, including any new requirements, should the Company\u2019s policies or protocols change with regard to COVID-19 vaccination.\n\nAmong other requirements, Boston Scientific maintains specific prohibited substance test requirements for safety-sensitive positions. This role is deemed safety-sensitive and, as such, candidates will be subject to a prohibited substance test as a requirement. The goal of the prohibited substance testing is to increase workplace safety in compliance with the applicable law.",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "Bachelor\u2019s degree in information technology, Computer Science, Engineering, or a related field",
                    "5+ years of professional experience in data engineering or a similar technical role focused on transforming and modeling data for business use",
                    "Strong expertise in dbt (Data Build Tool), SQL, and Snowflake",
                    "Proven experience developing\u202fmodular, documented, and tested dbt models\u202fin production environments",
                    "Familiarity with\u202fGit-based workflows, CI\/CD pipelines, and leading data test case creation to validate data quality and integrity",
                    "Familiarity with working in regulated industries or with sensitive business data (e.g., healthcare, life sciences)"
                ]
            },
            {
                "title":"Benefits",
                "items":[
                    "With access to the latest tools, information and training, we\u2019ll help you in advancing your skills and career",
                    "Minimum Salary: $ 82600",
                    "Maximum Salary: $ 156900",
                    "The anticipated compensation listed above and the value of core and optional employee benefits offered by Boston Scientific (BSC) \u2013 see www.bscbenefitsconnect.com\u2014will vary based on actual location of the position and other pertinent factors considered in determining actual compensation for the role",
                    "Compensation will be commensurate with demonstrable level of experience and training, pertinent education including licensure and certifications, among other relevant business or organizational needs",
                    "At BSC, it is not typical for an individual to be hired near the bottom or top of the anticipated salary range listed above",
                    "Compensation for non-exempt (hourly), non-sales roles may also include variable compensation from time to time (e.g., any overtime and shift differential) and annual bonus target (subject to plan eligibility and other requirements)",
                    "Compensation for exempt, non-sales roles may also include variable compensation, i.e., annual bonus target and long-term incentives (subject to plan eligibility and other requirements)"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "In this role, you will lead the design and development of scalable, analytics-ready data models in Snowflake using dbt and other modern data transformation tools",
                    "Your work will empower marketing and commercial teams with trusted, curated datasets that drive data-informed decision-making, advanced analytics, and AI-powered marketing initiatives",
                    "As a technical leader, you will shape the data transformation layer of our marketing analytics ecosystem\u2014turning complex, multi-source data into business-ready models that fuel dashboards, audience segmentation, and campaign optimization",
                    "Lead the\u202ftransformation and modeling\u202fof marketing, commercial, and healthcare data in\u202fSnowflake\u202fusing\u202fdbt, ensuring scalability, reliability, and documentation",
                    "Collaborate with analysts, data scientists, and marketers to\u202ftranslate business needs into well-defined data models\u202fand reusable data products",
                    "Implement\u202fbest practices in data modeling, testing, and version control to ensure high data quality and governance",
                    "Drive\u202fdata governance and standardization\u202fefforts to improve data trust, consistency, and accessibility, to ensure all data processes align with internal governance standards, privacy requirements, and documentation protocols",
                    "Partner with data platforms and architecture teams to enhance data workflows, orchestration, and automation",
                    "Serve as a\u202fmentor and thought leader\u202fin dbt, analytics engineering, and modern data development practices",
                    "Requisition ID: 616896"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"Jobs At Boston Scientific",
                "link":"https:\/\/jobs.bostonscientific.com\/job\/Maple-Grove-Senior-Analytics-Data-Engineer-MN-55311\/1336876400\/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Indeed",
                "link":"https:\/\/www.indeed.com\/viewjob?jk=d90686623bd63736&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"ZipRecruiter",
                "link":"https:\/\/www.ziprecruiter.com\/c\/Boston-Scientific\/Job\/Senior-Analytics-Data-Engineer\/-in-Maple-Grove,MN?jid=3c24718738d9afb9&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Glassdoor",
                "link":"https:\/\/www.glassdoor.com\/job-listing\/senior-analytics-data-engineer-boston-scientific-JV_IC1162297_KO0,30_KE31,48.htm?jl=1009915597890&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"LinkedIn",
                "link":"https:\/\/www.linkedin.com\/jobs\/view\/senior-analytics-data-engineer-at-boston-scientific-4331867440?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Jobright",
                "link":"https:\/\/jobright.ai\/jobs\/info\/68fe9838e04ac838fb5b47ae?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Ladders",
                "link":"https:\/\/www.theladders.com\/job\/senior-analytics-data-engineer-bostonscientific-maple-grove-mn_82167476?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Local Job Network",
                "link":"https:\/\/jobs.localjobnetwork.com\/j\/t-Senior-Analytics-Data-Engineer-e-Boston-Scientific-l-Maple-Grove,-MN-jobs-j85267750.html?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Full-time"
    },
    {
        "title":"Data Engineer, Manufacturing & Supply Chain",
        "company_name":"Cargill",
        "location":"Avondale Estates, GA",
        "description":"Cargill\u2019s size and scale allows us to make a positive impact in the world. Our purpose is to nourish the world in a safe, responsible and sustainable way.We are a family company providing food, ingredients, agricultural solutions and industrial products that are vital for living. We connect farmers with markets so they can prosper. We connect customers with ingredients so they can make meals people love. And we connect families with daily essentials \u2014 from eggs to edible oils, salt to skincare, feed to alternative fuel. Our 160,000 colleagues, operating in 70 countries, make essential products that touch billions of lives each day. Join us and reach your higher purpose at Cargill.\nJob Summary\n\nThe Professional, Data Engineering job designs, builds and maintains moderately complex data systems that enable data analysis and reporting. With limited supervision, this job collaborates to ensure that large sets of data are efficiently processed and made accessible for decision making.\nEssential Functions\n\u2022 DATA & ANALYTICAL SOLUTIONS: Develops moderately complex data products and solutions using advanced data engineering and cloud based technologies, ensuring they are designed and built to be scalable, sustainable and robust.\n\u2022 DATA PIPELINES: Maintains and supports the development of streaming and batch data pipelines that facilitate the seamless ingestion of data from various data sources, transform the data into information and move to data stores like data lake, data warehouse and others.\n\u2022 DATA SYSTEMS: Reviews existing data systems and architectures to implement the identified areas for improvement and optimization.\n\u2022 DATA INFRASTRUCTURE: Helps prepare data infrastructure to support the efficient storage and retrieval of data.\n\u2022 DATA FORMATS: Implements appropriate data formats to improve data usability and accessibility across the organization.\n\u2022 STAKEHOLDER MANAGEMENT: Partners with multi-functional data and advanced analytic teams to collect requirements and ensure that data solutions meet the functional and non-functional needs of various partners.\n\u2022 DATA FRAMEWORKS: Builds moderately complex prototypes to test new concepts and implements data engineering frameworks and architectures to support the improvement of data processing capabilities and advanced analytics initiatives.\n\u2022 AUTOMATED DEPLOYMENT PIPELINES: Implements automated deployment pipelines to support improving efficiency of code deployments with fit for purpose governance.\n\u2022 DATA MODELING: Performs moderately complex data modeling aligned with the datastore technology to ensure sustainable performance and accessibility.\nQualifications\n\nMinimum requirement of 2 years of relevant work experience. Typically reflects 3 years or more of relevant experience.\nPreferred Qualifications\n\u2022 2+ years of professional proficiency in SQL\n\u2022 2+ years of professional experience working with spark with a programming language (Python\/Scala)\n\u2022 2+ years of professional experience working with AWS Cloud tools like S3, Secret Manager, CloudWatch\n\u2022 2+ years of professional experience working with CI\/CD tools and any code repo\n\u2022 2+ years of professional experience with Kafka and Snowflake is highly preferred\n\nEqual Opportunity Employer, including Disability\/Vet.\n#J-18808-Ljbffr",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "Minimum requirement of 2 years of relevant work experience",
                    "Typically reflects 3 years or more of relevant experience"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "The Professional, Data Engineering job designs, builds and maintains moderately complex data systems that enable data analysis and reporting",
                    "With limited supervision, this job collaborates to ensure that large sets of data are efficiently processed and made accessible for decision making",
                    "DATA & ANALYTICAL SOLUTIONS: Develops moderately complex data products and solutions using advanced data engineering and cloud based technologies, ensuring they are designed and built to be scalable, sustainable and robust",
                    "DATA PIPELINES: Maintains and supports the development of streaming and batch data pipelines that facilitate the seamless ingestion of data from various data sources, transform the data into information and move to data stores like data lake, data warehouse and others",
                    "DATA SYSTEMS: Reviews existing data systems and architectures to implement the identified areas for improvement and optimization",
                    "DATA INFRASTRUCTURE: Helps prepare data infrastructure to support the efficient storage and retrieval of data",
                    "DATA FORMATS: Implements appropriate data formats to improve data usability and accessibility across the organization",
                    "STAKEHOLDER MANAGEMENT: Partners with multi-functional data and advanced analytic teams to collect requirements and ensure that data solutions meet the functional and non-functional needs of various partners",
                    "DATA FRAMEWORKS: Builds moderately complex prototypes to test new concepts and implements data engineering frameworks and architectures to support the improvement of data processing capabilities and advanced analytics initiatives",
                    "AUTOMATED DEPLOYMENT PIPELINES: Implements automated deployment pipelines to support improving efficiency of code deployments with fit for purpose governance",
                    "DATA MODELING: Performs moderately complex data modeling aligned with the datastore technology to ensure sustainable performance and accessibility"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"Flexible Jobs - Work Everlast",
                "link":"https:\/\/flexiblejobs.workeverlast.com\/joblistingpage\/gokv9b0d8wcw-1d22579-6873040a60733-c118392-9d979?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Full-time"
    },
    {
        "title":"Senior Data Engineer - Next Generation Big Data",
        "company_name":"American Express",
        "location":"Phoenix, AZ",
        "description":"At American Express, our culture is built on a 175-year history of innovation, shared values and Leadership Behaviors, and an unwavering commitment to back our customers, communities, and colleagues. As part of Team Amex, you'll experience this powerful backing with comprehensive support for your holistic well-being and many opportunities to learn new skills, develop as a leader, and grow your career.\n\nHere, your voice and ideas matter, your work makes an impact, and together, you will help us define the future of American Express.\n\nJoining Amex Tech means discovering and shaping your contribution to something big. Here, you can work alongside talented tech teams and build a unique career with the Powerful Backing of American Express. With a range of opportunities to work with the latest technologies, and a commitment to back the broader engineering community through open source, our mission is to power your success. Because Amex Tech is powered by our technology, our culture, and our colleagues.\n\nLUMI is company\u2019s largest Big Data Platform, ideally suited for computationally and\/or data intensive processing applications. Whether the data needs to be processed in batch, online, or streaming manner, Lumi provides robust capabilities to handle such workloads effectively, in a cost-efficient manner.\n\nA hub of very hardworking Big Data engineers and most exciting & upcoming technologies. Cornerstone platform offers an environment where Engineers are challenged every day to build world class products.\n\nAs we embark on the journey to move to public cloud - GCP you will be part of a fast-paced Agile team, design, develop, test, troubleshoot & optimize solutions created to simplify access to the Amex\u2019s Big Data Platform.\n\nFocus:\n\nDesigns, develops, solves problems, debugs, evaluates, modifies, deploys, and documents software and systems that meet the needs of customer-facing applications, business applications, and\/or internal end user applications.\n\nOrganizational Context:\n\nMember of an engineering or delivery and integration team reporting to an Engineering manager or Engineering Director.\n\nResponsibilities:\n\u2022 Implement scalable and efficient data architectures on GCP\n\u2022 Collaborate with cross-functional teams to understand data requirements and develop solutions that meet business needs\n\u2022 Data Pipeline Development:\n\u2022 Build, test, and deploy data pipelines to move, transform, and process data from various sources to GCP\n\u2022 Ensure the reliability, scalability, and performance of data pipelines\n\u2022 Utilize GCP's big data technologies such as BigQuery, Dataflow, Dataprep, and Pub\/Sub to implement effective data processing solutions\n\u2022 Monitor system performance and proactively optimize data pipelines for efficiency\n\u2022 Troubleshoot and resolve issues\n\u2022 Create and maintain comprehensive documentation for tools, architecture, processes, and solutions\n\nQualifications:\n\u2022 Understanding of GCP services Cloud dataflow, Cloud Pub-Sub, Big Query, Cloud Storage, Cloud Dataflow, Google Composer etc\n\u2022 Strong SQL knowledge\n\u2022 Understanding of fundamentals of Git and Git workflows\n\u2022 Experience of working in agile application development environment\n\u2022 Technical support to applications on trouble shooting Environment, software and application-level issues Write, test programs using Unix Shell scripting, oracle PL\/SQL programming\n\u2022 Experience of supporting platform Engineering Activities, Network, firewall\n\nSalary Range: $120,000.00 to $210,000.00 annually + bonus + benefits\n\nThe above represents the expected salary range for this job requisition. Ultimately, in determining your pay, we\u2019ll consider your location, experience, and other job-related factors.\n\nWe back you with benefits that support your holistic well-being so you can be and deliver your best. This means caring for you and your loved ones' physical, financial, and mental health, as well as providing the flexibility you need to thrive personally and professionally:\n\u2022 Competitive base salaries\n\u2022 Bonus incentives\n\u2022 6% Company Match on retirement savings plan\n\u2022 Free financial coaching and financial well-being support\n\u2022 Comprehensive medical, dental, vision, life insurance, and disability benefits\n\u2022 Flexible working model with hybrid, onsite or virtual arrangements depending on role and business need\n\u2022 20+ weeks paid parental leave for all parents, regardless of gender, offered for pregnancy, adoption or surrogacy\n\u2022 Free access to global on-site wellness centers staffed with nurses and doctors (depending on location)\n\u2022 Free and confidential counseling support through our Healthy Minds program\n\u2022 Career development and training opportunities\n\nFor a full list of Team Amex benefits, visit our Colleague Benefits Site.\n\nAmerican Express is an equal opportunity employer and makes employment decisions without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran status, disability status, age, or any other status protected by law. American Express will consider for employment all qualified applicants, including those with arrest or conviction records, in accordance with the requirements of applicable state and local laws, including, but not limited to, the California Fair Chance Act, the Los Angeles County Fair Chance Ordinance for Employers, and the City of Los Angeles\u2019 Fair Chance Initiative for Hiring Ordinance. For positions covered by federal and\/or state banking regulations, American Express will comply with such regulations as it relates to the consideration of applicants with criminal convictions.\n\nWe back our colleagues with the support they need to thrive, professionally and personally. That's why we have Amex Flex, our enterprise working model that provides greater flexibility to colleagues while ensuring we preserve the important aspects of our unique in-person culture. Depending on role and business needs, colleagues will either work onsite, in a hybrid model (combination of in-office and virtual days) or fully virtually.\n\nUS Job Seekers - Click to view the \u201cKnow Your Rights\u201d poster. If the link does not work, you may access the poster by copying and pasting the following URL in a new browser window: https:\/\/www.eeoc.gov\/poster\n\nDepending on factors such as business unit requirements, the nature of the position, cost and applicable laws, American Express may provide visa sponsorship for certain positions",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "Understanding of GCP services Cloud dataflow, Cloud Pub-Sub, Big Query, Cloud Storage, Cloud Dataflow, Google Composer etc",
                    "Strong SQL knowledge",
                    "Understanding of fundamentals of Git and Git workflows",
                    "Experience of working in agile application development environment",
                    "Technical support to applications on trouble shooting Environment, software and application-level issues Write, test programs using Unix Shell scripting, oracle PL\/SQL programming",
                    "Experience of supporting platform Engineering Activities, Network, firewall"
                ]
            },
            {
                "title":"Benefits",
                "items":[
                    "Salary Range: $120,000.00 to $210,000.00 annually + bonus + benefits",
                    "The above represents the expected salary range for this job requisition",
                    "Ultimately, in determining your pay, we\u2019ll consider your location, experience, and other job-related factors",
                    "We back you with benefits that support your holistic well-being so you can be and deliver your best",
                    "Competitive base salaries",
                    "Bonus incentives",
                    "6% Company Match on retirement savings plan",
                    "Free financial coaching and financial well-being support",
                    "Comprehensive medical, dental, vision, life insurance, and disability benefits",
                    "Flexible working model with hybrid, onsite or virtual arrangements depending on role and business need",
                    "20+ weeks paid parental leave for all parents, regardless of gender, offered for pregnancy, adoption or surrogacy",
                    "Free access to global on-site wellness centers staffed with nurses and doctors (depending on location)",
                    "Free and confidential counseling support through our Healthy Minds program",
                    "Career development and training opportunities"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "Designs, develops, solves problems, debugs, evaluates, modifies, deploys, and documents software and systems that meet the needs of customer-facing applications, business applications, and\/or internal end user applications",
                    "Member of an engineering or delivery and integration team reporting to an Engineering manager or Engineering Director",
                    "Implement scalable and efficient data architectures on GCP",
                    "Collaborate with cross-functional teams to understand data requirements and develop solutions that meet business needs",
                    "Data Pipeline Development:",
                    "Build, test, and deploy data pipelines to move, transform, and process data from various sources to GCP",
                    "Ensure the reliability, scalability, and performance of data pipelines",
                    "Utilize GCP's big data technologies such as BigQuery, Dataflow, Dataprep, and Pub\/Sub to implement effective data processing solutions",
                    "Monitor system performance and proactively optimize data pipelines for efficiency",
                    "Troubleshoot and resolve issues",
                    "Create and maintain comprehensive documentation for tools, architecture, processes, and solutions"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"Eightfold - Eightfold AI",
                "link":"https:\/\/aexp.eightfold.ai\/careers\/job\/37818384-senior-data-engineer-next-generation-big-data-phoenix-arizona-united-states?domain=aexp.com&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"ZipRecruiter",
                "link":"https:\/\/www.ziprecruiter.com\/c\/American-Express\/Job\/Senior-Data-Engineer-Next-Generation-Big-Data\/-in-Phoenix,AZ?jid=6da26f56b805231d&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Jobs And Careers",
                "link":"https:\/\/www.career.com\/job\/american-express\/senior-data-engineer-next-generation-big-data\/j202509081823379424854?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"JobGet",
                "link":"https:\/\/www.jobget.com\/jobs\/job\/9791b5dd-fc73-4771-9ec0-36a8f777cd96?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"LazyApply",
                "link":"https:\/\/lazyapply.com\/jobpreview\/senior-data-engineer-next-generation-big-data-americanexpress-phoenix-az_83233214?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Jobs.weekday.works",
                "link":"https:\/\/jobs.weekday.works\/american-express-senior-data-engineer---next-generation-big-data?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Adzuna",
                "link":"https:\/\/www.adzuna.com\/details\/5482201199?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"WhatJobs",
                "link":"https:\/\/www.whatjobs.com\/jobs\/emerging-technologies?id=2243685351&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Full-time"
    },
    {
        "title":"Senior Data Engineer",
        "company_name":"Oracle",
        "location":"Oakdale, NE",
        "description":"**Job Description**\nWe're looking for an experienced software engineer specializing in data engineering to join our growing team focused on building, supporting and enhancing the OCI Data Center telemetry big data and analytics platform. This platform is at the heart of driving hardware quality, reliability, and efficiency as well as providing operational insights for impact and root cause analysis. We're strongly focused on supporting the high scale use cases of Oracle Cloud Infrastructure (OCI). We provide self service tools to enable data discovery, data processing, data mining and research, reporting, and machine learning\/AI, all built using OCI services and industry standard open source distributed computing tools.\nThe Data Engineer will get to work with data at scale. This includes ingesting and processing large data volumes of hardware telemetry from every machine in our Cloud data centers, both in batch and near-real time. This enables us to provide measurement & alerting on key hardware quality KPI trends, as well as using machine learning to predict hardware failures before they happen and improve the OCI service offerings and customer experience. We are looking for engineers who are creative, passionate about data and its application in building solutions, and are excited to work at scale to solve customer problems.\n\u2022 *Responsibilities**\n\u2022 *A day in the life...**\n+ Support the development and evolution of our Data Platform.\n+ Understand, leverage and apply engineering best practices effectively. Lead by example and improve our coding standards and best practices.\n+ Build robust, highly available, performant, scalable and reliable data services.\n+ Collaborate with cross-functional teams, engineers, and program\/product management on building optimized and forward looking data tools, insights and reporting mechanisms that drive the business forward.\n+ Deliver quality features on-time and on-budget.\n+ Anticipate system\/application challenges and propose solutions.\n+ Contribute innovative & new ideas to the product\/program Roadmap.\n+ Mentor junior resources and drive end to end design, implementation and delivery of engineering components.\n+ Work with team members to manage the day-to-day development activities, participate in designs, design reviews, code reviews and implementation.\n+ Adjust positively to quickly changing priorities and shifting goals.\n+ Strive for continuous improvement of code quality and development practices.\n\u2022 *Required Qualifications**\n+ 4-6 years of professional experience in practice area.\n+ Excellent leadership, verbal and written communication skills.\n+ Proven high level of expertise in any or all of the programming languages Python, Java, Scala, SQL and related technology stacks.\n+ Knowledge & experience in big data technologies (Hadoop, Apache Airflow, Apache Spark, Hive, Presto, etc.).\n+ Knowledge & experience in data streaming technologies such as Apache Kafka, Flink etc.\n+ Deep proficiency in engineering best practices for the full software development lifecycle, including coding standards, code reviews, source control management, build processes, testing, and operations.\n+ Proven ability to take a project from scoping requirements through actual launch of the project.\n+ Experience with cloud computing fundamentals and building applications using cloud managed services.\n+ Willingness to adapt to and self learn new technologies and deliver on them.\n+ Technical depth and vision to perform POC's and evaluate different technologies.\n\u2022 *Additional Qualifications (not required, but each is a plus)**\n+ Prior experience building\/working with ML platforms\/frameworks & Gen-AI(RAG, Agents etc).\n+ Knowledge & experience of containerization technologies such as Docker\/Kubernetes.\n+ Prior experience building self service data platforms.\n+ Proven proficiency in API development (REST).\n\\#LI-LG1\nDisclaimer:\n\u2022 *Certain US customer or client-facing roles may be required to comply with applicable requirements, such as immunization and occupational health mandates.**\n\u2022 *Range and benefit information provided in this posting are specific to the stated locations only**\nUS: Hiring Range in USD from: $79,800 to $178,100 per annum. May be eligible for bonus and equity.\nOracle maintains broad salary ranges for its roles in order to account for variations in knowledge, skills, experience, market conditions and locations, as well as reflect Oracle's differing products, industries and lines of business.\nCandidates are typically placed into the range based on the preceding factors as well as internal peer equity.\nOracle US offers a comprehensive benefits package which includes the following:\n1. Medical, dental, and vision insurance, including expert medical opinion\n2. Short term disability and long term disability\n3. Life insurance and AD&D\n4. Supplemental life insurance (Employee\/Spouse\/Child)\n5. Health care and dependent care Flexible Spending Accounts\n6. Pre-tax commuter and parking benefits\n7. 401(k) Savings and Investment Plan with company match\n8. Paid time off: Flexible Vacation is provided to all eligible employees assigned to a salaried (non-overtime eligible) position. Accrued Vacation is provided to all other employees eligible for vacation benefits. For employees working at least 35 hours per week, the vacation accrual rate is 13 days annually for the first three years of employment and 18 days annually for subsequent years of employment. Vacation accrual is prorated for employees working between 20 and 34 hours per week. Employees working fewer than 20 hours per week are not eligible for vacation.\n9. 11 paid holidays\n10. Paid sick leave: 72 hours of paid sick leave upon date of hire. Refreshes each calendar year. Unused balance will carry over each year up to a maximum cap of 112 hours.\n11. Paid parental leave\n12. Adoption assistance\n13. Employee Stock Purchase Plan\n14. Financial planning and group legal\n15. Voluntary benefits including auto, homeowner and pet insurance\nThe role will generally accept applications for at least three calendar days from the posting date or as long as the job remains posted.\nCareer Level - IC3\n\u2022 *About Us**\nAs a world leader in cloud solutions, Oracle uses tomorrow's technology to tackle today's challenges. We've partnered with industry-leaders in almost every sector-and continue to thrive after 40+ years of change by operating with integrity.\nWe know that true innovation starts when everyone is empowered to contribute. That's why we're committed to growing an inclusive workforce that promotes opportunities for all.\nOracle careers open the door to global opportunities where work-life balance flourishes. We offer competitive benefits based on parity and consistency and support our people with flexible medical, life insurance, and retirement options. We also encourage employees to give back to their communities through our volunteer programs.\nWe're committed to including people with disabilities at all stages of the employment process. If you require accessibility assistance or accommodation for a disability at any point, let us know by emailing or by calling in the United States.\nOracle is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans' status, or any other characteristic protected by law. Oracle will consider for employment qualified applicants with arrest and conviction records pursuant to applicable law.",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "4-6 years of professional experience in practice area",
                    "Excellent leadership, verbal and written communication skills",
                    "Proven high level of expertise in any or all of the programming languages Python, Java, Scala, SQL and related technology stacks",
                    "Knowledge & experience in big data technologies (Hadoop, Apache Airflow, Apache Spark, Hive, Presto, etc.)",
                    "Knowledge & experience in data streaming technologies such as Apache Kafka, Flink etc",
                    "Deep proficiency in engineering best practices for the full software development lifecycle, including coding standards, code reviews, source control management, build processes, testing, and operations",
                    "Proven ability to take a project from scoping requirements through actual launch of the project",
                    "Experience with cloud computing fundamentals and building applications using cloud managed services",
                    "Willingness to adapt to and self learn new technologies and deliver on them",
                    "Technical depth and vision to perform POC's and evaluate different technologies",
                    "Prior experience building\/working with ML platforms\/frameworks & Gen-AI(RAG, Agents etc)",
                    "Knowledge & experience of containerization technologies such as Docker\/Kubernetes",
                    "Prior experience building self service data platforms",
                    "Proven proficiency in API development (REST)"
                ]
            },
            {
                "title":"Benefits",
                "items":[
                    "US: Hiring Range in USD from: $79,800 to $178,100 per annum",
                    "May be eligible for bonus and equity",
                    "Oracle maintains broad salary ranges for its roles in order to account for variations in knowledge, skills, experience, market conditions and locations, as well as reflect Oracle's differing products, industries and lines of business",
                    "Oracle US offers a comprehensive benefits package which includes the following:",
                    "Medical, dental, and vision insurance, including expert medical opinion",
                    "Short term disability and long term disability",
                    "Life insurance and AD&D",
                    "Supplemental life insurance (Employee\/Spouse\/Child)",
                    "Health care and dependent care Flexible Spending Accounts",
                    "Pre-tax commuter and parking benefits",
                    "401(k) Savings and Investment Plan with company match",
                    "Paid time off: Flexible Vacation is provided to all eligible employees assigned to a salaried (non-overtime eligible) position",
                    "Accrued Vacation is provided to all other employees eligible for vacation benefits",
                    "For employees working at least 35 hours per week, the vacation accrual rate is 13 days annually for the first three years of employment and 18 days annually for subsequent years of employment",
                    "Vacation accrual is prorated for employees working between 20 and 34 hours per week",
                    "Employees working fewer than 20 hours per week are not eligible for vacation",
                    "11 paid holidays",
                    "Paid sick leave: 72 hours of paid sick leave upon date of hire",
                    "Refreshes each calendar year",
                    "Unused balance will carry over each year up to a maximum cap of 112 hours",
                    "Paid parental leave",
                    "Adoption assistance",
                    "Employee Stock Purchase Plan",
                    "Financial planning and group legal",
                    "Voluntary benefits including auto, homeowner and pet insurance",
                    "We offer competitive benefits based on parity and consistency and support our people with flexible medical, life insurance, and retirement options"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "This platform is at the heart of driving hardware quality, reliability, and efficiency as well as providing operational insights for impact and root cause analysis",
                    "The Data Engineer will get to work with data at scale",
                    "This includes ingesting and processing large data volumes of hardware telemetry from every machine in our Cloud data centers, both in batch and near-real time",
                    "This enables us to provide measurement & alerting on key hardware quality KPI trends, as well as using machine learning to predict hardware failures before they happen and improve the OCI service offerings and customer experience",
                    "We are looking for engineers who are creative, passionate about data and its application in building solutions, and are excited to work at scale to solve customer problems",
                    "Support the development and evolution of our Data Platform",
                    "Understand, leverage and apply engineering best practices effectively",
                    "Lead by example and improve our coding standards and best practices",
                    "Build robust, highly available, performant, scalable and reliable data services",
                    "Collaborate with cross-functional teams, engineers, and program\/product management on building optimized and forward looking data tools, insights and reporting mechanisms that drive the business forward",
                    "Deliver quality features on-time and on-budget",
                    "Anticipate system\/application challenges and propose solutions",
                    "Contribute innovative & new ideas to the product\/program Roadmap",
                    "Mentor junior resources and drive end to end design, implementation and delivery of engineering components",
                    "Work with team members to manage the day-to-day development activities, participate in designs, design reviews, code reviews and implementation",
                    "Adjust positively to quickly changing priorities and shifting goals",
                    "Strive for continuous improvement of code quality and development practices",
                    "*Certain US customer or client-facing roles may be required to comply with applicable requirements, such as immunization and occupational health mandates.**",
                    "*Range and benefit information provided in this posting are specific to the stated locations only**"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"WhatJobs",
                "link":"https:\/\/www.whatjobs.com\/jobs\/senior-data-engineer?id=2243183302&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Full-time"
    },
    {
        "title":"Data Engineer",
        "company_name":"EP Wealth Advisors, LLC",
        "location":"Belen, NM",
        "description":"EP Wealth Advisors (EPWA) is a wealth management advisory firm with over $40.5 billion in AUM as of September 30, 2025, serving predominately high net worth individuals. EPWA fosters an inclusive environment that offers opportunities for our associates to learn, grow and enhance their skills to take on new challenges to progress in their professional careers.\n\nEP Wealth Advisors is launching its first enterprise data platform initiative, a strategic effort to build a modern, cloud-based data environment powered by Snowflake. As a Data Engineer, you will play a pivotal role in the design, development, and implementation of this platform \u2014 working in close partnership with both internal technology teams and an experienced Snowflake implementation service provider.\n\nThis position is ideal for a data professional who thrives in a collaborative, fast-paced environment and wants to contribute to building a foundational data infrastructure from the ground up. You will help establish best practices, design scalable pipelines, and ensure the new platform enables firmwide analytics, reporting, and data-driven decision-making. Our ideal candidate will have a demonstrated track record of delivering excellent client service, with exceptional organizational, communication and problem-solving skills. You will join a team of dynamic, collaborative, and client-focused professionals who are focused on delivering on our founding core values: Integrity, Entrepreneurial, Inclusion and Connection.\n\nSalary Range: $120,000 - $150,000 plus annual bonus\n\nDuties and Responsibilities:\n\n\u2022 Collaborate closely with EP Wealth\u2019s Snowflake implementation partner to design, configure, and deploy the firm\u2019s first enterprise data platform.\n\n\u2022 Participate in architecture design sessions, technical planning, and milestone reviews with both internal stakeholders and external partners.\n\n\u2022 Develop and maintain efficient, secure, and scalable ETL\/ELT data pipelines to ingest data into Snowflake.\n\n\u2022 Build and optimize Snowflake data models, schemas, and role-based access controls.\n\n\u2022 Define and enforce Snowflake best practices for performance optimization, cost management, and data security.\n\n\u2022 Support environment setup, data migration, and initial production rollout activities in coordination with the implementation partner.\n\nIntegrate data from multiple enterprise systems including CRM, portfolio management, financial planning, and custodial platforms\n\n\u2022 Design data models and reusable ingestion frameworks that support analytics and reporting use cases.\n\n\u2022 Contribute to the development of a unified enterprise data model and metadata framework.\n\n\u2022 Collaborate with the implementation partner to align technical design with EP Wealth\u2019s business strategy and compliance standards.\n\n\u2022 Implement data validation, monitoring, and auditing to ensure data accuracy, completeness, and reliability.\n\n\u2022 Collaborate with IT leadership to define and operationalize data governance standards.\n\n\u2022 Troubleshoot and resolve data pipeline issues while maintaining system performance and uptime.\n\n\u2022 Conduct query and warehouse optimization to ensure efficient Snowflake usage.\n\n\u2022 Work closely with data analysts and technology teams to deliver accessible, well-documented data sets.\n\n\u2022 Participate in design reviews, sprint planning, and architectural discussions with the implementation partner.\n\n\u2022 Identify opportunities to extend Snowflake functionality through features such as Snowpark, data sharing, or automation tools.\n\n\u2022 Stay informed on Snowflake\u2019s evolving ecosystem and cloud data engineering best practices.\n\nQualifications\n\n\u2022 Bachelor\u2019s degree in Computer Science, Information Systems, Data Engineering, or a related technical field.\n\u2022 5+ years of data engineering experience, with direct hands-on experience in Snowflake strongly preferred.\n\u2022 Demonstrated success contributing to or co-delivering a cloud data platform implementation alongside external partners or vendors.\n\u2022 Experience building scalable data pipelines and transformations for analytics and operational reporting.\n\u2022 Advanced proficiency in SQL, with deep experience in Snowflake query optimization and data modeling.\n\u2022 Proficiency in Python or another scripting language for data processing and automation.\n\u2022 Experience with ETL\/ELT tools and orchestration platforms (DBT, Airflow, Dagster, etc.).\n\u2022 Familiarity with cloud infrastructure (AWS, GCP, or Azure) and Snowflake integrations.\n\u2022 Experience with enterprise data integration from CRM, portfolio management, and financial planning systems (Salesforce, Tamarac, Orion, etc.).\n\u2022 Understanding of data governance, lineage, and observability tools.\n\u2022 Strong understanding of Snowflake architecture, data sharing, and multi-cluster warehouse capabilities.\n\u2022 Experience defining data security and access policies in regulated environments.\n\u2022 Ability to bridge technical and business needs through thoughtful data design and collaboration.\n\nWhat We Offer\nWe offer a highly competitive suite of holistic benefits designed to help our team members balance their personal and professional life commitments. These include options designed to encourage employee's health, happiness, and financial well-being.\n\u2022 11 Paid Holidays + 2 floating holidays\n\u2022 3 Weeks (PTO)\n\u2022 Paid Volunteer Time\n\u2022 Flexible Work Schedule\n\u2022 Highly subsidized Health, Dental, and Vision Plans\n\u2022 401k Retirement Account with company match contributions\n\u2022 Free Mental Health services, Life Insurance, Long & Short-Term Disability Insurance\n\u2022 Flexible Spending Accounts and Health Savings Accounts\n\u2022 Employee Financial Education\n\u2022 Employee Educational Expense Reimbursement\n\u2022 Employee Charitable Donations\n\u2022 Employee Referral Incentives\n\u2022 Employee Team Building Activities\n\u2022 Employee Assistance Program\n\nEPWA is an equal opportunity employer. Prospective employees will receive consideration without discrimination because of race, creed, color, sex, gender, gender expression, gender identity, sexual orientation, age, religion, national origin, ancestry, mental disability, physical disability, medical condition, genetic information, marital status, military and veteran status, or any other basis protected by law.",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "Bachelor\u2019s degree in Computer Science, Information Systems, Data Engineering, or a related technical field",
                    "Demonstrated success contributing to or co-delivering a cloud data platform implementation alongside external partners or vendors",
                    "Experience building scalable data pipelines and transformations for analytics and operational reporting",
                    "Advanced proficiency in SQL, with deep experience in Snowflake query optimization and data modeling",
                    "Proficiency in Python or another scripting language for data processing and automation",
                    "Experience with ETL\/ELT tools and orchestration platforms (DBT, Airflow, Dagster, etc.)",
                    "Familiarity with cloud infrastructure (AWS, GCP, or Azure) and Snowflake integrations",
                    "Experience with enterprise data integration from CRM, portfolio management, and financial planning systems (Salesforce, Tamarac, Orion, etc.)",
                    "Understanding of data governance, lineage, and observability tools",
                    "Strong understanding of Snowflake architecture, data sharing, and multi-cluster warehouse capabilities",
                    "Experience defining data security and access policies in regulated environments",
                    "Ability to bridge technical and business needs through thoughtful data design and collaboration"
                ]
            },
            {
                "title":"Benefits",
                "items":[
                    "Salary Range: $120,000 - $150,000 plus annual bonus",
                    "We offer a highly competitive suite of holistic benefits designed to help our team members balance their personal and professional life commitments",
                    "These include options designed to encourage employee's health, happiness, and financial well-being",
                    "11 Paid Holidays + 2 floating holidays",
                    "3 Weeks (PTO)",
                    "Paid Volunteer Time",
                    "Flexible Work Schedule",
                    "Highly subsidized Health, Dental, and Vision Plans",
                    "401k Retirement Account with company match contributions",
                    "Free Mental Health services, Life Insurance, Long & Short-Term Disability Insurance",
                    "Flexible Spending Accounts and Health Savings Accounts",
                    "Employee Financial Education",
                    "Employee Educational Expense Reimbursement",
                    "Employee Charitable Donations",
                    "Employee Referral Incentives"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "As a Data Engineer, you will play a pivotal role in the design, development, and implementation of this platform \u2014 working in close partnership with both internal technology teams and an experienced Snowflake implementation service provider",
                    "This position is ideal for a data professional who thrives in a collaborative, fast-paced environment and wants to contribute to building a foundational data infrastructure from the ground up",
                    "You will help establish best practices, design scalable pipelines, and ensure the new platform enables firmwide analytics, reporting, and data-driven decision-making",
                    "Our ideal candidate will have a demonstrated track record of delivering excellent client service, with exceptional organizational, communication and problem-solving skills",
                    "Collaborate closely with EP Wealth\u2019s Snowflake implementation partner to design, configure, and deploy the firm\u2019s first enterprise data platform",
                    "Participate in architecture design sessions, technical planning, and milestone reviews with both internal stakeholders and external partners",
                    "Develop and maintain efficient, secure, and scalable ETL\/ELT data pipelines to ingest data into Snowflake",
                    "Build and optimize Snowflake data models, schemas, and role-based access controls",
                    "Define and enforce Snowflake best practices for performance optimization, cost management, and data security",
                    "Support environment setup, data migration, and initial production rollout activities in coordination with the implementation partner",
                    "Integrate data from multiple enterprise systems including CRM, portfolio management, financial planning, and custodial platforms",
                    "Design data models and reusable ingestion frameworks that support analytics and reporting use cases",
                    "Contribute to the development of a unified enterprise data model and metadata framework",
                    "Collaborate with the implementation partner to align technical design with EP Wealth\u2019s business strategy and compliance standards",
                    "Implement data validation, monitoring, and auditing to ensure data accuracy, completeness, and reliability",
                    "Collaborate with IT leadership to define and operationalize data governance standards",
                    "Troubleshoot and resolve data pipeline issues while maintaining system performance and uptime",
                    "Conduct query and warehouse optimization to ensure efficient Snowflake usage",
                    "Work closely with data analysts and technology teams to deliver accessible, well-documented data sets",
                    "Participate in design reviews, sprint planning, and architectural discussions with the implementation partner",
                    "Identify opportunities to extend Snowflake functionality through features such as Snowpark, data sharing, or automation tools",
                    "Stay informed on Snowflake\u2019s evolving ecosystem and cloud data engineering best practices"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"EP Wealth Advisors, LLC",
                "link":"https:\/\/jobiak.epwealth.com\/careers\/data-engineer-belen-nm-690345ea5d24500a56fa34a5?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Full-time"
    },
    {
        "title":"Senior Data Engineer (Fixed Income\/Capital Markets)",
        "company_name":"ADDSOURCE",
        "location":"New York, NY",
        "description":"Job Title: Senior Data Engineer (Fixed Income\/Capital Markets) Location: New York, NY\n\nJob Description:\n\nLead the development and optimization of batch and real-time data pipelines, ensuring scalability, reliability, and performance.\n\nArchitect, design, and deploy data integration, streaming, and analytics solutions leveraging Spark, Kafka, and Snowflake.\n\nAbility to help voluntarily and proactively, and support Team Members, Peers to deliver their tasks to ensure End-to-end delivery.\n\nEvaluates technical performance challenges and recommend tuning solutions.\n\nHands-on knowledge of Data Service Engineer to design, develop, and maintain our Reference Data System utilizing modern data technologies including Kafka, Snowflake, and Python.\n\nRequired Skills:\n\u2022 Proven experience in building and maintaining data pipelines, especially using Kafka, Snowflake, and Python.\n\u2022 Strong expertise in distributed data processing and streaming architectures.\n\u2022 Experience with Snowflake data warehouse platform: data loading, performance tuning, and management.\n\u2022 Proficiency in Python scripting and programming for data manipulation and automation.\n\u2022 Familiarity with Kafka ecosystem (Confluent, Kafka Connect, Kafka Streams).\n\u2022 Knowledge of SQL, data modeling, and ETL\/ELT processes.\n\u2022 Understanding of cloud platforms (AWS, Azure, Google Cloud Platform) is a plus.\n\nDomain Knowledge in any of the below area:\n\u2022 Trade Processing, Settlement, Reconciliation, and related back\/middle-office functions within financial markets (Equities, Fixed Income, Derivatives, FX, etc.).\n\u2022 Strong understanding of trade lifecycle events, order types, allocation rules, and settlement processes.\n\u2022 Funding Support, Planning & Analysis, Regulatory reporting & Compliance.\n\u2022 Knowledge of regulatory standards (such as Dodd-Frank, EMIR, MiFID II) related to trade reporting and lifecycle management.",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "Proven experience in building and maintaining data pipelines, especially using Kafka, Snowflake, and Python",
                    "Strong expertise in distributed data processing and streaming architectures",
                    "Experience with Snowflake data warehouse platform: data loading, performance tuning, and management",
                    "Proficiency in Python scripting and programming for data manipulation and automation",
                    "Familiarity with Kafka ecosystem (Confluent, Kafka Connect, Kafka Streams)",
                    "Knowledge of SQL, data modeling, and ETL\/ELT processes",
                    "Trade Processing, Settlement, Reconciliation, and related back\/middle-office functions within financial markets (Equities, Fixed Income, Derivatives, FX, etc.)",
                    "Funding Support, Planning & Analysis, Regulatory reporting & Compliance",
                    "Knowledge of regulatory standards (such as Dodd-Frank, EMIR, MiFID II) related to trade reporting and lifecycle management"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "Lead the development and optimization of batch and real-time data pipelines, ensuring scalability, reliability, and performance",
                    "Architect, design, and deploy data integration, streaming, and analytics solutions leveraging Spark, Kafka, and Snowflake",
                    "Ability to help voluntarily and proactively, and support Team Members, Peers to deliver their tasks to ensure End-to-end delivery",
                    "Evaluates technical performance challenges and recommend tuning solutions",
                    "Hands-on knowledge of Data Service Engineer to design, develop, and maintain our Reference Data System utilizing modern data technologies including Kafka, Snowflake, and Python",
                    "Strong understanding of trade lifecycle events, order types, allocation rules, and settlement processes"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"Dice",
                "link":"https:\/\/www.dice.com\/job-detail\/adfde018-3e76-4445-bf60-4add88f977ac?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"LinkedIn",
                "link":"https:\/\/www.linkedin.com\/jobs\/view\/senior-data-engineer-credit-risk-domain-expert-at-accord-technologies-inc-4251361043?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"App: Otta",
                "link":"https:\/\/app.otta.com\/jobs\/BH65rrj1?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Welcome To The Jungle | Login",
                "link":"https:\/\/app.welcometothejungle.com\/jobs\/BH65rrj1?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Adzuna",
                "link":"https:\/\/www.adzuna.com\/details\/5480202883?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"BeBee",
                "link":"https:\/\/us.bebee.com\/job\/2a8e299d9fdc474d2538fe8bf0a2bae4?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Career.com",
                "link":"https:\/\/www.career.com\/job\/addsource\/senior-data-engineer-fixed-income-capital-markets\/j202511042222019873824?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Recruit.net",
                "link":"https:\/\/www.recruit.net\/job\/data-engineer-fixed-capital-markets-jobs\/3A43F15B4824670E?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Contractor"
    },
    {
        "title":"Senior Staff Data Engineer (Data Platform)",
        "company_name":"Grindr",
        "location":"San Francisco, CA (+1 other)",
        "description":"Requirements\n\u2022 10+ years of experience working with data at scale, with a strong focus on data platforms and developer experience,\n\u2022 Expert in Python and SQL, with significant experience in distributed data systems,\n\u2022 Hands-on experience with Airflow (must-have) and dbt (able to set up from scratch),\n\u2022 Strong knowledge of Spark, Databricks, or other distributed computing platforms for ML training,\n\u2022 Experience with AWS and cloud-native architectures,\n\u2022 Background in software engineering with a transition into data engineering preferred - able to apply SWE rigor to data platform design,\n\u2022 Demonstrated ability to enhance developer experience - for example, building holistic automated solutions rather than one-off tables or fixes,\n\u2022 Track record of mentorship and technical leadership at the organizational level,\n\u2022 Familiar with data governance, classification, retention, and compliance frameworks (SOC2, GDPR, CCPA)\n\nWhat the job involves\n\u2022 As a Senior Staff Data Engineer (Data Platform), you\u2019ll play a critical role in defining what \u201cgreat\u201d looks like for our data platform,\n\u2022 You\u2019ll focus on creating a seamless development experience for DS and ML engineers, ensuring they can move quickly from idea to production with reliable, scalable systems behind them,\n\u2022 At this level, you\u2019ll not only solve complex technical challenges\u2014you\u2019ll also mentor other engineers and influence the long-term direction of Grindr\u2019s data platform,\n\u2022 Design, develop, and deliver scalable data platforms that enhance the developer experience for data scientists and ML engineers,\n\u2022 Solve technical problems of the highest scope and complexity, influencing platform direction and architecture across teams,\n\u2022 Connect tools and systems into cohesive, automated workflows that eliminate repetitive work and unlock faster experimentation,\n\u2022 Lead implementation of core platform technologies like dbt (from the ground up), Airflow (critical), and distributed computing frameworks (Spark, Databricks, or similar),\n\u2022 Drive adoption of long-term, scalable solutions instead of one-off fixes,\n\u2022 Provide mentorship for engineers on your team and across the org,\n\u2022 Stay on top of new technologies through R&D and prototyping to continuously improve developer experience, scalability, and reliability,\n\u2022 Ensure governance and compliance standards are met around data representation, storage, classification, and retention,\n\u2022 Work closely with Data Scientists, ML Engineers, SREs, and Product Managers to make sure platforms and APIs meet real user needs",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "10+ years of experience working with data at scale, with a strong focus on data platforms and developer experience,",
                    "Expert in Python and SQL, with significant experience in distributed data systems,",
                    "Hands-on experience with Airflow (must-have) and dbt (able to set up from scratch),",
                    "Strong knowledge of Spark, Databricks, or other distributed computing platforms for ML training,",
                    "Experience with AWS and cloud-native architectures,",
                    "Demonstrated ability to enhance developer experience - for example, building holistic automated solutions rather than one-off tables or fixes,",
                    "Track record of mentorship and technical leadership at the organizational level,",
                    "Familiar with data governance, classification, retention, and compliance frameworks (SOC2, GDPR, CCPA)"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "As a Senior Staff Data Engineer (Data Platform), you\u2019ll play a critical role in defining what \u201cgreat\u201d looks like for our data platform,",
                    "You\u2019ll focus on creating a seamless development experience for DS and ML engineers, ensuring they can move quickly from idea to production with reliable, scalable systems behind them,",
                    "At this level, you\u2019ll not only solve complex technical challenges\u2014you\u2019ll also mentor other engineers and influence the long-term direction of Grindr\u2019s data platform,",
                    "Design, develop, and deliver scalable data platforms that enhance the developer experience for data scientists and ML engineers,",
                    "Solve technical problems of the highest scope and complexity, influencing platform direction and architecture across teams,",
                    "Connect tools and systems into cohesive, automated workflows that eliminate repetitive work and unlock faster experimentation,",
                    "Lead implementation of core platform technologies like dbt (from the ground up), Airflow (critical), and distributed computing frameworks (Spark, Databricks, or similar),",
                    "Drive adoption of long-term, scalable solutions instead of one-off fixes,",
                    "Provide mentorship for engineers on your team and across the org,",
                    "Stay on top of new technologies through R&D and prototyping to continuously improve developer experience, scalability, and reliability,",
                    "Ensure governance and compliance standards are met around data representation, storage, classification, and retention,",
                    "Work closely with Data Scientists, ML Engineers, SREs, and Product Managers to make sure platforms and APIs meet real user needs"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"App: Otta",
                "link":"https:\/\/app.otta.com\/jobs\/netRpPTK?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Welcome To The Jungle | Login",
                "link":"https:\/\/app.welcometothejungle.com\/jobs\/netRpPTK?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Full-time"
    },
    {
        "title":"Sr Data Engineering Manager",
        "company_name":"Honeywell",
        "location":"Phoenix, AZ",
        "description":"This role offers a unique opportunity to shape our data strategy for SAP S\/4HANA, drive innovation in data architecture and migration methodologies, and ensure that our enterprise data solutions align with business objectives while supporting our end-to-end ERP transformation.\n\nYou will report directly to our IT Director of Delivery, and you'll work out of one of our Phoenix, AZ or Charlotte, NC locations on a hybrid work schedule.\n\nIn this role, you will have a significant impact on the success of our S\/4HANA transformation by overseeing the development of data migration strategies, master data management frameworks, and analytics solutions. Your strategic vision will empower your team to innovate and deliver high-quality data solutions that enable seamless S\/4HANA deployment, drive business value, and ensure operational excellence across our global enterprise.\n\nKEY RESPONSIBILITIES\n\u2022 Lead and mentor a high-performing team focused on data engineering, data migration, master data management, and analytics for S\/4HANA transformation initiatives.\n\u2022 Develop and execute comprehensive data migration strategies including data extraction, transformation, cleansing, validation, and loading (ETL) for S\/4HANA deployment waves.\n\u2022 Oversee master data governance frameworks ensuring data quality, consistency, and alignment across materials, customers, vendors, financials, and organizational hierarchies within S\/4HANA.\n\u2022 Design and implement SAP-native and hybrid data architectures leveraging SAP Datasphere, SAP Data Intelligence, BW\/4HANA, and cloud data platforms to support analytics and reporting.\n\u2022 Collaborate with cross-functional teams including business process owners, functional consultants, and technical architects to define data requirements, mapping rules, and transformation logic.\n\u2022 Establish data quality frameworks with automated validation, reconciliation, and monitoring processes to ensure accuracy and integrity throughout migration and stabilization phases.\n\u2022 Lead the design of real-time and batch integration patterns for data flows between S\/4HANA and connected systems (CRM, PLM, MES, legacy ERP, analytics platforms).\n\u2022 Drive innovation in data engineering practices by implementing DataOps methodologies, automation tools, and AI-assisted data quality solutions.\n\u2022 Monitor and evaluate data engineering processes, making recommendations for improvements and ensuring compliance with enterprise data governance policies and industry best practices.\n\u2022 Partner with enterprise architecture and infrastructure teams to optimize data platform performance, scalability, and cloud integration strategies.\n\u2022 Manage stakeholder expectations and provide transparent reporting on data migration readiness, quality metrics, and risk mitigation activities.\n\nUS Person Requirement\n\nDue to compliance with U.S. export control laws and regulations, candidate must be a U.S. Person, which is defined as, a U.S. citizen, a U.S. permanent resident, or have protected status in the U.S. under asylum or refugee status or have the ability to obtain an export authorization.\n\nYOU MUST HAVE\n\u2022 Bachelor's degree in Computer Science, Engineering, Information Systems, or a related field\n\u2022 6+ years of experience in data engineering, data migration, or related fields, including a minimum of 3 years in a leadership role.\n\u2022 Strong expertise in data migration methodologies, data architecture, data modeling, and ETL\/ELT processes, specifically within SAP ERP or S\/4HANA environments.\n\u2022 Proficiency in SAP data migration tools and approaches including SAP Migration Cockpit, LTMC, LSMW, SAP Data Services, or S\/4HANA Migration Object Modeler.\n\u2022 Experience with programming languages such as Python, SQL, ABAP, Java, or Scala for data transformation and automation.\n\u2022 Hands-on experience with SAP data platforms (SAP Datasphere, BW\/4HANA, SAP HANA) and modern cloud data platforms (e.g., AWS, Azure, GCP data services).\n\u2022 Deep understanding of SAP master data structures, business objects, and data relationships across finance, supply chain, sales, and manufacturing modules.\n\u2022 Proven track record of successfully leading cross-functional data teams and building strong stakeholder relationships across business and IT organizations.\n\u2022 Experience with data quality management, data reconciliation frameworks, and automated testing approaches for large-scale data migrations.\n\nWE VALUE\n\u2022 Master's degree in Computer Science, Data Science, Information Systems, or a related field.\n\u2022 8+ years of experience in data engineering with specific focus on large-scale ERP data transformations.\n\u2022 Experience leading S\/4HANA data migration initiatives across multiple deployment waves or global rollouts.\n\u2022 Expertise in SAP Data Intelligence, SAP Datasphere, BW\/4HANA, or SAP Analytics Cloud implementation and architecture.\n\u2022 Familiarity with SAP Master Data Governance (MDG) and enterprise-wide master data management strategies.\n\u2022 Knowledge of big data technologies like Hadoop, Spark, Kafka, or cloud-native data processing frameworks.\n\u2022 Experience with DataOps, DevOps for data pipelines, CI\/CD automation, and Infrastructure as Code for data platforms.\n\u2022 Ability to communicate complex data concepts, migration strategies, and technical risks to non-technical stakeholders and executive leadership.\n\u2022 Strong problem-solving skills and a strategic mindset with ability to balance technical excellence with pragmatic delivery timelines.\n\u2022 Deep familiarity with data governance frameworks, data privacy regulations (GDPR, CCPA), and compliance standards in regulated industries.\n\u2022 Prior experience in Aerospace & Defense, Manufacturing, or other complex industries with high data volumes and stringent quality requirements.\n\u2022 Certifications such as SAP Certified Application Associate (Data Migration), AWS\/Azure\/GCP Data Engineering, or similar credentials.\n\n#HONAeroCareersS4HANA\n\nHoneywell helps organizations solve the world's most complex challenges in automation, the future of aviation and energy transition. As a trusted partner, we provide actionable solutions and innovation through our Aerospace Technologies, Building Automation, Energy and Sustainability Solutions, and Industrial Automation business segments \u2013 powered by our Honeywell Forge software \u2013 that help make the world smarter, safer and more sustainable.",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "Due to compliance with U.S. export control laws and regulations, candidate must be a U.S. Person, which is defined as, a U.S. citizen, a U.S. permanent resident, or have protected status in the U.S. under asylum or refugee status or have the ability to obtain an export authorization",
                    "Bachelor's degree in Computer Science, Engineering, Information Systems, or a related field",
                    "6+ years of experience in data engineering, data migration, or related fields, including a minimum of 3 years in a leadership role",
                    "Strong expertise in data migration methodologies, data architecture, data modeling, and ETL\/ELT processes, specifically within SAP ERP or S\/4HANA environments",
                    "Proficiency in SAP data migration tools and approaches including SAP Migration Cockpit, LTMC, LSMW, SAP Data Services, or S\/4HANA Migration Object Modeler",
                    "Experience with programming languages such as Python, SQL, ABAP, Java, or Scala for data transformation and automation",
                    "Hands-on experience with SAP data platforms (SAP Datasphere, BW\/4HANA, SAP HANA) and modern cloud data platforms (e.g., AWS, Azure, GCP data services)",
                    "Deep understanding of SAP master data structures, business objects, and data relationships across finance, supply chain, sales, and manufacturing modules",
                    "Proven track record of successfully leading cross-functional data teams and building strong stakeholder relationships across business and IT organizations",
                    "Experience with data quality management, data reconciliation frameworks, and automated testing approaches for large-scale data migrations",
                    "Master's degree in Computer Science, Data Science, Information Systems, or a related field",
                    "8+ years of experience in data engineering with specific focus on large-scale ERP data transformations",
                    "Experience leading S\/4HANA data migration initiatives across multiple deployment waves or global rollouts",
                    "Expertise in SAP Data Intelligence, SAP Datasphere, BW\/4HANA, or SAP Analytics Cloud implementation and architecture",
                    "Familiarity with SAP Master Data Governance (MDG) and enterprise-wide master data management strategies",
                    "Knowledge of big data technologies like Hadoop, Spark, Kafka, or cloud-native data processing frameworks",
                    "Experience with DataOps, DevOps for data pipelines, CI\/CD automation, and Infrastructure as Code for data platforms",
                    "Ability to communicate complex data concepts, migration strategies, and technical risks to non-technical stakeholders and executive leadership",
                    "Strong problem-solving skills and a strategic mindset with ability to balance technical excellence with pragmatic delivery timelines",
                    "Deep familiarity with data governance frameworks, data privacy regulations (GDPR, CCPA), and compliance standards in regulated industries",
                    "Prior experience in Aerospace & Defense, Manufacturing, or other complex industries with high data volumes and stringent quality requirements",
                    "Certifications such as SAP Certified Application Associate (Data Migration), AWS\/Azure\/GCP Data Engineering, or similar credentials"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "You will report directly to our IT Director of Delivery, and you'll work out of one of our Phoenix, AZ or Charlotte, NC locations on a hybrid work schedule",
                    "In this role, you will have a significant impact on the success of our S\/4HANA transformation by overseeing the development of data migration strategies, master data management frameworks, and analytics solutions",
                    "Your strategic vision will empower your team to innovate and deliver high-quality data solutions that enable seamless S\/4HANA deployment, drive business value, and ensure operational excellence across our global enterprise",
                    "Lead and mentor a high-performing team focused on data engineering, data migration, master data management, and analytics for S\/4HANA transformation initiatives",
                    "Develop and execute comprehensive data migration strategies including data extraction, transformation, cleansing, validation, and loading (ETL) for S\/4HANA deployment waves",
                    "Oversee master data governance frameworks ensuring data quality, consistency, and alignment across materials, customers, vendors, financials, and organizational hierarchies within S\/4HANA",
                    "Design and implement SAP-native and hybrid data architectures leveraging SAP Datasphere, SAP Data Intelligence, BW\/4HANA, and cloud data platforms to support analytics and reporting",
                    "Collaborate with cross-functional teams including business process owners, functional consultants, and technical architects to define data requirements, mapping rules, and transformation logic",
                    "Establish data quality frameworks with automated validation, reconciliation, and monitoring processes to ensure accuracy and integrity throughout migration and stabilization phases",
                    "Lead the design of real-time and batch integration patterns for data flows between S\/4HANA and connected systems (CRM, PLM, MES, legacy ERP, analytics platforms)",
                    "Drive innovation in data engineering practices by implementing DataOps methodologies, automation tools, and AI-assisted data quality solutions",
                    "Monitor and evaluate data engineering processes, making recommendations for improvements and ensuring compliance with enterprise data governance policies and industry best practices",
                    "Partner with enterprise architecture and infrastructure teams to optimize data platform performance, scalability, and cloud integration strategies",
                    "Manage stakeholder expectations and provide transparent reporting on data migration readiness, quality metrics, and risk mitigation activities"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"Indeed",
                "link":"https:\/\/www.indeed.com\/viewjob?jk=91e17e1b3ff29981&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"ZipRecruiter",
                "link":"https:\/\/www.ziprecruiter.com\/c\/Honeywell\/Job\/Sr-Data-Engineering-Manager\/-in-Phoenix,AZ?jid=3a729ba368487ea7&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Ladders",
                "link":"https:\/\/www.theladders.com\/job\/sr-data-engineering-architect-manager-avanade-phoenix-az_83762962?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"LinkedIn",
                "link":"https:\/\/www.linkedin.com\/jobs\/view\/sr-data-engineering-manager-at-honeywell-4334250913?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"SimplyHired",
                "link":"https:\/\/www.simplyhired.com\/job\/KX8o50gYCSXkVZlz9PLFyju3W7q-9VpfDeSmD43YjoTmD13BH-aM1Q?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Levels.fyi",
                "link":"https:\/\/www.levels.fyi\/jobs?jobId=142339535001264838&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Experteer",
                "link":"https:\/\/us.experteer.com\/career\/view-jobs\/sr-data-engineering-manager-phoenix-az-usa-54708942?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"BeBee",
                "link":"https:\/\/us.bebee.com\/job\/cbdab933c2ae57b1e1f615de6ef820ee?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Full-time"
    },
    {
        "title":"AI Data Engineer",
        "company_name":"Syracuse University",
        "location":"Syracuse, NY",
        "description":"About the position\n\nResponsibilities\n\u2022 Design and implement scalable pipelines that ingest, clean, transform, and aggregate data from diverse sources (ERP, LMS, research systems, APIs, and external datasets) into formats optimized for AI and analytics.\n\u2022 Ensure data quality, integrity, and reproducibility through robust engineering practices.\n\u2022 Build connectors, workflows, and APIs to unify and operationalize data across cloud and on-premises platforms.\n\u2022 Support deployment and lifecycle management of AI\/ML models, ensuring seamless integration with enterprise applications.\n\u2022 Maintain metadata, lineage, and documentation to support transparency and auditability.\n\u2022 Ensure adherence to Syracuse University's ISF, FERPA, HIPAA, and other regulatory requirements, while applying ethical AI and data governance principles.\n\u2022 Partner with academic and administrative units to understand AI and data needs, delivering tailored solutions aligned with institutional priorities.\n\u2022 Act as a liaison between AI\/ML teams and business units to maximize value from data assets.\n\u2022 Provide technical mentorship on data engineering and integration best practices.\n\u2022 Stay ahead of emerging technologies (e.g., MCP, serverless, containerization, and generative AI) to drive innovation in data and AI adoption.\n\nRequirements\n\u2022 Bachelor's degree in Artificial Intelligence, Computer Science, Data Science, or related field, or equivalent combination of education and experience.\n\u2022 4+ years of experience in data engineering, AI\/ML integration, or enterprise IT.\n\u2022 Experience in higher education IT environments preferred.\n\u2022 Expertise in data wrangling for structured and unstructured data.\n\u2022 Proficiency in SQL and at least one programming language (Python, Java, or C#).\n\u2022 Familiarity with API development, microservices, and Model Context Protocol (MCP) integrations.\n\u2022 Experience with cloud infrastructure (Azure, AWS, GCP), containerization (Docker, Kubernetes), and serverless platforms (e.g., Logic Apps).\n\u2022 Familiarity in deploying AI\/ML platforms (Azure AI Foundry, Google Vertex, Amazon Bedrock, OpenAI, etc.).\n\u2022 Understanding of data governance, privacy, and ethical AI principles.\n\u2022 Strong problem-solving, communication, and collaboration skills.",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "Bachelor's degree in Artificial Intelligence, Computer Science, Data Science, or related field, or equivalent combination of education and experience",
                    "4+ years of experience in data engineering, AI\/ML integration, or enterprise IT",
                    "Expertise in data wrangling for structured and unstructured data",
                    "Proficiency in SQL and at least one programming language (Python, Java, or C#)",
                    "Familiarity with API development, microservices, and Model Context Protocol (MCP) integrations",
                    "Experience with cloud infrastructure (Azure, AWS, GCP), containerization (Docker, Kubernetes), and serverless platforms (e.g., Logic Apps)",
                    "Familiarity in deploying AI\/ML platforms (Azure AI Foundry, Google Vertex, Amazon Bedrock, OpenAI, etc.)",
                    "Understanding of data governance, privacy, and ethical AI principles",
                    "Strong problem-solving, communication, and collaboration skills"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "Design and implement scalable pipelines that ingest, clean, transform, and aggregate data from diverse sources (ERP, LMS, research systems, APIs, and external datasets) into formats optimized for AI and analytics",
                    "Ensure data quality, integrity, and reproducibility through robust engineering practices",
                    "Build connectors, workflows, and APIs to unify and operationalize data across cloud and on-premises platforms",
                    "Support deployment and lifecycle management of AI\/ML models, ensuring seamless integration with enterprise applications",
                    "Maintain metadata, lineage, and documentation to support transparency and auditability",
                    "Ensure adherence to Syracuse University's ISF, FERPA, HIPAA, and other regulatory requirements, while applying ethical AI and data governance principles",
                    "Partner with academic and administrative units to understand AI and data needs, delivering tailored solutions aligned with institutional priorities",
                    "Act as a liaison between AI\/ML teams and business units to maximize value from data assets",
                    "Provide technical mentorship on data engineering and integration best practices",
                    "Stay ahead of emerging technologies (e.g., MCP, serverless, containerization, and generative AI) to drive innovation in data and AI adoption"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"Teal",
                "link":"https:\/\/www.tealhq.com\/job\/ai-data-engineer_64348bb2-2cdf-4b17-ae0f-4e7863c45420?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Bandana.com",
                "link":"https:\/\/bandana.com\/jobs\/b694efb6-f49c-41d3-8f7a-7bc6f716fe33?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"National War College Alumni Association (NWC), NWC Career Center",
                "link":"https:\/\/careers.nationalwarcollege.org\/jobs\/21700172\/ai-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Jobs.weekday.works",
                "link":"https:\/\/jobs.weekday.works\/syracuse-university-ai-data-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"BeBee",
                "link":"https:\/\/us.bebee.com\/job\/1dc3ba5a3c12e5e4295a92260c341e99?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Full-time"
    },
    {
        "title":"Sr. Data Engineer",
        "company_name":"CVS Health",
        "location":"Silver Springs, NV",
        "description":"At CVS Health, we're building a world of health around every consumer and surrounding ourselves with dedicated colleagues who are passionate about transforming health care.\nAs the nation's leading health solutions company, we reach millions of Americans through our local presence, digital channels and more than 300,000 purpose-driven colleagues - caring for people where, when and how they choose in a way that is uniquely more connected, more convenient and more compassionate. And we do it all with heart, each and every day.\n\u200b **Position Summary**\nWe're seeking a Sr. Data Engineer to design and implement data pipelines that power analytical capabilities. This hands-on role requires an understanding of data engineering best practices and the ability to translate business requirements into technical solutions.\nYou will be part of a dedicated team creating datasets for analytic and data science workloads.\nKey Responsibilities:\n+ Data Pipeline Development: Design and build ETL\/ELT data pipelines to ingest, process, and transform large datasets from multiple sources.\n+ Performance Optimization: Implement best practices for performance tuning, partitioning, and clustering to optimize data queries and reduce costs.\n+ Data Quality & Governance: Establish and enforce data quality standards, data governance frameworks, and security policies for data storage and access.\n+ Data Modeling & Architecture: Develop and optimize data models and schemas to support analytics, reporting, and machine learning requirements.\n+ Data Integration & Transformation: Collaborate with data scientists and analysts to design data solutions that integrate with BI tools and machine learning models.\n+ Documentation & Knowledge Sharing: Create comprehensive documentation for data pipelines, workflows, and processes. Share best practices and mentor junior data engineers.\n+ Design and architect data infrastructure analytical workloads.\n\u2022 *Required Qualifications**\n+ 5+ years of applicable work experience\n+ Proficiency in Python, specifically with ETL pipelines.\n+ Strong proficiency in SQL and experience in developing complex queries.\n+ Familiarity with pySpark, DBT, or other similar frameworks.\n+ Experience deploying data pipelines in a cloud environment (Azure, AWS, GCP).\n+ Understanding of data warehousing concepts, dimensional modeling, and building data marts.\n+ Excellent communication and interpersonal skills, with the ability to collaborate effectively with data scientists, analysts, and product owners.\n\u2022 *Preferred Qualifications**\n+ Knowledge of data governance best practices in a cloud environment.\n+ Experience with machine learning flows on GCP.\n+ Experience with data design in BigQuery\n+ Experience working with the Epic data model.\n+ Experience working with healthcare data (Tuva or OMOP Models a plus).\n\u2022 *Education and Experience**\n+ College degree or certification in related fields\n\u2022 *Anticipated Weekly Hours**\n40\n\u2022 *Time Type**\nFull time\n\u2022 *Pay Range**\nThe typical pay range for this role is:\n$83,430.00 - $222,480.00\nThis pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above.\nOur people fuel our future. Our teams reflect the customers, patients, members and communities we serve and we are committed to fostering a workplace where every colleague feels valued and that they belong.\n\u2022 *Great benefits for great people**\nWe take pride in our comprehensive and competitive mix of pay and benefits - investing in the physical, emotional and financial wellness of our colleagues and their families to help them be the healthiest they can be. In addition to our competitive wages, our great benefits include:\n+ **Affordable medical plan options,** a **401(k) plan** (including matching company contributions), and an **employee stock purchase plan** .\n+ **No-cost programs for all colleagues** including wellness screenings, tobacco cessation and weight management programs, confidential counseling and financial coaching.\n+ **Benefit solutions that address the different needs and preferences of our colleagues** including paid time off, flexible work schedules, family leave, dependent care resources, colleague assistance programs, tuition assistance, retiree medical access and many other benefits depending on eligibility.\nFor more information, visit anticipate the application window for this opening will close on: 11\/14\/2025\nQualified applicants with arrest or conviction records will be considered for employment in accordance with all federal, state and local laws.\nWe are an equal opportunity and affirmative action employer. We do not discriminate in recruiting, hiring, promotion, or any other personnel action based on race, ethnicity, color, national origin, sex\/gender, sexual orientation, gender identity or expression, religion, age, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law.",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "This hands-on role requires an understanding of data engineering best practices and the ability to translate business requirements into technical solutions",
                    "5+ years of applicable work experience",
                    "Proficiency in Python, specifically with ETL pipelines",
                    "Strong proficiency in SQL and experience in developing complex queries",
                    "Familiarity with pySpark, DBT, or other similar frameworks",
                    "Experience deploying data pipelines in a cloud environment (Azure, AWS, GCP)",
                    "Understanding of data warehousing concepts, dimensional modeling, and building data marts",
                    "Excellent communication and interpersonal skills, with the ability to collaborate effectively with data scientists, analysts, and product owners",
                    "Knowledge of data governance best practices in a cloud environment",
                    "Experience with machine learning flows on GCP",
                    "Experience with data design in BigQuery",
                    "Experience working with the Epic data model",
                    "College degree or certification in related fields"
                ]
            },
            {
                "title":"Benefits",
                "items":[
                    "*Pay Range**",
                    "$83,430.00 - $222,480.00",
                    "This pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls",
                    "The actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors",
                    "This position is eligible for a CVS Health bonus, commission or short-term incentive program in addition to the base pay range listed above",
                    "*Great benefits for great people**",
                    "We take pride in our comprehensive and competitive mix of pay and benefits - investing in the physical, emotional and financial wellness of our colleagues and their families to help them be the healthiest they can be",
                    "In addition to our competitive wages, our great benefits include:",
                    "**Affordable medical plan options,** a **401(k) plan** (including matching company contributions), and an **employee stock purchase plan** ",
                    "**No-cost programs for all colleagues** including wellness screenings, tobacco cessation and weight management programs, confidential counseling and financial coaching",
                    "**Benefit solutions that address the different needs and preferences of our colleagues** including paid time off, flexible work schedules, family leave, dependent care resources, colleague assistance programs, tuition assistance, retiree medical access and many other benefits depending on eligibility"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "Data Engineer to design and implement data pipelines that power analytical capabilities",
                    "You will be part of a dedicated team creating datasets for analytic and data science workloads",
                    "Data Pipeline Development: Design and build ETL\/ELT data pipelines to ingest, process, and transform large datasets from multiple sources",
                    "Performance Optimization: Implement best practices for performance tuning, partitioning, and clustering to optimize data queries and reduce costs",
                    "Data Quality & Governance: Establish and enforce data quality standards, data governance frameworks, and security policies for data storage and access",
                    "Data Modeling & Architecture: Develop and optimize data models and schemas to support analytics, reporting, and machine learning requirements",
                    "Data Integration & Transformation: Collaborate with data scientists and analysts to design data solutions that integrate with BI tools and machine learning models",
                    "Documentation & Knowledge Sharing: Create comprehensive documentation for data pipelines, workflows, and processes",
                    "Share best practices and mentor junior data engineers",
                    "Design and architect data infrastructure analytical workloads",
                    "*Anticipated Weekly Hours**"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"WhatJobs",
                "link":"https:\/\/www.whatjobs.com\/jobs\/sr-data-engineer?id=2251585557&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Full-time"
    },
    {
        "title":"Data Engineer",
        "company_name":"Veracity Insurance Solutions",
        "location":"Pleasant Grove, UT",
        "description":"At Veracity, we aim to be a different kind of insurance partner - one that is free from outside investors, venture capital, or the pressures of a corporate parent.\n\nOurs is a culture of empowerment - one that believes in effort, results, and accountability. We believe that transparency fosters trust, trust fosters growth, and that growth drives innovation. Our commitment to rigorous evaluation and relentless execution lead to rapid evolution.\n\nWe answer only to the small business owners we serve, and this independence allows us to stay focused on what matters most: helping their businesses thrive by providing expert guidance and best-in-class insurance policies.\n\nWe're growing fast and want you to be a part of it!\n\nWe're seeking a highly skilled Data Engineer to join our team on a hybrid schedule. Reporting to the Director of Data Services and Workflow this role will be responsible for driving the development of scalable data infrastructure and pipelines that drive advanced analytics and AI initiatives. This role plays a pivotal part in empowering data science and machine learning teams by delivering reliable, well-structured, and production-ready datasets optimized for model training, evaluation, and deployment.\n\nKey Responsibilities\n\u2022 Design, build, and maintain robust, scalable data pipelines and ETL processes\n\u2022 Collaborate with stakeholders to define data requirements for model development and ensure timely delivery of clean, labeled, and feature-rich datasets\n\u2022 Develop and optimize data architectures to support structured, semi-structured, and unstructured data sources\n\u2022 Ensure data quality, integrity, and compliance with governance, security, and privacy standards\n\u2022 Monitor, troubleshoot, and optimize data workflows and infrastructure performance\n\u2022 Establish and enforce best practices for data modeling, documentation, and team-wide engineering standards\n\u2022 Evaluate and integrate emerging tools and technologies to strengthen data engineering capabilities, with a focus on enabling AI\/ML use cases\n\u2022 Required to perform other duties as requested, directed, or assigned\nRequirements and Qualifications\nRequired\n\u2022 4+ years of proven experience in data engineering or a closely related role\n\u2022 Strong proficiency in SQL and programming languages such as Python, Scala, or Java\n\u2022 Hands-on experience with cloud platforms (e.g., AWS, Azure, GCP) and modern data warehousing solutions (e.g., Snowflake, Redshift, BigQuery)\n\u2022 Deep understanding of data modeling, ETL frameworks\/tools (Fivetran), and distributed data systems\n\u2022 Demonstrated experience working with large-scale datasets to support AI\/ML applications\nPreferred\n\u2022 Bachelor's or Master's degree in Computer Science, Engineering, or a related field\n\u2022 Knowledge of feature engineering, data labeling, and model monitoring pipelines\n\u2022 Proficiency with workflow orchestration tools (e.g., Airflow, Prefect, AWS Glue) and version control systems (e.g., Git)\n\u2022 Exposure to MLOps tools and practices for scaling and operationalizing ML workflows\n\u2022 Understanding of data privacy regulations, governance, and ethical AI principles\nPerks\n\u2022 Health, dental, and vision plans\n\u2022 Amazing work-life balance with 4 weeks of Paid Time Off\n\u2022 10 Paid Company Holidays with 2 floating holidays\n\u2022 401K Programs with employer match\n\u2022 Personal assistance programs for support in a healthy personal and work life\nWhy Veracity?\n\nHere at Veracity, you'll be part of a team of trailblazers and visionaries. We're not just revolutionizing the way people \"do\" insurance; we are creating a whole new paradigm. Here, you will experience a vibrant and inclusive workplace where your ideas matter! With us, you have a chance to:\n\u2022 Engage in groundbreaking projects that are reshaping the insurance landscape\n\u2022 Collaborate with a group of dedicated, like-minded professionals\n\u2022 Experience a culture that prioritizes growth and development\nCompensation Range: $100k\/yr - $115k\/yr\n\nWe are proud to be an equal-opportunity employer. We are committed to providing equal opportunities to all qualified applicants, regardless of race, color, religion, sex, national origin, disability, or any other legally protected characteristics.\n\nIf you need accommodation, please let us know during the interview process.",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "4+ years of proven experience in data engineering or a closely related role",
                    "Strong proficiency in SQL and programming languages such as Python, Scala, or Java",
                    "Hands-on experience with cloud platforms (e.g., AWS, Azure, GCP) and modern data warehousing solutions (e.g., Snowflake, Redshift, BigQuery)",
                    "Deep understanding of data modeling, ETL frameworks\/tools (Fivetran), and distributed data systems",
                    "Demonstrated experience working with large-scale datasets to support AI\/ML applications"
                ]
            },
            {
                "title":"Benefits",
                "items":[
                    "Perks",
                    "Health, dental, and vision plans",
                    "Amazing work-life balance with 4 weeks of Paid Time Off",
                    "10 Paid Company Holidays with 2 floating holidays",
                    "401K Programs with employer match",
                    "Personal assistance programs for support in a healthy personal and work life",
                    "Engage in groundbreaking projects that are reshaping the insurance landscape",
                    "Experience a culture that prioritizes growth and development",
                    "Compensation Range: $100k\/yr - $115k\/yr"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "Reporting to the Director of Data Services and Workflow this role will be responsible for driving the development of scalable data infrastructure and pipelines that drive advanced analytics and AI initiatives",
                    "This role plays a pivotal part in empowering data science and machine learning teams by delivering reliable, well-structured, and production-ready datasets optimized for model training, evaluation, and deployment",
                    "Design, build, and maintain robust, scalable data pipelines and ETL processes",
                    "Collaborate with stakeholders to define data requirements for model development and ensure timely delivery of clean, labeled, and feature-rich datasets",
                    "Develop and optimize data architectures to support structured, semi-structured, and unstructured data sources",
                    "Ensure data quality, integrity, and compliance with governance, security, and privacy standards",
                    "Monitor, troubleshoot, and optimize data workflows and infrastructure performance",
                    "Establish and enforce best practices for data modeling, documentation, and team-wide engineering standards",
                    "Evaluate and integrate emerging tools and technologies to strengthen data engineering capabilities, with a focus on enabling AI\/ML use cases",
                    "Required to perform other duties as requested, directed, or assigned",
                    "Collaborate with a group of dedicated, like-minded professionals"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"ZipRecruiter",
                "link":"https:\/\/www.ziprecruiter.com\/c\/Veracity-Insurance-Solutions\/Job\/Data-Engineer\/-in-Pleasant-Grove,UT?jid=418fcd1f23e0c5ff&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Glassdoor",
                "link":"https:\/\/www.glassdoor.com\/job-listing\/data-engineer-veracity-insurance-JV_IC1157847_KO0,13_KE14,32.htm?jl=1009888684148&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"EP Wealth Advisors, LLC",
                "link":"https:\/\/jobiak.epwealth.com\/careers\/data-engineer-pleasant-grove-ut-690345ea5d24500a56fa34ac?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Monster",
                "link":"https:\/\/www.monster.com\/job-openings\/data-engineer-pleasant-grove-ut--42a91f91-1226-457b-9b49-8cd5fa13a771?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"BeBee",
                "link":"https:\/\/us.bebee.com\/job\/1a64baa6562d6c974dd0e8088ce1dbc2?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Talent.com",
                "link":"https:\/\/www.talent.com\/view?id=2fc5c738c664&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Lensa",
                "link":"https:\/\/lensa.com\/job-v1\/veracity-insurance\/pleasant-grove-ut\/data-engineer\/3ffa6099726584e1dd1fff0b81e982eb?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Jobs Trabajo.org",
                "link":"https:\/\/us.trabajo.org\/job-3630-c625476b11f3308e718d2d1e68055421?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Full-time"
    },
    {
        "title":"Databricks Data Engineer - Senior - Consulting - Location Open",
        "company_name":"EY",
        "location":"New Jersey",
        "description":"Location: Anywhere in Country\n\nAt EY, we\u2019re all in to shape your future with confidence.\n\nWe\u2019ll help you succeed in a globally connected powerhouse of diverse teams and take your career wherever you want it to go. Join EY and help to build a better working world.\n\nTechnology \u2013 Data and Decision Science \u2013 Data Engineering \u2013 Senior\n\nWe are seeking a highly skilled Senior Consultant Data Engineer with expertise in cloud data engineering, specifically Databricks. The ideal candidate will have strong client management and communication skills, along with a proven track record of successful end-to-end implementations in data engineering projects.\n\nThe opportunity\n\nIn this role, you will design and build analytics solutions that deliver significant business value. You will collaborate with other data and analytics professionals, management, and stakeholders to ensure that technical requirements align with business needs. Your responsibilities will include creating scalable data architecture and modeling solutions that support the entire data asset lifecycle.\n\nYour Key Responsibilities\n\nAs a Senior Data Engineer, you will play a pivotal role in transforming data into actionable insights. Your time will be spent on various responsibilities, including:\n\u2022 Designing, building, and operating scalable on-premises or cloud data architecture.\n\u2022 Analyzing business requirements and translating them into technical specifications.\n\u2022 Optimizing data flows for target data platform designs.\n\u2022 Design, develop, and implement data engineering solutions using Databricks on cloud platforms (e.g., AWS, Azure, GCP).\n\u2022 Collaborate with clients to understand their data needs and provide tailored solutions that meet their business objectives.\n\u2022 Lead end-to-end data pipeline development, including data ingestion, transformation, and storage.\n\u2022 Ensure data quality, integrity, and security throughout the data lifecycle.\n\u2022 Provide technical guidance and mentorship to junior data engineers and team members.\n\u2022 Communicate effectively with stakeholders, including technical and non-technical audiences, to convey complex data concepts.\n\u2022 Manage client relationships and expectations, ensuring high levels of satisfaction and engagement.\n\u2022 Stay updated with the latest trends and technologies in data engineering and cloud computing.\n\nThis role offers the opportunity to work with cutting-edge technologies and stay ahead of industry trends, ensuring you gain a competitive advantage in the market. The position may require regular travel to meet with external clients.\n\nSkills And Attributes For Success\n\nTo thrive in this role, you will need a blend of technical and interpersonal skills. Your ability to communicate effectively and build relationships will be crucial. Here are some key attributes we look for:\n\u2022 Strong analytical and decision-making skills.\n\u2022 Proficiency in cloud computing and data architecture design.\n\u2022 Experience in data integration and security.\n\u2022 Ability to manage complex problem-solving scenarios.\n\nTo qualify for the role, you must have\n\u2022 A Bachelor\u2019s degree in Computer Science, Engineering, or a related field required (4-year degree). Master\u2019s degree preferred\n\u2022 Typically, no less than 2 - 4 years relevant experience in data engineering, with a focus on cloud data solutions.\n\u2022 5+ years of experience in data engineering, with a focus on cloud data solutions.\n\u2022 Expertise in Databricks and experience with Spark for big data processing.\n\u2022 Proven experience in at least two end-to-end data engineering implementations, including:\n\u2022 Implementation of a data lake solution using Databricks, integrating various data sources, and enabling analytics for business intelligence.\n\u2022 Development of a real-time data processing pipeline using Databricks and cloud services, delivering insights for operational decision-making.\n\u2022 Strong programming skills in languages such as Python, Scala, or SQL.\n\u2022 Experience with data modeling, ETL processes, and data warehousing concepts.\n\u2022 Excellent problem-solving skills and the ability to work independently and as part of a team.\n\u2022 Strong communication and interpersonal skills, with a focus on client management.\n\nRequired Expertise for Senior Consulting Projects:\n\u2022 Strategic Thinking: Ability to align data engineering solutions with business strategies and objectives.\n\u2022 Project Management: Experience in managing multiple projects simultaneously, ensuring timely delivery and adherence to project scope.\n\u2022 Stakeholder Engagement: Proficiency in engaging with various stakeholders, including executives, to understand their needs and present solutions effectively.\n\u2022 Change Management: Skills in guiding clients through change processes related to data transformation and technology adoption.\n\u2022 Risk Management: Ability to identify potential risks in data projects and develop mitigation strategies.\n\u2022 Technical Leadership: Experience in leading technical discussions and making architectural decisions that impact project outcomes.\n\u2022 Documentation and Reporting: Proficiency in creating comprehensive documentation and reports to communicate project progress and outcomes to clients.\n\nIdeally, you\u2019ll also have\n\u2022 Experience with data quality management.\n\u2022 Familiarity with semantic layers in data architecture.\n\u2022 Familiarity with cloud platforms (AWS, Azure, GCP) and their data services.\n\u2022 Knowledge of data governance and compliance standards.\n\u2022 Experience with machine learning frameworks and tools.\n\nWhat We Look For\n\nWe seek individuals who are not only technically proficient but also possess the qualities of top performers. You should be adaptable, collaborative, and driven by a desire to achieve excellence in every project you undertake.\n\nFY26NATAID\n\nWhat We Offer You\n\nAt EY, we\u2019ll develop you with future-focused skills and equip you with world-class experiences. We\u2019ll empower you in a flexible environment, and fuel you and your extraordinary talents in a diverse and inclusive culture of globally connected teams. Learn more.\n\u2022 We offer a comprehensive compensation and benefits package where you\u2019ll be rewarded based on your performance and recognized for the value you bring to the business. The base salary range for this job in all geographic locations in the US is $106,900 to $176,500. The base salary range for New York City Metro Area, Washington State and California (excluding Sacramento) is $128,400 to $200,600. Individual salaries within those ranges are determined through a wide variety of factors including but not limited to education, experience, knowledge, skills and geography. In addition, our Total Rewards package includes medical and dental coverage, pension and 401(k) plans, and a wide range of paid time off options.\n\u2022 Join us in our team-led and leader-enabled hybrid model. Our expectation is for most people in external, client serving roles to work together in person 40-60% of the time over the course of an engagement, project or year.\n\u2022 Under our flexible vacation policy, you\u2019ll decide how much vacation time you need based on your own personal circumstances. You\u2019ll also be granted time off for designated EY Paid Holidays, Winter\/Summer breaks, Personal\/Family Care, and other leaves of absence when needed to support your physical, financial, and emotional well-being.\n\nAre you ready to shape your future with confidence? Apply today.\n\nEY accepts applications for this position on an on-going basis.\n\nFor those living in California, please click here for additional information.\n\nEY focuses on high-ethical standards and integrity among its employees and expects all candidates to demonstrate these qualities.\n\nEY | Building a better working world\n\nEY is building a better working world by creating new value for clients, people, society and the planet, while building trust in capital markets.\n\nEnabled by data, AI and advanced technology, EY teams help clients shape the future with confidence and develop answers for the most pressing issues of today and tomorrow.\n\nEY teams work across a full spectrum of services in assurance, consulting, tax, strategy and transactions. Fueled by sector insights, a globally connected, multi-disciplinary network and diverse ecosystem partners, EY teams can provide services in more than 150 countries and territories.\n\nEY provides equal employment opportunities to applicants and employees without regard to race, color, religion, age, sex, sexual orientation, gender identity\/expression, pregnancy, genetic information, national origin, protected veteran status, disability status, or any other legally protected basis, including arrest and conviction records, in accordance with applicable law.\u202f\n\nEY is committed to providing reasonable accommodation to qualified individuals with disabilities including veterans with disabilities. If you have a disability and either need assistance applying online or need to request an accommodation during any part of the application process, please call 1-800-EY-HELP3, select Option 2 for candidate related inquiries, then select Option 1 for candidate queries and finally select Option 2 for candidates with an inquiry which will route you to EY\u2019s Talent Shared Services Team (TSS) or email the TSS at ssc.customersupport@ey.com.",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "We are seeking a highly skilled Senior Consultant Data Engineer with expertise in cloud data engineering, specifically Databricks",
                    "The ideal candidate will have strong client management and communication skills, along with a proven track record of successful end-to-end implementations in data engineering projects",
                    "To thrive in this role, you will need a blend of technical and interpersonal skills",
                    "Your ability to communicate effectively and build relationships will be crucial",
                    "Strong analytical and decision-making skills",
                    "Proficiency in cloud computing and data architecture design",
                    "Experience in data integration and security",
                    "Ability to manage complex problem-solving scenarios",
                    "A Bachelor\u2019s degree in Computer Science, Engineering, or a related field required (4-year degree)",
                    "Required Expertise for Senior Consulting Projects:",
                    "Strategic Thinking: Ability to align data engineering solutions with business strategies and objectives",
                    "Project Management: Experience in managing multiple projects simultaneously, ensuring timely delivery and adherence to project scope",
                    "Stakeholder Engagement: Proficiency in engaging with various stakeholders, including executives, to understand their needs and present solutions effectively",
                    "Change Management: Skills in guiding clients through change processes related to data transformation and technology adoption",
                    "Risk Management: Ability to identify potential risks in data projects and develop mitigation strategies",
                    "Technical Leadership: Experience in leading technical discussions and making architectural decisions that impact project outcomes",
                    "Experience with data quality management",
                    "Familiarity with semantic layers in data architecture",
                    "Familiarity with cloud platforms (AWS, Azure, GCP) and their data services",
                    "Knowledge of data governance and compliance standards",
                    "Experience with machine learning frameworks and tools",
                    "You should be adaptable, collaborative, and driven by a desire to achieve excellence in every project you undertake"
                ]
            },
            {
                "title":"Benefits",
                "items":[
                    "At EY, we\u2019ll develop you with future-focused skills and equip you with world-class experiences",
                    "We offer a comprehensive compensation and benefits package where you\u2019ll be rewarded based on your performance and recognized for the value you bring to the business",
                    "The base salary range for this job in all geographic locations in the US is $106,900 to $176,500",
                    "The base salary range for New York City Metro Area, Washington State and California (excluding Sacramento) is $128,400 to $200,600",
                    "Individual salaries within those ranges are determined through a wide variety of factors including but not limited to education, experience, knowledge, skills and geography",
                    "In addition, our Total Rewards package includes medical and dental coverage, pension and 401(k) plans, and a wide range of paid time off options",
                    "Under our flexible vacation policy, you\u2019ll decide how much vacation time you need based on your own personal circumstances",
                    "You\u2019ll also be granted time off for designated EY Paid Holidays, Winter\/Summer breaks, Personal\/Family Care, and other leaves of absence when needed to support your physical, financial, and emotional well-being"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "In this role, you will design and build analytics solutions that deliver significant business value",
                    "You will collaborate with other data and analytics professionals, management, and stakeholders to ensure that technical requirements align with business needs",
                    "Your responsibilities will include creating scalable data architecture and modeling solutions that support the entire data asset lifecycle",
                    "As a Senior Data Engineer, you will play a pivotal role in transforming data into actionable insights",
                    "Your time will be spent on various responsibilities, including:",
                    "Designing, building, and operating scalable on-premises or cloud data architecture",
                    "Analyzing business requirements and translating them into technical specifications",
                    "Optimizing data flows for target data platform designs",
                    "Design, develop, and implement data engineering solutions using Databricks on cloud platforms (e.g., AWS, Azure, GCP)",
                    "Collaborate with clients to understand their data needs and provide tailored solutions that meet their business objectives",
                    "Lead end-to-end data pipeline development, including data ingestion, transformation, and storage",
                    "Ensure data quality, integrity, and security throughout the data lifecycle",
                    "Provide technical guidance and mentorship to junior data engineers and team members",
                    "Communicate effectively with stakeholders, including technical and non-technical audiences, to convey complex data concepts",
                    "Manage client relationships and expectations, ensuring high levels of satisfaction and engagement",
                    "Stay updated with the latest trends and technologies in data engineering and cloud computing",
                    "This role offers the opportunity to work with cutting-edge technologies and stay ahead of industry trends, ensuring you gain a competitive advantage in the market",
                    "The position may require regular travel to meet with external clients",
                    "Documentation and Reporting: Proficiency in creating comprehensive documentation and reports to communicate project progress and outcomes to clients"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"LinkedIn",
                "link":"https:\/\/www.linkedin.com\/jobs\/view\/databricks-data-engineer-senior-consulting-location-open-at-ey-4282841810?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Jobs",
                "link":"https:\/\/ey.jobs\/iselin-nj\/databricks-data-engineer-senior-consulting-location-open\/40389A0AF6214AEC945A2FC47B393E24\/job\/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Full-time"
    },
    {
        "title":"Data Engineer (Analytics)",
        "company_name":"Meta",
        "location":"Santa Fe, NM",
        "description":"Summary:\n\nMeta Platforms, Inc. (Meta), formerly known as Facebook Inc., builds technologies that help people connect, find communities, and grow businesses. When Facebook launched in 2004, it changed the way people connect. Apps and services like Messenger, Instagram, and WhatsApp further empowered billions around the world. Now, Meta is moving beyond 2D screens toward immersive experiences like augmented and virtual reality to help build the next evolution in social technology. To apply, click \u201cApply to Job\u201d online on this web page.\n\nRequired Skills:\n\nData Engineer (Analytics) Responsibilities:\n\u2022 Design, model, and implement data warehousing activities to deliver the data foundation that drives impact through informed decision making.\n\u2022 Design, build and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains.\n\u2022 Collaborate with engineers, product managers and data scientists to understand data needs, representing key data insights visually in a meaningful way.\n\u2022 Define and manage SLA for all data sets in allocated areas of ownership.\n\u2022 Create and contribute to frameworks that improve the efficacy of logging data, while working with data infrastructure to triage issues and resolve.\n\u2022 Determine and implement the security model based on privacy requirements, confirm safeguards are followed, address data quality issues, and evolve governance processes within allocated areas of ownership.\n\u2022 Solve challenging data integration problems utilizing optimal ETL patterns, frameworks, query techniques, and sourcing from structured and unstructured data sources.\n\u2022 Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts.\n\u2022 Influence product and cross-functional teams to identify data opportunities to drive impact.\n\u2022 Work on problems of diverse scope where analysis of data requires evaluation of identifiable factors.\n\u2022 Demonstrate good judgment in selecting methods and techniques for obtaining solutions.\n\u2022 Telecommuting is permitted from anywhere in the U.S.\n\u2022 Domestic and International Travel Required 5%.\n\nMinimum Qualifications:\n\nMinimum Qualifications:\n\u2022 Bachelor's degree (or foreign equivalent) in Computer Science, Engineering, Information Systems, Mathematics, Statistics, Data Analytics or related field and 5 years of progressive, post- baccalaureate work experience in the job offered or in a related occupation\n\u2022 Experience must include 5 years in the following:\n\u2022 * Features, design, and use-case scenarios across a big data ecosystem\n\u2022 * Custom ETL design, implementation, and maintenance\n\u2022 * Object-oriented programming languages\n\u2022 * Schema design and dimensional data modeling\n\u2022 * Writing SQL statements\n\u2022 * Analyzing data to identify deliverables, gaps and inconsistencies\n\u2022 * Managing and communicating data warehouse plans to internal clients\n\u2022 * MapReduce or MPP system\n\u2022 * Python\n\nPublic Compensation:\n\n$243,665\/year to $279,400\/year + bonus + equity + benefits\n\nIndustry: Internet\n\nEqual Opportunity:\n\nMeta is proud to be an Equal Employment Opportunity and Affirmative Action employer. We do not discriminate based upon race, religion, color, national origin, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law. Meta participates in the E-Verify program in certain locations, as required by law. Please note that Meta may leverage artificial intelligence and machine learning technologies in connection with applications for employment.\n\nMeta is committed to providing reasonable accommodations for candidates with disabilities in our recruiting process. If you need any assistance or accommodations due to a disability, please let us know at accommodations-ext@fb.com.",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "Bachelor's degree (or foreign equivalent) in Computer Science, Engineering, Information Systems, Mathematics, Statistics, Data Analytics or related field and 5 years of progressive, post- baccalaureate work experience in the job offered or in a related occupation",
                    "Experience must include 5 years in the following:",
                    "Features, design, and use-case scenarios across a big data ecosystem",
                    "Custom ETL design, implementation, and maintenance",
                    "Object-oriented programming languages",
                    "Schema design and dimensional data modeling",
                    "Writing SQL statements",
                    "Analyzing data to identify deliverables, gaps and inconsistencies",
                    "Managing and communicating data warehouse plans to internal clients",
                    "MapReduce or MPP system",
                    "Python",
                    "Meta participates in the E-Verify program in certain locations, as required by law"
                ]
            },
            {
                "title":"Benefits",
                "items":[
                    "$243,665\/year to $279,400\/year + bonus + equity + benefits"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "Design, model, and implement data warehousing activities to deliver the data foundation that drives impact through informed decision making",
                    "Design, build and launch collections of sophisticated data models and visualizations that support multiple use cases across different products or domains",
                    "Collaborate with engineers, product managers and data scientists to understand data needs, representing key data insights visually in a meaningful way",
                    "Define and manage SLA for all data sets in allocated areas of ownership",
                    "Create and contribute to frameworks that improve the efficacy of logging data, while working with data infrastructure to triage issues and resolve",
                    "Determine and implement the security model based on privacy requirements, confirm safeguards are followed, address data quality issues, and evolve governance processes within allocated areas of ownership",
                    "Solve challenging data integration problems utilizing optimal ETL patterns, frameworks, query techniques, and sourcing from structured and unstructured data sources",
                    "Optimize pipelines, dashboards, frameworks, and systems to facilitate easier development of data artifacts",
                    "Influence product and cross-functional teams to identify data opportunities to drive impact",
                    "Work on problems of diverse scope where analysis of data requires evaluation of identifiable factors",
                    "Demonstrate good judgment in selecting methods and techniques for obtaining solutions",
                    "Telecommuting is permitted from anywhere in the U.S",
                    "Domestic and International Travel Required 5%"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"Jobs",
                "link":"https:\/\/metacareers.dejobs.org\/santa-fe-nm\/data-engineer-analytics\/0C4CF9AB2AF3432E89B6037A4ABE65DF\/job\/?vs=25&utm_source=RR+RSS+Feed-DE&utm_medium=Other&utm_campaign=RR+RSS+Feed&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"SimplyHired",
                "link":"https:\/\/www.simplyhired.com\/job\/95NnUpi-Sx6Ht2MwhlhprChDhv7hM-DpZav_fj0TbrABlqukH0eTwQ?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Monster",
                "link":"https:\/\/www.monster.com\/job-openings\/data-engineer-analytics-santa-fe-nm--0c32eb06-e271-4605-96c0-25f38257ac9b?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"JobGet",
                "link":"https:\/\/www.jobget.com\/jobs\/job\/66e8c941-c48b-4f05-bbe7-7a67689922fc?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Snagajob",
                "link":"https:\/\/www.snagajob.com\/jobs\/1167295634?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Adzuna",
                "link":"https:\/\/www.adzuna.com\/details\/5489466804?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Lensa",
                "link":"https:\/\/lensa.com\/job-v1\/cvs-health\/santa-fe-nm\/data-analytics-engineer\/ca35f68ffa686c0e638f55d7b9be803d?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Sign In - The Muse",
                "link":"https:\/\/google-indexers.themuse.com\/jobs\/cvshealth\/analytic-data-engineer-338498?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Full-time"
    },
    {
        "title":"Data Engineer",
        "company_name":"EP Wealth Advisors, LLC",
        "location":"Hampton, NH",
        "description":"EP Wealth Advisors (EPWA) is a wealth management advisory firm with over $40.5 billion in AUM as of September 30, 2025, serving predominately high net worth individuals. EPWA fosters an inclusive environment that offers opportunities for our associates to learn, grow and enhance their skills to take on new challenges to progress in their professional careers.\n\nEP Wealth Advisors is launching its first enterprise data platform initiative, a strategic effort to build a modern, cloud-based data environment powered by Snowflake. As a Data Engineer, you will play a pivotal role in the design, development, and implementation of this platform \u2014 working in close partnership with both internal technology teams and an experienced Snowflake implementation service provider.\n\nThis position is ideal for a data professional who thrives in a collaborative, fast-paced environment and wants to contribute to building a foundational data infrastructure from the ground up. You will help establish best practices, design scalable pipelines, and ensure the new platform enables firmwide analytics, reporting, and data-driven decision-making. Our ideal candidate will have a demonstrated track record of delivering excellent client service, with exceptional organizational, communication and problem-solving skills. You will join a team of dynamic, collaborative, and client-focused professionals who are focused on delivering on our founding core values: Integrity, Entrepreneurial, Inclusion and Connection.\n\nSalary Range: $120,000 - $150,000 plus annual bonus\n\nDuties and Responsibilities:\n\n\u2022 Collaborate closely with EP Wealth\u2019s Snowflake implementation partner to design, configure, and deploy the firm\u2019s first enterprise data platform.\n\n\u2022 Participate in architecture design sessions, technical planning, and milestone reviews with both internal stakeholders and external partners.\n\n\u2022 Develop and maintain efficient, secure, and scalable ETL\/ELT data pipelines to ingest data into Snowflake.\n\n\u2022 Build and optimize Snowflake data models, schemas, and role-based access controls.\n\n\u2022 Define and enforce Snowflake best practices for performance optimization, cost management, and data security.\n\n\u2022 Support environment setup, data migration, and initial production rollout activities in coordination with the implementation partner.\n\nIntegrate data from multiple enterprise systems including CRM, portfolio management, financial planning, and custodial platforms\n\n\u2022 Design data models and reusable ingestion frameworks that support analytics and reporting use cases.\n\n\u2022 Contribute to the development of a unified enterprise data model and metadata framework.\n\n\u2022 Collaborate with the implementation partner to align technical design with EP Wealth\u2019s business strategy and compliance standards.\n\n\u2022 Implement data validation, monitoring, and auditing to ensure data accuracy, completeness, and reliability.\n\n\u2022 Collaborate with IT leadership to define and operationalize data governance standards.\n\n\u2022 Troubleshoot and resolve data pipeline issues while maintaining system performance and uptime.\n\n\u2022 Conduct query and warehouse optimization to ensure efficient Snowflake usage.\n\n\u2022 Work closely with data analysts and technology teams to deliver accessible, well-documented data sets.\n\n\u2022 Participate in design reviews, sprint planning, and architectural discussions with the implementation partner.\n\n\u2022 Identify opportunities to extend Snowflake functionality through features such as Snowpark, data sharing, or automation tools.\n\n\u2022 Stay informed on Snowflake\u2019s evolving ecosystem and cloud data engineering best practices.\n\nQualifications\n\n\u2022 Bachelor\u2019s degree in Computer Science, Information Systems, Data Engineering, or a related technical field.\n\u2022 5+ years of data engineering experience, with direct hands-on experience in Snowflake strongly preferred.\n\u2022 Demonstrated success contributing to or co-delivering a cloud data platform implementation alongside external partners or vendors.\n\u2022 Experience building scalable data pipelines and transformations for analytics and operational reporting.\n\u2022 Advanced proficiency in SQL, with deep experience in Snowflake query optimization and data modeling.\n\u2022 Proficiency in Python or another scripting language for data processing and automation.\n\u2022 Experience with ETL\/ELT tools and orchestration platforms (DBT, Airflow, Dagster, etc.).\n\u2022 Familiarity with cloud infrastructure (AWS, GCP, or Azure) and Snowflake integrations.\n\u2022 Experience with enterprise data integration from CRM, portfolio management, and financial planning systems (Salesforce, Tamarac, Orion, etc.).\n\u2022 Understanding of data governance, lineage, and observability tools.\n\u2022 Strong understanding of Snowflake architecture, data sharing, and multi-cluster warehouse capabilities.\n\u2022 Experience defining data security and access policies in regulated environments.\n\u2022 Ability to bridge technical and business needs through thoughtful data design and collaboration.\n\nWhat We Offer\nWe offer a highly competitive suite of holistic benefits designed to help our team members balance their personal and professional life commitments. These include options designed to encourage employee's health, happiness, and financial well-being.\n\u2022 11 Paid Holidays + 2 floating holidays\n\u2022 3 Weeks (PTO)\n\u2022 Paid Volunteer Time\n\u2022 Flexible Work Schedule\n\u2022 Highly subsidized Health, Dental, and Vision Plans\n\u2022 401k Retirement Account with company match contributions\n\u2022 Free Mental Health services, Life Insurance, Long & Short-Term Disability Insurance\n\u2022 Flexible Spending Accounts and Health Savings Accounts\n\u2022 Employee Financial Education\n\u2022 Employee Educational Expense Reimbursement\n\u2022 Employee Charitable Donations\n\u2022 Employee Referral Incentives\n\u2022 Employee Team Building Activities\n\u2022 Employee Assistance Program\n\nEPWA is an equal opportunity employer. Prospective employees will receive consideration without discrimination because of race, creed, color, sex, gender, gender expression, gender identity, sexual orientation, age, religion, national origin, ancestry, mental disability, physical disability, medical condition, genetic information, marital status, military and veteran status, or any other basis protected by law.",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "Bachelor\u2019s degree in Computer Science, Information Systems, Data Engineering, or a related technical field",
                    "Demonstrated success contributing to or co-delivering a cloud data platform implementation alongside external partners or vendors",
                    "Experience building scalable data pipelines and transformations for analytics and operational reporting",
                    "Advanced proficiency in SQL, with deep experience in Snowflake query optimization and data modeling",
                    "Proficiency in Python or another scripting language for data processing and automation",
                    "Experience with ETL\/ELT tools and orchestration platforms (DBT, Airflow, Dagster, etc.)",
                    "Familiarity with cloud infrastructure (AWS, GCP, or Azure) and Snowflake integrations",
                    "Experience with enterprise data integration from CRM, portfolio management, and financial planning systems (Salesforce, Tamarac, Orion, etc.)",
                    "Understanding of data governance, lineage, and observability tools",
                    "Strong understanding of Snowflake architecture, data sharing, and multi-cluster warehouse capabilities",
                    "Experience defining data security and access policies in regulated environments",
                    "Ability to bridge technical and business needs through thoughtful data design and collaboration"
                ]
            },
            {
                "title":"Benefits",
                "items":[
                    "Salary Range: $120,000 - $150,000 plus annual bonus",
                    "We offer a highly competitive suite of holistic benefits designed to help our team members balance their personal and professional life commitments",
                    "These include options designed to encourage employee's health, happiness, and financial well-being",
                    "11 Paid Holidays + 2 floating holidays",
                    "3 Weeks (PTO)",
                    "Paid Volunteer Time",
                    "Flexible Work Schedule",
                    "Highly subsidized Health, Dental, and Vision Plans",
                    "401k Retirement Account with company match contributions",
                    "Free Mental Health services, Life Insurance, Long & Short-Term Disability Insurance",
                    "Flexible Spending Accounts and Health Savings Accounts",
                    "Employee Financial Education",
                    "Employee Educational Expense Reimbursement",
                    "Employee Charitable Donations",
                    "Employee Referral Incentives"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "As a Data Engineer, you will play a pivotal role in the design, development, and implementation of this platform \u2014 working in close partnership with both internal technology teams and an experienced Snowflake implementation service provider",
                    "This position is ideal for a data professional who thrives in a collaborative, fast-paced environment and wants to contribute to building a foundational data infrastructure from the ground up",
                    "You will help establish best practices, design scalable pipelines, and ensure the new platform enables firmwide analytics, reporting, and data-driven decision-making",
                    "Our ideal candidate will have a demonstrated track record of delivering excellent client service, with exceptional organizational, communication and problem-solving skills",
                    "Collaborate closely with EP Wealth\u2019s Snowflake implementation partner to design, configure, and deploy the firm\u2019s first enterprise data platform",
                    "Participate in architecture design sessions, technical planning, and milestone reviews with both internal stakeholders and external partners",
                    "Develop and maintain efficient, secure, and scalable ETL\/ELT data pipelines to ingest data into Snowflake",
                    "Build and optimize Snowflake data models, schemas, and role-based access controls",
                    "Define and enforce Snowflake best practices for performance optimization, cost management, and data security",
                    "Support environment setup, data migration, and initial production rollout activities in coordination with the implementation partner",
                    "Integrate data from multiple enterprise systems including CRM, portfolio management, financial planning, and custodial platforms",
                    "Design data models and reusable ingestion frameworks that support analytics and reporting use cases",
                    "Contribute to the development of a unified enterprise data model and metadata framework",
                    "Collaborate with the implementation partner to align technical design with EP Wealth\u2019s business strategy and compliance standards",
                    "Implement data validation, monitoring, and auditing to ensure data accuracy, completeness, and reliability",
                    "Collaborate with IT leadership to define and operationalize data governance standards",
                    "Troubleshoot and resolve data pipeline issues while maintaining system performance and uptime",
                    "Conduct query and warehouse optimization to ensure efficient Snowflake usage",
                    "Work closely with data analysts and technology teams to deliver accessible, well-documented data sets",
                    "Participate in design reviews, sprint planning, and architectural discussions with the implementation partner",
                    "Identify opportunities to extend Snowflake functionality through features such as Snowpark, data sharing, or automation tools",
                    "Stay informed on Snowflake\u2019s evolving ecosystem and cloud data engineering best practices"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"EP Wealth Advisors, LLC",
                "link":"https:\/\/jobiak.epwealth.com\/careers\/data-engineer-hampton-nh-690345e65d24500a56fa3467?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Full-time"
    },
    {
        "title":"Data Engineer-(Required \u2013 State of NY or NJ agency experience)",
        "company_name":"Cloud and Things Inc",
        "location":"Albany, NY",
        "description":"Our goal is to solve problems and deliver results for our clients. At Cloud and Things, you can be a part of transforming the public sector\u2019s IT environment. Our team is on the forefront of helping to solve the government's most complex IT challenges. If you are seeking a role that offers the opportunity to work on rewarding projects, consider a career with Cloud and Things.\n\u2022 This is an exempt position. Salary commensurate with experience*\n\nOverview:\u202f\n\nWe are seeking a Data Engineer who will support our State of NY client. The ideal candidate will have a strong background in data engineering, analytics, and modern data architecture, with specific expertise in healthcare data processing and claims analytics. This role requires adaptability to a fast-paced Agile environment, excellent problem-solving skills, and the ability to assess and communicate data quality nuances effectively.\n\nDuties:\n\u2022 Build and maintain high-quality data pipelines to support analytic solutions.\n\u2022 Process and analyze standard healthcare data, including claims, ensuring accuracy and quality.\n\u2022 Design and implement modern data architecture patterns, such as medallion or data lakehouse.\n\u2022 Partner with business analysts to align technical solutions with business requirements.\n\nMandatory Qualifications:\n\u2022 Bachelor's degree in computer science, MIS, Engineering, or a related field.\n\u2022 5 + years in data engineering or architecture, with a minimum of 2 years working with healthcare claims data.\n\u2022 Demonstrated experience in at least 1-2 Agile projects, showcasing adaptability to changing priorities and evolving requirements.\n\u2022 5 + years' experience in SQL and data analysis.\n\u2022 2 + years experience in programming languages such as Python or Scala\n\u2022 Experience using Vertica analytics database platform\n\u2022 Strong problem-solving abilities and adaptability in Agile environments.\n\u2022 Excellent communication skills to effectively convey data quality nuances and technical concepts.\n\u2022 A strong work ethic and flexibility to meet evolving project demands.\n\u2022 Understanding modern data architecture patterns, including medallion and data lakehouse.\n\nDesirable Qualifications:\u202f\n\u2022 Familiarity with machine learning, data science workflows, and statistical & data-mining techniques.\n\u2022 Experience in developing and maintaining data warehouses and big data solutions.\n\u2022 6 months experience with cloud computing platforms (e.g., AWS, Azure) and Infrastructure as Code (IaC) tools.\n\u2022 Experience with big data tools and frameworks, such as Hadoop, BigQuery, Hive, Impala, or Spark.\n\u2022 Knowledge of business intelligence tools like Tableau, Power BI, or Looker.\n\nCloud and Things complies with all applicable federal, state, and local laws regarding recruitment and hiring. All qualified applicants are considered for employment without regard to race,\u202fcolor, religion, sex, sexual orientation, gender identity, national origin, age, disability, protected veteran status, or any other category protected by applicable federal, state, or local laws.\n\nAI-Assisted Resume Evaluation Notice\n\nCloud and Things \u2013 Talent Management\n\nNotice to Candidates\n\nCloud and Things utilizes artificial intelligence (AI) tools to assist our recruiting team in evaluating candidate applications for streamlining; consistency, efficiency, and thoroughness. All hiring decisions are ultimately made by our human recruiting professionals.\n\nHow AI Is Used\n\nOur AI tools assist by:\n\u2022 Analyzing resumes against job requirements\n\u2022 Supporting our recruiters in candidate data evaluation\n\u2022 Ensuring consistent review standards across all applications\n\nImportant: AI serves as a support tool only. As noted above, all candidate selection and hiring decisions are made by experienced human recruiters. Your unedited resume will be processed by our AI tools as part of this evaluation.\n\nYour Data and Privacy\n\nCloud and Things Data Handling:\n\u2022 Your information is processed securely and used exclusively for recruitment purposes\n\u2022 Cloud and Things may store your resume in our Applicant Tracking System (ATS) indefinitely for future job matching opportunities\n\u2022 You may opt out of long-term ATS storage by emailing your name and your request to opt out of storing your resume in the ATS to: security@cloudandthings.com\n\u2022 All personal information is handled confidentially in accordance with our privacy policy\n\nAI Tool Data Processing:\n\u2022 AI processing data is retained for a maximum of 90 days, after which it is deleted\n\u2022 All data sent to AI tools is encrypted in transit and at rest\n\u2022 AI tools comply with applicable privacy laws including GDPR and CCPA\n\u2022 Personal data is anonymized or minimized wherever possible during AI processing\n\nYour Participation\n\nBy submitting your application, you acknowledge this notice and consent to AI-assisted evaluation as part of our recruitment process. You may opt out only by choosing not to submit your resume for consideration.\n\nJob Type: Contract\n\nPay: $85,000.00 - $110,000.00 per year\n\nWork Location: Remote",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "The ideal candidate will have a strong background in data engineering, analytics, and modern data architecture, with specific expertise in healthcare data processing and claims analytics",
                    "This role requires adaptability to a fast-paced Agile environment, excellent problem-solving skills, and the ability to assess and communicate data quality nuances effectively",
                    "Bachelor's degree in computer science, MIS, Engineering, or a related field",
                    "5 + years in data engineering or architecture, with a minimum of 2 years working with healthcare claims data",
                    "Demonstrated experience in at least 1-2 Agile projects, showcasing adaptability to changing priorities and evolving requirements",
                    "5 + years' experience in SQL and data analysis",
                    "2 + years experience in programming languages such as Python or Scala",
                    "Experience using Vertica analytics database platform",
                    "Strong problem-solving abilities and adaptability in Agile environments",
                    "Excellent communication skills to effectively convey data quality nuances and technical concepts",
                    "A strong work ethic and flexibility to meet evolving project demands",
                    "Understanding modern data architecture patterns, including medallion and data lakehouse",
                    "Familiarity with machine learning, data science workflows, and statistical & data-mining techniques",
                    "Experience in developing and maintaining data warehouses and big data solutions",
                    "6 months experience with cloud computing platforms (e.g., AWS, Azure) and Infrastructure as Code (IaC) tools",
                    "Experience with big data tools and frameworks, such as Hadoop, BigQuery, Hive, Impala, or Spark",
                    "Knowledge of business intelligence tools like Tableau, Power BI, or Looker",
                    "Cloud and Things complies with all applicable federal, state, and local laws regarding recruitment and hiring",
                    "Analyzing resumes against job requirements",
                    "Supporting our recruiters in candidate data evaluation",
                    "AI processing data is retained for a maximum of 90 days, after which it is deleted",
                    "AI tools comply with applicable privacy laws including GDPR and CCPA",
                    "Personal data is anonymized or minimized wherever possible during AI processing"
                ]
            },
            {
                "title":"Benefits",
                "items":[
                    "Pay: $85,000.00 - $110,000.00 per year",
                    "Work Location: Remote"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "Build and maintain high-quality data pipelines to support analytic solutions",
                    "Process and analyze standard healthcare data, including claims, ensuring accuracy and quality",
                    "Design and implement modern data architecture patterns, such as medallion or data lakehouse",
                    "Partner with business analysts to align technical solutions with business requirements",
                    "Ensuring consistent review standards across all applications",
                    "Important: AI serves as a support tool only",
                    "Your information is processed securely and used exclusively for recruitment purposes",
                    "All data sent to AI tools is encrypted in transit and at rest"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"Indeed",
                "link":"https:\/\/www.indeed.com\/viewjob?jk=b5490d059dc2afb3&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"SimplyHired",
                "link":"https:\/\/www.simplyhired.com\/job\/_X-bhjJ7Or6H74gr5SmlDE9bjXDw6N-i29p2h9ZXmPcqGqrCzf78KA?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Glassdoor",
                "link":"https:\/\/www.glassdoor.com\/job-listing\/data-engineer-required-%E2%80%93-state-of-ny-or-nj-agency-experience-cloud-and-things-JV_IC1131650_KO0,60_KE61,77.htm?jl=1009879146799&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"BeBee",
                "link":"https:\/\/us.bebee.com\/job\/063dd69ec02faa0c6ea32bcd65485b4e?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Jobs Trabajo.org",
                "link":"https:\/\/us.trabajo.org\/job-3821-c7b6212ef2a4dacffb64b62fbdbf97f7?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Learn4Good",
                "link":"https:\/\/www.learn4good.com\/jobs\/new-york\/info_technology\/4558771115\/e\/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Contractor"
    },
    {
        "title":"10025 - Sr. Big Data Engineer",
        "company_name":"Hyundai Autoever America",
        "location":"Fountain Valley, CA",
        "description":"10025 \u2013 Sr. Big Data Engineer\n\nJob Summary\n\nDesign, build, and maintain the Information\/Proposed Changes platform that enables large-scale data processing and analysis. Responsible for developing and maintaining data pipelines, data lakes, and other data-related platform. Work closely with data scientists, analysts, and other stakeholders to ensure that data is properly collected, stored, and processed for analysis and reporting purposes. Implement and maintain data security and access controls to ensure that data is protected. Responsible for troubleshooting and resolving technical issues related to data infrastructure and ensuring that data systems are scalable and efficient. Play a critical role in enabling organization to derive insights and value from their data assets.\n\nESSENTIAL FUNCTIONS\n\u2022 This job requires experience in building and maintaining scalable data pipelines and robust data models from structured and unstructured sources for AI\/ML.\n\u2022 The ideal candidate should have advanced SQL skills and be able to query and transform large structured\/unstructured datasets using Spark\/PySpark, Spark SQL\/Hive and Hive\/NoSQL.\n\u2022 They should also have experience in developing Big Data pipelines in orchestration tools such as Airflow and Oozie, designing tooling for access management, monitoring, data controls, and self-service ETL\/Analytics pipelines.\n\u2022 Other requirements include hands-on experience with On-Prem Big Data Platform, sound knowledge of Distributed Data Processing frameworks, resource management frameworks like YARN, and proficiency in writing data pipelines using Spark, Python and Scala.\n\u2022 The ideal candidate should also have experience in developing frameworks\/utilities in Python, working in a Dev\/Ops environment, and following development best practices such as code reviews and unit testing.\n\u2022 Additionally, the candidate should be able to diagnose software issues and engineering workarounds, have a good understanding of BI tools such as Tableau\/Power BI and MicroStrategy for Big Data, and be able to lead, guide and assist team members with project development and problem solving.\n\u2022 The candidate should also be flexible and able to learn and use new technologies, work well in a team environment as well as independently to achieve goals.\n\nPlease note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.\n\nRequired Skills, Attributes & Education\n\u2022 Bachelor\u2019s degree OR equivalent (with major course work in computer science) preferred\n\u2022 10+ years of IT experience working with hands on working experience, in software development, building data pipelines and data processing frameworks.\n\u2022 7+ years of experience as a Data Engineer\n\u2022 Big Data Technologies: Knowledge of Big Data technologies such as Hadoop, Spark, Hive, Pig, Kafka, and NoSQL databases such as MongoDB, Cassandra, and HBase.\n\u2022 Distributed Systems: Understanding of distributed systems and distributed computing principles.\n\u2022 Programming Languages: Proficiency in programming languages such as Java, Python, Scala, and SQL.\n\u2022 Data Modeling: Knowledge of data modeling techniques and tools to design efficient data structures for Big Data systems.\n\u2022 Data Processing: Experience with data processing and ETL (Extract, Transform, Load) tools and techniques.\n\u2022 Cloud Computing: Familiarity with cloud computing platforms such as AWS, Azure, and Google Cloud.\n\u2022 Data Security: Knowledge of data security principles and experience implementing security measures for Big Data systems.\n\u2022 Data Warehousing: Understanding of data warehousing concepts and experience designing and maintaining data warehouses.\n\u2022 Analytics and Machine Learning: Familiarity with analytics and machine learning tools and techniques and their implementation in Big Data systems.\n\u2022 Performance Tuning: Experience with performance tuning and optimization techniques for Big Data systems to ensure scalability, reliability, and high availability.\n\nCertifications\n\nCloudera\/Hortonworks\/Databricks - Spark\/Hadoop certification\n\nSalary Range: $103,170 to $158,873",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "The ideal candidate should have advanced SQL skills and be able to query and transform large structured\/unstructured datasets using Spark\/PySpark, Spark SQL\/Hive and Hive\/NoSQL",
                    "They should also have experience in developing Big Data pipelines in orchestration tools such as Airflow and Oozie, designing tooling for access management, monitoring, data controls, and self-service ETL\/Analytics pipelines",
                    "Other requirements include hands-on experience with On-Prem Big Data Platform, sound knowledge of Distributed Data Processing frameworks, resource management frameworks like YARN, and proficiency in writing data pipelines using Spark, Python and Scala",
                    "The ideal candidate should also have experience in developing frameworks\/utilities in Python, working in a Dev\/Ops environment, and following development best practices such as code reviews and unit testing",
                    "Additionally, the candidate should be able to diagnose software issues and engineering workarounds, have a good understanding of BI tools such as Tableau\/Power BI and MicroStrategy for Big Data, and be able to lead, guide and assist team members with project development and problem solving",
                    "The candidate should also be flexible and able to learn and use new technologies, work well in a team environment as well as independently to achieve goals",
                    "10+ years of IT experience working with hands on working experience, in software development, building data pipelines and data processing frameworks",
                    "7+ years of experience as a Data Engineer",
                    "Big Data Technologies: Knowledge of Big Data technologies such as Hadoop, Spark, Hive, Pig, Kafka, and NoSQL databases such as MongoDB, Cassandra, and HBase",
                    "Distributed Systems: Understanding of distributed systems and distributed computing principles",
                    "Programming Languages: Proficiency in programming languages such as Java, Python, Scala, and SQL",
                    "Data Modeling: Knowledge of data modeling techniques and tools to design efficient data structures for Big Data systems",
                    "Data Processing: Experience with data processing and ETL (Extract, Transform, Load) tools and techniques",
                    "Cloud Computing: Familiarity with cloud computing platforms such as AWS, Azure, and Google Cloud",
                    "Data Security: Knowledge of data security principles and experience implementing security measures for Big Data systems",
                    "Data Warehousing: Understanding of data warehousing concepts and experience designing and maintaining data warehouses",
                    "Analytics and Machine Learning: Familiarity with analytics and machine learning tools and techniques and their implementation in Big Data systems",
                    "Performance Tuning: Experience with performance tuning and optimization techniques for Big Data systems to ensure scalability, reliability, and high availability",
                    "Cloudera\/Hortonworks\/Databricks - Spark\/Hadoop certification"
                ]
            },
            {
                "title":"Benefits",
                "items":[
                    "Salary Range: $103,170 to $158,873"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "Design, build, and maintain the Information\/Proposed Changes platform that enables large-scale data processing and analysis",
                    "Responsible for developing and maintaining data pipelines, data lakes, and other data-related platform",
                    "Work closely with data scientists, analysts, and other stakeholders to ensure that data is properly collected, stored, and processed for analysis and reporting purposes",
                    "Implement and maintain data security and access controls to ensure that data is protected",
                    "Responsible for troubleshooting and resolving technical issues related to data infrastructure and ensuring that data systems are scalable and efficient",
                    "Play a critical role in enabling organization to derive insights and value from their data assets",
                    "This job requires experience in building and maintaining scalable data pipelines and robust data models from structured and unstructured sources for AI\/ML",
                    "Duties, responsibilities, and activities may change at any time with or without notice"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"Jooble",
                "link":"https:\/\/jooble.org\/jdp\/-2156155785055062610?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Ladders",
                "link":"https:\/\/www.theladders.com\/job\/10025-sr-big-data-engineer-hyundai-autoever-america-fountain-valley-ca_82260166?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Jobrapido",
                "link":"https:\/\/us.jobrapido.com\/jobpreview\/8299004154592886784?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Quick Job California",
                "link":"https:\/\/quickjobcalifornia.com\/job-details\/674213958165423000?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Full-time"
    },
    {
        "title":"Data Engineer AI II",
        "company_name":"Cotiviti",
        "location":"Remote, OR",
        "description":"Overview\n\nAt Cotiviti, we are custodians of data for our clients. Using their technical experience in ETL processes, Data Engineers ensure operational functions are occurring as expected. This includes but is not limited to managing data implementations, data integrations, data production, data quality, and data security.\nResponsibilities\n\u2022 Create, maintain and execute intermediate to advanced Spark scripts for data management and data validation, and data integration.\n\u2022 Create, maintain and execute basic to intermediate SQL scripts for data management and data validation.\n\u2022 Optimize the queries to improve the efficiency of daily tasks.\n\u2022 Perform data analysis and identify any issues.\n\u2022 Work with other groups such as Engineering team, DBA, Cloud ops, etc. to troubleshoot and resolve any environmental or network issues that impact your work. Extend your support to after - hours or weekends as needed.\n\u2022 Create and maintain data pipelines as needed.\n\u2022 Validates the tasks results to ensure that all the requirements are met.\n\u2022 Adhere to all the industry level and organization level compliance rules and regulations to maintain data integrity.\n\u2022 Complete individual productivity tracking.\n\u2022 Complete task assignments using department ticketing system within assigned deadline.\n\u2022 Achieve organizational and individual goals as identified in performance reviews and goal setting exercises.\n\u2022 Complete all special projects and other duties as assigned.\n\u2022 Must be able to perform duties with or without reasonable accommodation.\n\nThis job description is intended to describe the general nature and level of work being performed and is not to be construed as an exhaustive list of responsibilities, duties and skills required. This job description does not constitute an employment agreement and is subject to change as the needs of Cotiviti and requirements of the job change.\nQualifications\n\u2022 Bachelor's degree in Computer Science, Information Technology or equivalent work experience.\n\u2022 6+ years of working knowledge of big data technologies (Spark, S3, Kafka, Ray, Hadoop, etc.).\n\u2022 6+ years of working knowledge of cloud (Databricks, AWS, Azure, GCP, OCI etc.).\n\u2022 6+ years of working knowledge of RDBMS (Oracle, MS SQL, Vertica, etc.) and experience using SQL, PL\/SQL or other data integration\/ETL tools.\n\u2022 6+ years of data analysis. Preferably in the Healthcare industry of enrollment, medical claims and\/or pharmacy claims.\n\u2022 Databricks and\/or Snowflake environment familiarity a big plus.\n\u2022 Any Databricks \/ AWS certifications is a big plus.\n\u2022 Familiarity with data pipeline orchestration tools (e.g., Airflow, Databricks Workflows).\n\u2022 Proficient in Microsoft Office Suite applications PowerPoint, Word, Excel and Outlook.\n\u2022 Flexible work schedule.\n\u2022 Experience with project management tools like JIRA.\n\u2022 Strong analytical skills.\n\u2022 Excellent verbal, listening and written communication skills.\n\nCognitive\/Mental Requirements:\n\u2022 Ability to multitask and prioritize projects to meet scheduled deadlines and tight turnaround times. Ability to work well independently or in a team environment.\n\u2022 Communicating with others to exchange information.\n\u2022 Assessing the accuracy, neatness and thoroughness of the work assigned.\n\nWorking Conditions and Physical Requirements:\n\u2022 Remaining in a stationary position, often standing or sitting for prolonged periods.\n\u2022 Repeating motions that may include the wrists, hands and\/or fingers.\n\u2022 No adverse environmental conditions expected.\n\u2022 Must be able to provide a dedicated, secure work area.\n\u2022 Must be able to provide high-speed internet access \/ connectivity and office setup and maintenance.\n\nBase compensation ranges from $120,000 to $145,000 per year. Specific offers are determined by various factors, such as experience, education, skills, certifications, and other business needs.\n\nCotiviti offers team members a competitive benefits package to address a wide range of personal and family needs, including medical, dental, vision, disability, and life insurance coverage, 401(k) savings plans, paid family leave, 9 paid holidays per year, and 17-27 days of Paid Time Off (PTO) per year, depending on specific level and length of service with Cotiviti. For information about our benefits package, please refer to our Careers page.\n\nSince this job will be based remotely, all interviews will be conducted virtually.\n\nDate of posting: 10\/07\/2025\n\nApplications are assessed on a rolling basis. We anticipate that the application window will close on 12\/07\/2025, but the application window may change depending on the volume of applications received or close immediately if a qualified candidate is selected.\n\n#LI-LL1\n\n#LI-remote\n\n#senior\n\nEmployment Type: OTHER",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "Must be able to perform duties with or without reasonable accommodation",
                    "Bachelor's degree in Computer Science, Information Technology or equivalent work experience",
                    "6+ years of working knowledge of big data technologies (Spark, S3, Kafka, Ray, Hadoop, etc.)",
                    "6+ years of working knowledge of cloud (Databricks, AWS, Azure, GCP, OCI etc.)",
                    "6+ years of working knowledge of RDBMS (Oracle, MS SQL, Vertica, etc.) and experience using SQL, PL\/SQL or other data integration\/ETL tools",
                    "6+ years of data analysis",
                    "Preferably in the Healthcare industry of enrollment, medical claims and\/or pharmacy claims",
                    "Databricks and\/or Snowflake environment familiarity a big plus",
                    "Any Databricks \/ AWS certifications is a big plus",
                    "Familiarity with data pipeline orchestration tools (e.g., Airflow, Databricks Workflows)",
                    "Proficient in Microsoft Office Suite applications PowerPoint, Word, Excel and Outlook",
                    "Flexible work schedule",
                    "Experience with project management tools like JIRA",
                    "Strong analytical skills",
                    "Excellent verbal, listening and written communication skills",
                    "Ability to multitask and prioritize projects to meet scheduled deadlines and tight turnaround times",
                    "Ability to work well independently or in a team environment",
                    "No adverse environmental conditions expected",
                    "Must be able to provide a dedicated, secure work area",
                    "Must be able to provide high-speed internet access \/ connectivity and office setup and maintenance"
                ]
            },
            {
                "title":"Benefits",
                "items":[
                    "Base compensation ranges from $120,000 to $145,000 per year",
                    "Specific offers are determined by various factors, such as experience, education, skills, certifications, and other business needs",
                    "Cotiviti offers team members a competitive benefits package to address a wide range of personal and family needs, including medical, dental, vision, disability, and life insurance coverage, 401(k) savings plans, paid family leave, 9 paid holidays per year, and 17-27 days of Paid Time Off (PTO) per year, depending on specific level and length of service with Cotiviti"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "Using their technical experience in ETL processes, Data Engineers ensure operational functions are occurring as expected",
                    "This includes but is not limited to managing data implementations, data integrations, data production, data quality, and data security",
                    "Create, maintain and execute intermediate to advanced Spark scripts for data management and data validation, and data integration",
                    "Create, maintain and execute basic to intermediate SQL scripts for data management and data validation",
                    "Optimize the queries to improve the efficiency of daily tasks",
                    "Perform data analysis and identify any issues",
                    "Work with other groups such as Engineering team, DBA, Cloud ops, etc",
                    "to troubleshoot and resolve any environmental or network issues that impact your work",
                    "Extend your support to after - hours or weekends as needed",
                    "Create and maintain data pipelines as needed",
                    "Validates the tasks results to ensure that all the requirements are met",
                    "Adhere to all the industry level and organization level compliance rules and regulations to maintain data integrity",
                    "Complete individual productivity tracking",
                    "Complete task assignments using department ticketing system within assigned deadline",
                    "Achieve organizational and individual goals as identified in performance reviews and goal setting exercises",
                    "Complete all special projects and other duties as assigned",
                    "Communicating with others to exchange information",
                    "Assessing the accuracy, neatness and thoroughness of the work assigned",
                    "Remaining in a stationary position, often standing or sitting for prolonged periods",
                    "Repeating motions that may include the wrists, hands and\/or fingers"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"ZipRecruiter",
                "link":"https:\/\/www.ziprecruiter.com\/c\/Cotiviti\/Job\/Data-Engineer-AI-II\/-in-Remote,OR?jid=4d7a8598d2f299e6&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"SportsTech Jobs",
                "link":"https:\/\/sportstechjobs.com\/principal-data-engineer-ii-kaizen-gaming\/6767408de56f5ac720ef51d3?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"BeBee",
                "link":"https:\/\/us.bebee.com\/job\/4062274e8abdd957b0e26d4acc15485b?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"RemoteAmbition",
                "link":"https:\/\/remoteambition.com\/jobs\/74e7895e-cb1e-4564-ade9-63c6408a37f4?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Jobs Trabajo.org",
                "link":"https:\/\/us.trabajo.org\/job-3310-bf4b996c5187848e160ad896d8e1a991?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Full-time"
    },
    {
        "title":"Data Engineering Manager",
        "company_name":"Boingo",
        "location":"Frisco, TX",
        "description":"Data Engineering Manager\n\nWe are seeking an experienced and visionary Data Engineering Manager to lead the strategy, innovation, and evolution of our enterprise data platform, with a strong focus on Generative AI (GenAI) capabilities within Google Cloud Platform (GCP). You will manage a high-performing data engineering team, guiding them in building scalable, high-performance data solutions, enabling analytics through Looker, and delivering production-grade systems that power data-driven decision-making and AI-enabled applications across the organization.\n\nKey Responsibilities\n\u2022 Lead and manage the data engineering team, setting priorities, allocating resources, and ensuring successful delivery of strategic initiatives.\n\u2022 Define and execute the company's ETL and ELT strategy, optimizing data movement, transformation, and orchestration for efficiency, scalability, and cost-effectiveness.\n\u2022 Architect and oversee the development of robust data pipelines using BigQuery, Dataflow, Pub\/Sub, and other GCP-native services.\n\u2022 Drive the development of foundational infrastructure to support GenAI use cases (Microsoft Copilot\/GCP Vertex AI) and emerging AI-driven capabilities.\n\u2022 Partner with product, marketing, customer care, and other cross-functional teams to translate business needs into data- and AI-powered solutions.\n\u2022 Ensure Looker models, dashboards, and governance processes enable self-service analytics across the enterprise.\n\u2022 Champion engineering best practices including CI\/CD, automated testing, and high-quality coding standards in Go and Python.\n\u2022 Mentor, coach, and develop team members, building technical depth and leadership skills across the team.\n\u2022 Ensure security, reliability, and performance across all data platforms and services.\n\u2022 Serve as a thought leader and change agent, driving innovation and positioning the organization at the forefront of modern data engineering and AI.\n\nRequired Qualifications\n\u2022 8+ years of experience in data engineering, with at least 3+ years in a management or team leadership role.\n\u2022 Proven expertise in Google Cloud Platform (GCP): BigQuery, Vertex AI, Dataflow, Cloud Functions, and Cloud Storage.\n\u2022 Strong programming skills in Go and Python, with experience building scalable backend and data systems.\n\u2022 Deep understanding of ETL\/ELT architecture, data movement strategies, and large-scale data platform design.\n\u2022 Experience with Generative AI tools and frameworks (Vertex AI, Gemini).\n\u2022 Strong SQL skills and data modeling expertise for enterprise-scale data platforms.\n\u2022 Experience with Looker (LookML modeling, dashboard design, governance).\n\u2022 Excellent leadership, communication, and stakeholder management skills.\n\nPreferred Qualifications\n\u2022 Experience with containerized deployment (Docker, Kubernetes) and orchestration tools (Cloud Composer, Airflow).\n\u2022 Proven ability to operate in agile, fast-paced environments while managing multiple priorities.\n\u2022 Experience managing budgets, vendor relationships, and cloud cost optimization.\n\nBenefits & Perks\n\u2022 Health, dental, vision\n\u2022 401(k) match\n\u2022 Unlimited vacation\n\u2022 Tuition reimbursement\n\u2022 Cell phone reimbursement\n\u2022 Pet care benefits\n\u2022 12 weeks of paid parental leave, and more!\n\u2022 No wonder we've been named among the Best Places to Work!\n\nMeet Boingo \u2013 named among the Best Places to Work!\n\nBoingo Wireless simplifies complex wireless challenges to connect people, business and things. Our vast footprint of Wi-Fi and cellular networks reaches more than a billion consumers annually. From airports and stadiums to military bases, Boingo helps folks stay connected to the people and things they love.\n\nOur team is creative, collaborative, and on the leading edge of technology. There is no doubt the \"secret sauce\" to Boingo's success is our incredible team and culture. We take pride in having fun and building awesome products.\n\nBoingo's new headquarters is at The Star \u2013 home of the Dallas Cowboys. The Star District offers 35 shops, restaurants, and specialty services.\n\nWe are an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, gender, gender identity, sexual orientation, age, disability or veteran status.\n\n#LI-Hybrid",
        "job_highlights":[
            {
                "title":"Qualifications",
                "items":[
                    "8+ years of experience in data engineering, with at least 3+ years in a management or team leadership role",
                    "Proven expertise in Google Cloud Platform (GCP): BigQuery, Vertex AI, Dataflow, Cloud Functions, and Cloud Storage",
                    "Strong programming skills in Go and Python, with experience building scalable backend and data systems",
                    "Deep understanding of ETL\/ELT architecture, data movement strategies, and large-scale data platform design",
                    "Experience with Generative AI tools and frameworks (Vertex AI, Gemini)",
                    "Strong SQL skills and data modeling expertise for enterprise-scale data platforms",
                    "Experience with Looker (LookML modeling, dashboard design, governance)",
                    "Excellent leadership, communication, and stakeholder management skills"
                ]
            },
            {
                "title":"Benefits",
                "items":[
                    "Health, dental, vision",
                    "401(k) match",
                    "Unlimited vacation",
                    "Tuition reimbursement",
                    "Cell phone reimbursement",
                    "Pet care benefits",
                    "12 weeks of paid parental leave, and more!"
                ]
            },
            {
                "title":"Responsibilities",
                "items":[
                    "You will manage a high-performing data engineering team, guiding them in building scalable, high-performance data solutions, enabling analytics through Looker, and delivering production-grade systems that power data-driven decision-making and AI-enabled applications across the organization",
                    "Lead and manage the data engineering team, setting priorities, allocating resources, and ensuring successful delivery of strategic initiatives",
                    "Define and execute the company's ETL and ELT strategy, optimizing data movement, transformation, and orchestration for efficiency, scalability, and cost-effectiveness",
                    "Architect and oversee the development of robust data pipelines using BigQuery, Dataflow, Pub\/Sub, and other GCP-native services",
                    "Drive the development of foundational infrastructure to support GenAI use cases (Microsoft Copilot\/GCP Vertex AI) and emerging AI-driven capabilities",
                    "Partner with product, marketing, customer care, and other cross-functional teams to translate business needs into data- and AI-powered solutions",
                    "Ensure Looker models, dashboards, and governance processes enable self-service analytics across the enterprise",
                    "Champion engineering best practices including CI\/CD, automated testing, and high-quality coding standards in Go and Python",
                    "Mentor, coach, and develop team members, building technical depth and leadership skills across the team",
                    "Ensure security, reliability, and performance across all data platforms and services",
                    "Serve as a thought leader and change agent, driving innovation and positioning the organization at the forefront of modern data engineering and AI"
                ]
            }
        ],
        "apply_options":[
            {
                "title":"Indeed",
                "link":"https:\/\/www.indeed.com\/viewjob?jk=ba51cfafdb1b5f5d&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Built In",
                "link":"https:\/\/builtin.com\/job\/data-engineering-manager\/7601501?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"LinkedIn",
                "link":"https:\/\/www.linkedin.com\/jobs\/view\/data-engineering-manager-at-boingo-wireless-4334221592?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Ladders",
                "link":"https:\/\/www.theladders.com\/job\/data-engineering-manager-boingowireless-frisco-tx_84272927?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Career.io",
                "link":"https:\/\/career.io\/job\/data-engineering-manager-frisco-boingo-19eb1e4bf21b865eb654619309ceae0b?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Swooped",
                "link":"https:\/\/swooped.co\/job-postings\/data-engineering-manager-frisco-boingo-2df35?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"Glassdoor",
                "link":"https:\/\/www.glassdoor.com\/job-listing\/data-engineering-manager-boingo-JV_IC1139994_KO0,24_KE25,31.htm?jl=1009929045209&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            },
            {
                "title":"SimplyHired",
                "link":"https:\/\/www.simplyhired.com\/job\/-TD5AIPJht2NXb-b0iOgz5tEU4yiV3emerwnNk1uPU5gnrefCBaWFw?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic"
            }
        ],
        "schedule_type":"Full-time"
    }
]